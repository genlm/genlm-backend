{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>GenLM Backend is a high-performance backend for language model probabilistic programs, built for the GenLM ecosystem. It provides an asynchronous and autobatched interface to <code>vllm</code> and <code>transformers</code> language models, enabling scalable and efficient inference.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automatic batching of concurrent log-probability requests\u2014enabling efficient large-scale inference without having to write batching logic yourself</li> <li>Byte-level decoding support for transformers tokenizers\u2014enabling advanced token-level control</li> <li>Supports for arbitrary Hugging Face models (e.g., LLaMA, DeepSeek, etc.) with fast inference and automatic KV caching using vllm</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>This library supports installation via pip:</p> <pre><code>pip install genlm-backend\n</code></pre>"},{"location":"#example-autobatched-sequential-importance-sampling-with-llms","title":"Example: Autobatched Sequential Importance Sampling with LLMs","text":"<p>This example demonstrates how <code>genlm-backend</code> enables concise, scalable probabilistic inference with language models. It implements a Sequential Importance Sampling (SIS) algorithm that makes asynchronous log-probabality requests which get automatically batched by the language model.</p> <pre><code>import torch\nimport asyncio\nfrom genlm.backend import load_model_by_name\n\n# --- Token-level masking using the byte-level vocabulary --- #\ndef make_masking_function(llm, max_token_length, max_tokens):\n    eos_id = llm.tokenizer.eos_token_id\n    valid_ids = torch.tensor([\n        token_id == eos_id or len(token) &lt;= max_token_length\n        for token_id, token in enumerate(llm.byte_vocab)\n    ], dtype=torch.float).log()\n    eos_one_hot = torch.nn.functional.one_hot(\n        torch.tensor(eos_id), len(llm.byte_vocab)\n    ).log()\n\n    def masking_function(context):\n        return eos_one_hot if len(context) &gt;= max_tokens else valid_ids\n\n    return masking_function\n\n# --- Particle class for SIS --- #\nclass Particle:\n    def __init__(self, llm, mask_function, prompt_ids):\n        self.context = []\n        self.prompt_ids = prompt_ids\n        self.log_weight = 0.0\n        self.active = True\n        self.llm = llm\n        self.mask_function = mask_function\n\n    async def extend(self):\n        logps = await self.llm.next_token_logprobs(self.prompt_ids + self.context)\n        masked_logps = logps + self.mask_function(self.context).to(logps.device)\n        logZ = masked_logps.logsumexp(dim=-1)\n        self.log_weight += logZ\n        next_token_id = torch.multinomial((masked_logps - logZ).exp(), 1).item()\n        if next_token_id == self.llm.tokenizer.eos_token_id:\n            self.active = False\n        else:\n            self.context.append(next_token_id)\n\n# --- Autobatched SIS loop --- #\nasync def autobatched_sis(n_particles, llm, masking_function, prompt_ids):\n    particles = [Particle(llm, masking_function, prompt_ids) for _ in range(n_particles)]\n    while any(p.active for p in particles):\n        await asyncio.gather(*[p.extend() for p in particles if p.active])\n    return particles\n\n# --- Run the example --- #\nllm = load_model_by_name(\"gpt2\") # or e.g., \"meta-llama/Llama-3.2-1B\" if you have access\nmask_function = make_masking_function(llm, max_token_length=10, max_tokens=10)\nprompt_ids = llm.tokenizer.encode(\"Montreal is\")\nparticles = await autobatched_sis( # use asyncio.run(autobatched_sis(...)) if you are not in an async context\n    n_particles=10, llm=llm, masking_function=mask_function, prompt_ids=prompt_ids\n)\n\nstrings = [llm.tokenizer.decode(p.context) for p in particles]\nlog_weights = torch.tensor([p.log_weight for p in particles])\nprobs = torch.exp(log_weights - log_weights.logsumexp(dim=-1))\n\nfor s, p in sorted(zip(strings, probs), key=lambda x: -x[1]):\n    print(f\"{repr(s)} (probability: {p:.4f})\")\n</code></pre> <p>This example highlights the following features:</p> <ul> <li>\ud83c\udf00 Asynchronous Inference Loop. Each particle runs independently, but all LLM calls are scheduled concurrently via <code>asyncio.gather</code>. The backend batches them automatically, so you get the efficiency of large batched inference without writing batching logic yourself.</li> <li>\ud83d\udd01 Byte-level Tokenization Support. Token filtering is done using the model\u2019s byte-level vocabulary, which <code>genlm-backend</code> exposes\u2014enabling low-level control over generation in ways not possible with most high-level APIs.</li> </ul>"},{"location":"#development","title":"Development","text":"<p>See the DEVELOPING.md file for information on how to install the project for local development.</p>"},{"location":"#main-components","title":"Main Components","text":""},{"location":"#asynchronous-language-model-backends","title":"Asynchronous Language Model Backends","text":"<p>The <code>genlm.backend.llm</code> module provides asynchronous interfaces for computing next-token probabilities with <code>vllm</code> and <code>transformer</code> language models.</p> <pre><code>from genlm.backend import AsyncVirtualLM\n# Initialize model with vLLM backend from a HuggingFace model name\nllm = AsyncVirtualLM.from_name(\"meta-llama/Llama-3.2-1B\")\n</code></pre> <p>This submodule includes two key classes:</p> <ul> <li>AsyncVirtualLM (GPU): vLLM-based backend optimized for next-token probability computations. Fastest and most memory-efficient; requires a GPU. Uses vLLM's prefix caching feature for KV caching.</li> <li>AsyncTransformer (CPU): HuggingFace-based backend for next-token probability computations. Slower and less memory-efficient; for CPU usage.</li> </ul> <p>See the LLM Code Reference for detailed API documentation.</p>"},{"location":"#vocabulary-decoding","title":"Vocabulary Decoding","text":"<p>The <code>genlm.backend.tokenization</code> module converts Hugging Face tokenizer vocabularies into byte and string representations, with each token's representation stored at its corresponding token ID in the output lists.</p> <pre><code>from transformers import AutoTokenizer\nfrom genlm.backend import decode_vocab\n\n# Load a tokenizer and decode its vocabulary\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nbyte_vocab, str_vocab = decode_vocab(tokenizer)\nbyte_vocab[10] # Byte representation of token with ID 10\n</code></pre> <p>Warning</p> <p>The byte representation (<code>byte_vocab</code>) is the canonical form and should be preferred for reliable token handling. The string representation (<code>str_vocab</code>) is provided for convenience and debugging but may not correctly represent all tokens, especially those containing invalid UTF-8 sequences.</p>"},{"location":"#token-character-tries","title":"Token-Character Tries","text":"<p>The <code>genlm.backend.trie</code> module provides an efficient trie data structure for mapping weight distributions over tokens to weight distributions over token prefixes.</p> <pre><code>from genlm.backend import TokenCharacterTrie\n# Initialize TokenCharacterTrie from a byte vocabulary\ntrie = TokenCharacterTrie(decode=[b'cat', b'cats', b'dog', b'dogs'])\nprobs = [0.4, 0.1, 0.3, 0.2]\n# Get mass at each node given a distribution over the vocab\ntrie_ws = trie.weight_sum(probs)\ntrie.visualize(trie_ws)\n</code></pre> <p></p> <p>This submodule includes three key classes:</p> <ul> <li>TokenCharacterTrie (CPU): Base implementation for CPU usage.</li> <li>ParallelTokenCharacterTrie (GPU): GPU-accelerated version which uses sparse matrix operations for mass summing.</li> <li>AsyncTokenCharacterTrie (Async): Asynchronous wrapper for use in asynchronous contexts; enables automatic batching of concurrent requests. This class can wrap either the sequential or parallel trie implementations.</li> </ul> <p>See the Trie Code Reference for detailed API documentation.</p>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you are getting:     <pre><code>A module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy&lt;2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n</code></pre>     then you should downgrade your version of <code>numpy</code> with <code>pip install \"numpy&lt;2\"</code>.</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>genlm<ul> <li>backend<ul> <li>cache</li> <li>llm<ul> <li>base</li> <li>hf</li> <li>mlx</li> <li>vllm</li> </ul> </li> <li>tokenization<ul> <li>bytes</li> <li>vocab</li> </ul> </li> <li>trie<ul> <li>async_impl</li> <li>base</li> <li>parallel</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/genlm/backend/__init__/","title":"backend","text":""},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM","title":"<code>AsyncVirtualLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>class AsyncVirtualLM(AsyncLM):\n    default_params = {\n        \"max_tokens\": 1,\n        \"n\": 1,\n        \"detokenize\": False,\n        \"stop\": None,\n        \"ignore_eos\": True,\n    }\n\n    def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n        \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n        Args:\n            async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n            cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n            cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm.backend.cache.OutputCache] constructor. Defaults to {}.\n\n        Note:\n            The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n        \"\"\"\n        self.async_llm_engine = async_llm_engine\n        self.tokenizer = async_llm_engine.engine.get_tokenizer()\n        self.request_counter = Counter()\n        self.cache = (\n            OutputCache(maxsize=cache_size, **cache_opts)\n            if cache_size &gt; 0\n            else None\n        )\n\n        async_llm_engine.engine.log_stats = False\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    @classmethod\n    def from_name(cls, model_name, engine_opts=None, **kwargs):\n        \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n                configured with prefix caching enabled and async output processing disabled by default.\n            **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n        Returns:\n            (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n        \"\"\"\n        if not HAS_VLLM:\n            raise ImportError(  # pragma: no cover\n                \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n            )\n\n        if engine_opts is not None and \"enable_chunked_prefill\" in engine_opts:\n            if engine_opts[\"enable_chunked_prefill\"]:\n                warnings.warn(  # pragma: no cover\n                    \"Setting enable_chunked_prefill to True may interfere with AsyncVirtualLM's \"\n                    \"custom sampling functionality.\"\n                )\n\n        engine_opts = {\n            \"enable_prefix_caching\": True,\n            \"disable_log_requests\": True,\n            \"disable_async_output_proc\": True,  # This parameter forces vLLM to use v0, which is currently what we want to do.\n            **(engine_opts or {}),\n        }\n\n        engine = AsyncLLMEngine.from_engine_args(\n            AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n        )\n\n        return cls(engine, **kwargs)\n\n    @property\n    def underlying_model(self):\n        return self.async_llm_engine.engine.model_executor.driver_worker.model_runner.model\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            result (torch.Tensor): Normalized log probability tensor.\n\n        Warning:\n            Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n            For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n        \"\"\"\n        key = tuple(token_ids)\n\n        if self.cache is not None and key in self.cache:\n            return self.cache[key]\n\n        result = await self._next_token_logprobs(key)\n\n        if self.cache is not None:\n            self.cache[key] = result\n\n        return result\n\n    async def _next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        req_id = str(next(self.request_counter))\n        prompt = TokensPrompt(prompt_token_ids=token_ids)\n\n        outputs = []\n        processor = PassThroughLogitsProcessor()\n        async for output in self.async_llm_engine.generate(\n            prompt=prompt,\n            sampling_params=SamplingParams(\n                **self.default_params, logits_processors=[processor]\n            ),\n            request_id=req_id,\n        ):\n            if output.finished:\n                outputs.append(output)\n\n        assert processor.log_probs is not None, (\n            \"Log probs should be set by the logits processor.\"\n        )\n        return processor.log_probs\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self.batch_next_token_logprobs_sync([token_ids])[0]\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"\n        Request log probabilities of next tokens in a batch synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n        \"\"\"\n        req_ids = []\n        req_id2processors = {}\n        for token_ids in token_ids_list:\n            req_id = str(next(self.request_counter))\n            req_ids.append(req_id)\n            processor = PassThroughLogitsProcessor()\n            req_id2processors[req_id] = processor\n            self.async_llm_engine.engine.add_request(\n                prompt=TokensPrompt(prompt_token_ids=token_ids),\n                params=SamplingParams(\n                    **self.default_params, logits_processors=[processor]\n                ),\n                request_id=req_id,\n            )\n\n        while self.async_llm_engine.engine.has_unfinished_requests():\n            output = self.async_llm_engine.engine.step()\n            for out in output:\n                if out.finished:\n                    assert out.request_id in req_id2processors, (\n                        f\"{out.request_id} not in requested IDs\"\n                    )\n\n        return torch.stack(\n            [req_id2processors[req_id].log_probs for req_id in req_ids]\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear output cache.\"\"\"\n        if self.cache:\n            self.cache.clear()\n\n    def __del__(self):\n        \"\"\"Clean up resources on deletion.\"\"\"\n        self._cleanup_engine()\n\n    def _cleanup_engine(self):\n        \"\"\"Clean up the vLLM engine and associated resources.\"\"\"\n        if async_engine := getattr(self, \"async_llm_engine\", None):\n            async_engine.shutdown_background_loop()\n            destroy_model_parallel()\n            destroy_distributed_environment()\n\n    async def sample(\n        self,\n        prompt_token_ids,\n        max_tokens,\n        eos_token_ids,\n        temperature=1.0,\n        seed=None,\n    ):\n        \"\"\"Sample from the language model.\n\n        Args:\n            prompt_token_ids (list[int]): The token IDs of the prompt.\n            eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n            temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n            max_tokens (int): The maximum number of tokens to generate.\n            seed (int, optional): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            (list[int]): The sampled token IDs.\n        \"\"\"\n        async for output in self.async_llm_engine.generate(\n            prompt=TokensPrompt(prompt_token_ids=prompt_token_ids),\n            sampling_params=SamplingParams(\n                n=1,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                seed=seed,\n                stop=[self.byte_vocab[i].decode() for i in eos_token_ids],\n            ),\n            request_id=str(next(self.request_counter)),\n        ):\n            if output.finished:\n                assert len(output.outputs) == 1, (\n                    \"Expected exactly one sequence group\"\n                )\n                token_ids = list(output.outputs[0].token_ids)\n                if token_ids[-1] in eos_token_ids:\n                    token_ids = token_ids[:-1]\n                return token_ids\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM.__init__","title":"<code>__init__(async_llm_engine, cache_size=0, cache_opts={})</code>","text":"<p>Initialize an <code>AsyncVirtualLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>async_llm_engine</code> <code>AsyncLLMEngine</code> <p>The async vLLM engine instance.</p> required <code>cache_size</code> <code>int</code> <p>Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.</p> <code>0</code> <code>cache_opts</code> <code>dict</code> <p>Additional options to pass to the <code>OutputCache</code> constructor. Defaults to {}.</p> <code>{}</code> Note <p>The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n    \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n    Args:\n        async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n        cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n        cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm.backend.cache.OutputCache] constructor. Defaults to {}.\n\n    Note:\n        The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n    \"\"\"\n    self.async_llm_engine = async_llm_engine\n    self.tokenizer = async_llm_engine.engine.get_tokenizer()\n    self.request_counter = Counter()\n    self.cache = (\n        OutputCache(maxsize=cache_size, **cache_opts)\n        if cache_size &gt; 0\n        else None\n    )\n\n    async_llm_engine.engine.log_stats = False\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM.from_name","title":"<code>from_name(model_name, engine_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>AsyncVirtualLM</code> instance from a model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>engine_opts</code> <code>dict</code> <p>Additional options to pass to the <code>AsyncLLMEngine</code>. The engine will be configured with prefix caching enabled and async output processing disabled by default.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to <code>AsyncVirtualLM</code> constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncVirtualLM</code> <p>An <code>AsyncVirtualLM</code> instance.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, engine_opts=None, **kwargs):\n    \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n            configured with prefix caching enabled and async output processing disabled by default.\n        **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n    Returns:\n        (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n    \"\"\"\n    if not HAS_VLLM:\n        raise ImportError(  # pragma: no cover\n            \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n        )\n\n    if engine_opts is not None and \"enable_chunked_prefill\" in engine_opts:\n        if engine_opts[\"enable_chunked_prefill\"]:\n            warnings.warn(  # pragma: no cover\n                \"Setting enable_chunked_prefill to True may interfere with AsyncVirtualLM's \"\n                \"custom sampling functionality.\"\n            )\n\n    engine_opts = {\n        \"enable_prefix_caching\": True,\n        \"disable_log_requests\": True,\n        \"disable_async_output_proc\": True,  # This parameter forces vLLM to use v0, which is currently what we want to do.\n        **(engine_opts or {}),\n    }\n\n    engine = AsyncLLMEngine.from_engine_args(\n        AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n    )\n\n    return cls(engine, **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token asynchronously with output caching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>Tensor</code> <p>Normalized log probability tensor.</p> Warning <p>Do not use <code>asyncio.run(next_token_logprobs())</code> as it may interfere with vLLM's background loop. For synchronous usage, use the <code>next_token_logprobs_sync()</code> method instead.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        result (torch.Tensor): Normalized log probability tensor.\n\n    Warning:\n        Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n        For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n    \"\"\"\n    key = tuple(token_ids)\n\n    if self.cache is not None and key in self.cache:\n        return self.cache[key]\n\n    result = await self._next_token_logprobs(key)\n\n    if self.cache is not None:\n        self.cache[key] = result\n\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self.batch_next_token_logprobs_sync([token_ids])[0]\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Request log probabilities of next tokens in a batch synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists, each representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of normalized log probability tensors, one for each prompt in the input list.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"\n    Request log probabilities of next tokens in a batch synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n    \"\"\"\n    req_ids = []\n    req_id2processors = {}\n    for token_ids in token_ids_list:\n        req_id = str(next(self.request_counter))\n        req_ids.append(req_id)\n        processor = PassThroughLogitsProcessor()\n        req_id2processors[req_id] = processor\n        self.async_llm_engine.engine.add_request(\n            prompt=TokensPrompt(prompt_token_ids=token_ids),\n            params=SamplingParams(\n                **self.default_params, logits_processors=[processor]\n            ),\n            request_id=req_id,\n        )\n\n    while self.async_llm_engine.engine.has_unfinished_requests():\n        output = self.async_llm_engine.engine.step()\n        for out in output:\n            if out.finished:\n                assert out.request_id in req_id2processors, (\n                    f\"{out.request_id} not in requested IDs\"\n                )\n\n    return torch.stack(\n        [req_id2processors[req_id].log_probs for req_id in req_ids]\n    )\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear output cache.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear output cache.\"\"\"\n    if self.cache:\n        self.cache.clear()\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM.__del__","title":"<code>__del__()</code>","text":"<p>Clean up resources on deletion.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up resources on deletion.\"\"\"\n    self._cleanup_engine()\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncVirtualLM.sample","title":"<code>sample(prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids</code> <code>list[int]</code> <p>The token IDs of the prompt.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs of the end-of-sequence tokens.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use to rescale the logits. Defaults to 1.0.</p> <code>1.0</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>async def sample(\n    self,\n    prompt_token_ids,\n    max_tokens,\n    eos_token_ids,\n    temperature=1.0,\n    seed=None,\n):\n    \"\"\"Sample from the language model.\n\n    Args:\n        prompt_token_ids (list[int]): The token IDs of the prompt.\n        eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n        temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n        max_tokens (int): The maximum number of tokens to generate.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        (list[int]): The sampled token IDs.\n    \"\"\"\n    async for output in self.async_llm_engine.generate(\n        prompt=TokensPrompt(prompt_token_ids=prompt_token_ids),\n        sampling_params=SamplingParams(\n            n=1,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            seed=seed,\n            stop=[self.byte_vocab[i].decode() for i in eos_token_ids],\n        ),\n        request_id=str(next(self.request_counter)),\n    ):\n        if output.finished:\n            assert len(output.outputs) == 1, (\n                \"Expected exactly one sequence group\"\n            )\n            token_ids = list(output.outputs[0].token_ids)\n            if token_ids[-1] in eos_token_ids:\n                token_ids = token_ids[:-1]\n            return token_ids\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer","title":"<code>AsyncTransformer</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Asynchronous wrapper around a HuggingFace causal language model with caching support.</p> <p>This class provides an asynchronous interface to HuggingFace language models with automatic batching and caching (output and KV) for improved efficiency.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>class AsyncTransformer(AsyncLM):\n    \"\"\"Asynchronous wrapper around a HuggingFace causal language model with caching support.\n\n    This class provides an asynchronous interface to HuggingFace language models with automatic batching\n    and caching (output and KV) for improved efficiency.\n    \"\"\"\n\n    @classmethod\n    def from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n        \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n        Args:\n            model_id (str): Model identifier in HuggingFace's model hub.\n            bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n                Defaults to None.\n            hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n                Defaults to None.\n            **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n        Returns:\n            (AsyncTransformer): An initialized `AsyncTransformer` instance.\n        \"\"\"\n        if bitsandbytes_opts:\n            bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n        else:\n            bnb_config = None\n\n        _hf_opts = {\n            \"device_map\": \"auto\",\n            \"torch_dtype\": \"auto\",\n        }\n        if hf_opts:\n            _hf_opts.update(hf_opts)\n\n        tok = AutoTokenizer.from_pretrained(model_id)\n        mod = AutoModelForCausalLM.from_pretrained(\n            model_id, quantization_config=bnb_config, **_hf_opts\n        )\n\n        return cls(mod, tok, **kwargs)\n\n    @torch.no_grad()\n    def __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n        \"\"\"Initialize an AsyncTransformer instance.\n\n        Args:\n            hf_model: A HuggingFace CausalLM model instance.\n            hf_tokenizer: A HuggingFace Tokenizer.\n            batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n                Defaults to 20.\n            timeout (float, optional): Seconds to wait since last query before processing current batch.\n                Defaults to 0.02.\n        \"\"\"\n        self.model = hf_model\n        self.tokenizer = hf_tokenizer\n        self.device = hf_model.device\n        self.cache = TokenTrie()\n\n        # Queries to be batched. Each query is a sequence of tokens,\n        # and a Future to be called when the query is resolved.\n        self.queries = []\n        self.batch_size = batch_size\n        self.timeout = timeout\n        self.timer = None\n\n        self.model.eval()\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    def clear_cache(self):\n        \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n        self.cache = TokenTrie()\n\n    def clear_kv_cache(self):\n        \"\"\"Clear any key and value vectors from the cache.\"\"\"\n        self.cache.clear_kv_cache()\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n        to completion.\"\"\"\n        self.queries = []\n\n    @torch.no_grad()\n    def cache_kv(self, prompt_tokens):\n        \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n        Args:\n            prompt_tokens (list[int]): token ids for the prompt to cache.\n        \"\"\"\n        result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n        node = self.cache.extend_cache(0, prompt_tokens, result.logits[0], 0)\n        node.past_key_values = result.past_key_values\n\n    @torch.no_grad()\n    def batch_evaluate_queries(self):\n        \"\"\"\n        Process a batch of queued language model queries.\n\n        This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n        \"\"\"\n\n        queries, self.queries = self.queries, []\n        if len(queries) == 0:\n            return\n\n        query_groups = defaultdict(list)\n        for query in queries:\n            key = tuple(query.prompt)  # XXX: cache based on past_len too?\n            query_groups[key].append(query)\n\n        # Use one representative query from each group\n        unique_queries = [group[0] for group in query_groups.values()]\n\n        past_example = next((q.past for q in unique_queries if q.past), False)\n        max_past_length = max(q.past_len for q in unique_queries)\n        max_query_length = max(len(q.prompt) for q in unique_queries)\n\n        padding_token_id = (\n            self.tokenizer.pad_token_id\n            if self.tokenizer.pad_token_id is not None\n            else 0\n        )\n\n        input_ids = torch.tensor(\n            [\n                q.prompt_padded(padding_token_id, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        attn_masks = torch.tensor(\n            [\n                q.attention_mask(max_past_length, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        posn_ids = torch.tensor(\n            [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n        ).to(self.device)\n        if past_example:\n            pasts = [\n                [\n                    torch.cat(\n                        (\n                            *(\n                                q.past_padded(\n                                    layer,\n                                    j,\n                                    max_past_length,\n                                    past_example[0][0].dtype,\n                                    self.device,\n                                    past_example[0][0].shape,\n                                )\n                                for q in unique_queries\n                            ),\n                        ),\n                        dim=0,\n                    )\n                    for j in range(2)\n                ]\n                for layer in range(len(past_example))\n            ]\n        else:\n            pasts = None\n\n        pasts = DynamicCache.from_legacy_cache(pasts)\n\n        results = self.model(\n            input_ids,\n            attention_mask=attn_masks,\n            position_ids=posn_ids,\n            past_key_values=pasts,\n            use_cache=pasts is not None,\n        )\n\n        assert len(results.logits) == len(unique_queries)\n\n        for i, q in enumerate(unique_queries):\n            result = results.logits[i]\n            for dup_query in query_groups[tuple(q.prompt)]:\n                dup_query.future.set_result(result)\n\n    @torch.no_grad()\n    def add_query(self, query, future, past):\n        \"\"\"Add a query to be evaluated in the next batch.\n\n        This method is called internally when a `next_token_logprobs` request is made.\n\n        Args:\n            query (list[int]): Token IDs representing the query prompt\n            future (asyncio.Future): Future to store the result in\n            past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n                or None if this is a new query\n        \"\"\"\n        self.queries.append(Query(query, future, past))\n\n        if self.timer:\n            self.timer.cancel()\n            self.timer = None\n        if len(self.queries) &gt;= self.batch_size:\n            self.batch_evaluate_queries()\n        else:\n            self.timer = asyncio.get_running_loop().call_later(\n                self.timeout, lambda: self.batch_evaluate_queries()\n            )\n\n    def walk_cache(self, token_ids):\n        \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n        Args:\n            token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n        Returns:\n            tuple:\n                - CacheNode: The deepest node in the cache tree that matches the token sequence\n                - int: Number of tokens matched from the start of token_ids\n                - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                    or None if no cached states were found\n                - int: Base index indicating where the past states start in token_ids\n        \"\"\"\n        # Walk while tokens can be found\n        node = self.cache\n        next_token_index = 0\n\n        past = None\n        base = 0\n        while next_token_index &lt; len(token_ids):\n            if node.past_key_values is not None:\n                past = node.past_key_values\n                base = next_token_index\n            if node.has_token(token_ids[next_token_index]):\n                node = node.get_token(token_ids[next_token_index])\n                next_token_index += 1\n            else:\n                break\n\n        return node, next_token_index, past, base\n\n    @torch.no_grad()\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        # If we processed all tokens, then we're done.\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        # Create a future with the prompt\n        future = asyncio.get_running_loop().create_future()\n        self.add_query(token_ids[base:], future, past)\n        logits = await future\n\n        # Create new nodes\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    @torch.no_grad()\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        # Walk while tokens can be found\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        logits = self.model(\n            torch.tensor([token_ids[base:]]).to(self.device),\n            past_key_values=node.past_key_values,\n            use_cache=node.past_key_values is not None,\n        ).logits[0]\n\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    def next_token_logprobs_uncached(self, token_ids):\n        \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        with torch.no_grad():\n            logits = self.model(\n                torch.tensor([token_ids]).to(self.device),\n                past_key_values=None,\n                use_cache=False,\n            ).logits[0]\n            return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.from_name","title":"<code>from_name(model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an AsyncTransformer instance from a pretrained HuggingFace model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Model identifier in HuggingFace's model hub.</p> required <code>bitsandbytes_opts</code> <code>dict</code> <p>Additional configuration options for bitsandbytes quantization. Defaults to None.</p> <code>None</code> <code>hf_opts</code> <code>dict</code> <p>Additional configuration options for loading the HuggingFace model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the <code>AsyncTransformer</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTransformer</code> <p>An initialized <code>AsyncTransformer</code> instance.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@classmethod\ndef from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n    \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n    Args:\n        model_id (str): Model identifier in HuggingFace's model hub.\n        bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n            Defaults to None.\n        hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n            Defaults to None.\n        **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n    Returns:\n        (AsyncTransformer): An initialized `AsyncTransformer` instance.\n    \"\"\"\n    if bitsandbytes_opts:\n        bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n    else:\n        bnb_config = None\n\n    _hf_opts = {\n        \"device_map\": \"auto\",\n        \"torch_dtype\": \"auto\",\n    }\n    if hf_opts:\n        _hf_opts.update(hf_opts)\n\n    tok = AutoTokenizer.from_pretrained(model_id)\n    mod = AutoModelForCausalLM.from_pretrained(\n        model_id, quantization_config=bnb_config, **_hf_opts\n    )\n\n    return cls(mod, tok, **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.__init__","title":"<code>__init__(hf_model, hf_tokenizer, batch_size=20, timeout=0.02)</code>","text":"<p>Initialize an AsyncTransformer instance.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <p>A HuggingFace CausalLM model instance.</p> required <code>hf_tokenizer</code> <p>A HuggingFace Tokenizer.</p> required <code>batch_size</code> <code>int</code> <p>Maximum queries to process in one batch during auto-batching. Defaults to 20.</p> <code>20</code> <code>timeout</code> <code>float</code> <p>Seconds to wait since last query before processing current batch. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n    \"\"\"Initialize an AsyncTransformer instance.\n\n    Args:\n        hf_model: A HuggingFace CausalLM model instance.\n        hf_tokenizer: A HuggingFace Tokenizer.\n        batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n            Defaults to 20.\n        timeout (float, optional): Seconds to wait since last query before processing current batch.\n            Defaults to 0.02.\n    \"\"\"\n    self.model = hf_model\n    self.tokenizer = hf_tokenizer\n    self.device = hf_model.device\n    self.cache = TokenTrie()\n\n    # Queries to be batched. Each query is a sequence of tokens,\n    # and a Future to be called when the query is resolved.\n    self.queries = []\n    self.batch_size = batch_size\n    self.timeout = timeout\n    self.timer = None\n\n    self.model.eval()\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache of log probabilities and key/value pairs.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n    self.cache = TokenTrie()\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.clear_kv_cache","title":"<code>clear_kv_cache()</code>","text":"<p>Clear any key and value vectors from the cache.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def clear_kv_cache(self):\n    \"\"\"Clear any key and value vectors from the cache.\"\"\"\n    self.cache.clear_kv_cache()\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing to completion.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n    to completion.\"\"\"\n    self.queries = []\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.cache_kv","title":"<code>cache_kv(prompt_tokens)</code>","text":"<p>Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_tokens</code> <code>list[int]</code> <p>token ids for the prompt to cache.</p> required Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef cache_kv(self, prompt_tokens):\n    \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n    Args:\n        prompt_tokens (list[int]): token ids for the prompt to cache.\n    \"\"\"\n    result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n    node = self.cache.extend_cache(0, prompt_tokens, result.logits[0], 0)\n    node.past_key_values = result.past_key_values\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.batch_evaluate_queries","title":"<code>batch_evaluate_queries()</code>","text":"<p>Process a batch of queued language model queries.</p> <p>This method is called internally when the <code>batch_size</code> has been met or the <code>timeout</code> has expired.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef batch_evaluate_queries(self):\n    \"\"\"\n    Process a batch of queued language model queries.\n\n    This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n    \"\"\"\n\n    queries, self.queries = self.queries, []\n    if len(queries) == 0:\n        return\n\n    query_groups = defaultdict(list)\n    for query in queries:\n        key = tuple(query.prompt)  # XXX: cache based on past_len too?\n        query_groups[key].append(query)\n\n    # Use one representative query from each group\n    unique_queries = [group[0] for group in query_groups.values()]\n\n    past_example = next((q.past for q in unique_queries if q.past), False)\n    max_past_length = max(q.past_len for q in unique_queries)\n    max_query_length = max(len(q.prompt) for q in unique_queries)\n\n    padding_token_id = (\n        self.tokenizer.pad_token_id\n        if self.tokenizer.pad_token_id is not None\n        else 0\n    )\n\n    input_ids = torch.tensor(\n        [\n            q.prompt_padded(padding_token_id, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    attn_masks = torch.tensor(\n        [\n            q.attention_mask(max_past_length, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    posn_ids = torch.tensor(\n        [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n    ).to(self.device)\n    if past_example:\n        pasts = [\n            [\n                torch.cat(\n                    (\n                        *(\n                            q.past_padded(\n                                layer,\n                                j,\n                                max_past_length,\n                                past_example[0][0].dtype,\n                                self.device,\n                                past_example[0][0].shape,\n                            )\n                            for q in unique_queries\n                        ),\n                    ),\n                    dim=0,\n                )\n                for j in range(2)\n            ]\n            for layer in range(len(past_example))\n        ]\n    else:\n        pasts = None\n\n    pasts = DynamicCache.from_legacy_cache(pasts)\n\n    results = self.model(\n        input_ids,\n        attention_mask=attn_masks,\n        position_ids=posn_ids,\n        past_key_values=pasts,\n        use_cache=pasts is not None,\n    )\n\n    assert len(results.logits) == len(unique_queries)\n\n    for i, q in enumerate(unique_queries):\n        result = results.logits[i]\n        for dup_query in query_groups[tuple(q.prompt)]:\n            dup_query.future.set_result(result)\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.add_query","title":"<code>add_query(query, future, past)</code>","text":"<p>Add a query to be evaluated in the next batch.</p> <p>This method is called internally when a <code>next_token_logprobs</code> request is made.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[int]</code> <p>Token IDs representing the query prompt</p> required <code>future</code> <code>Future</code> <p>Future to store the result in</p> required <code>past</code> <code>list[tuple[Tensor]] | None</code> <p>Past key/value states from previous evaluation, or None if this is a new query</p> required Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef add_query(self, query, future, past):\n    \"\"\"Add a query to be evaluated in the next batch.\n\n    This method is called internally when a `next_token_logprobs` request is made.\n\n    Args:\n        query (list[int]): Token IDs representing the query prompt\n        future (asyncio.Future): Future to store the result in\n        past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n            or None if this is a new query\n    \"\"\"\n    self.queries.append(Query(query, future, past))\n\n    if self.timer:\n        self.timer.cancel()\n        self.timer = None\n    if len(self.queries) &gt;= self.batch_size:\n        self.batch_evaluate_queries()\n    else:\n        self.timer = asyncio.get_running_loop().call_later(\n            self.timeout, lambda: self.batch_evaluate_queries()\n        )\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.walk_cache","title":"<code>walk_cache(token_ids)</code>","text":"<p>Walk the cache tree to find the deepest node matching a sequence of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Sequence of token IDs to follow in the cache tree</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>CacheNode: The deepest node in the cache tree that matches the token sequence</li> <li>int: Number of tokens matched from the start of token_ids</li> <li>list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,     or None if no cached states were found</li> <li>int: Base index indicating where the past states start in token_ids</li> </ul> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def walk_cache(self, token_ids):\n    \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n    Args:\n        token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n    Returns:\n        tuple:\n            - CacheNode: The deepest node in the cache tree that matches the token sequence\n            - int: Number of tokens matched from the start of token_ids\n            - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                or None if no cached states were found\n            - int: Base index indicating where the past states start in token_ids\n    \"\"\"\n    # Walk while tokens can be found\n    node = self.cache\n    next_token_index = 0\n\n    past = None\n    base = 0\n    while next_token_index &lt; len(token_ids):\n        if node.past_key_values is not None:\n            past = node.past_key_values\n            base = next_token_index\n        if node.has_token(token_ids[next_token_index]):\n            node = node.get_token(token_ids[next_token_index])\n            next_token_index += 1\n        else:\n            break\n\n    return node, next_token_index, past, base\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    # If we processed all tokens, then we're done.\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    # Create a future with the prompt\n    future = asyncio.get_running_loop().create_future()\n    self.add_query(token_ids[base:], future, past)\n    logits = await future\n\n    # Create new nodes\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token. Not asynchronous, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    # Walk while tokens can be found\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    logits = self.model(\n        torch.tensor([token_ids[base:]]).to(self.device),\n        past_key_values=node.past_key_values,\n        use_cache=node.past_key_values is not None,\n    ).logits[0]\n\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTransformer.next_token_logprobs_uncached","title":"<code>next_token_logprobs_uncached(token_ids)</code>","text":"<p>Request log probabilities of next token. No KV or output caching, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def next_token_logprobs_uncached(self, token_ids):\n    \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    with torch.no_grad():\n        logits = self.model(\n            torch.tensor([token_ids]).to(self.device),\n            past_key_values=None,\n            use_cache=False,\n        ).logits[0]\n        return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.load_model_by_name","title":"<code>load_model_by_name(name, backend=None, llm_opts=None)</code>","text":"<p>Load a language model by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Hugging Face model name (e.g. \"gpt2\", \"meta-llama/Llama-3.2-1B-Instruct\")</p> required <code>backend</code> <code>str</code> <p>Backend to use for inference. Can be \"vllm\", \"hf\" or \"mock\". If None, defaults to \"vllm\" if CUDA is available, otherwise \"hf\".</p> <code>None</code> <code>llm_opts</code> <code>dict</code> <p>Additional options to pass to the backend constructor. See AsyncVirtualLM and AsyncTransformer documentation for details.</p> <code>None</code> <p>Returns:</p> Type Description <code>AsyncLM</code> <p>An asynchronous language model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid backend is specified.</p> Source code in <code>genlm/backend/llm/__init__.py</code> <pre><code>def load_model_by_name(name, backend=None, llm_opts=None):\n    \"\"\"Load a language model by name.\n\n    Args:\n        name (str): Hugging Face model name (e.g. \"gpt2\", \"meta-llama/Llama-3.2-1B-Instruct\")\n        backend (str, optional): Backend to use for inference. Can be \"vllm\", \"hf\" or \"mock\".\n            If None, defaults to \"vllm\" if CUDA is available, otherwise \"hf\".\n        llm_opts (dict, optional): Additional options to pass to the backend constructor.\n            See AsyncVirtualLM and AsyncTransformer documentation for details.\n\n    Returns:\n        (AsyncLM): An asynchronous language model.\n\n    Raises:\n        (ValueError): If an invalid backend is specified.\n    \"\"\"\n    if backend is None:\n        backend = \"vllm\" if torch.cuda.is_available() else \"hf\"\n\n    if llm_opts is None:\n        llm_opts = {}\n\n    if backend == \"vllm\":\n        return AsyncVirtualLM.from_name(name, **llm_opts)\n    elif backend == \"hf\":\n        return AsyncTransformer.from_name(name, **llm_opts)\n    elif backend == \"mock\":\n        return MockAsyncLM.from_name(name, **llm_opts)\n    elif backend == \"mlx\":\n        return AsyncMlxLM.from_name(name, **llm_opts)\n    else:\n        raise ValueError(f\"Invalid backend: {backend}\")\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.decode_vocab","title":"<code>decode_vocab(tokenizer, byte2str_fallback='tokenizer')</code>","text":"<p>Convert tokenizer vocabulary into byte and string representations.</p> Warning <p>The byte representation is the canonical form. The string representation is provided for convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte2str_fallback</code> <code>str</code> <p>Strategy for converting invalid UTF-8 bytes to strings. Options:</p> <ul> <li>'tokenizer': Use tokenizer's <code>convert_ids_to_tokens</code> (default)</li> <li>'latin1': Decode using latin1 encoding</li> <li>'replace': Use Unicode replacement character '\ufffd'</li> </ul> <code>'tokenizer'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(byte_vocab, str_vocab)</p> Source code in <code>genlm/backend/tokenization/vocab.py</code> <pre><code>def decode_vocab(tokenizer, byte2str_fallback=\"tokenizer\"):\n    \"\"\"Convert tokenizer vocabulary into byte and string representations.\n\n    Warning:\n        The byte representation is the canonical form. The string representation is provided for\n        convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte2str_fallback (str): Strategy for converting invalid UTF-8 bytes to strings. Options:\\n\n            - 'tokenizer': Use tokenizer's `convert_ids_to_tokens` (default)\n            - 'latin1': Decode using latin1 encoding\n            - 'replace': Use Unicode replacement character '\ufffd'\n\n    Returns:\n        (tuple): (byte_vocab, str_vocab)\n    \"\"\"\n    if byte2str_fallback not in [\"latin1\", \"tokenizer\", \"replace\"]:\n        raise ValueError(f\"Unknown byte2str_fallback strategy: {byte2str_fallback}\")\n\n    if tokenizer.is_fast:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer.name_or_path, use_fast=False\n        )\n\n    # Try slow tokenizer.\n    try:\n        byte_vocab = get_byte_vocab(tokenizer)\n    except ByteVocabError:\n        # warnings.warn(\"Could not decode vocabulary from slow tokenizer. Trying using fast tokenizer.\")\n\n        # Try fast tokenizer.\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer.name_or_path, use_fast=True)\n        try:\n            byte_vocab = get_byte_vocab(tokenizer)\n        except ByteVocabError as e:\n            raise ValueError(\n                f\"Could not decode byte representation of token vocabuary from tokenizer {tokenizer.name_or_path}\"\n            ) from e\n\n    str_vocab = bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback)\n\n    return byte_vocab, str_vocab\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.TokenCharacterTrie","title":"<code>TokenCharacterTrie</code>","text":"<p>A trie data structure for efficient token-to-character mapping.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>class TokenCharacterTrie:\n    \"\"\"A trie data structure for efficient token-to-character mapping.\"\"\"\n\n    def __init__(self, decode):\n        \"\"\"Initialize a `TokenCharacterTrie`.\n\n        Args:\n            decode (list): List representing the token vocabulary.\n                Each element of the list must be iterable.\n        \"\"\"\n        self.decode = decode\n        self.word2leaf = {}\n        self.children = [{}]  # First node is root\n        self.root = 0\n        self.token_id_to_leaf = []\n\n        for token_id, word in enumerate(self.decode):\n            curr = self.root\n            for letter in word:\n                if letter not in self.children[curr]:\n                    self.children[curr][letter] = len(self.children)\n                    self.children.append({})\n                curr = self.children[curr][letter]\n\n            self.children[curr][None] = last = len(self.children)\n            self.children.append({})\n            assert word not in self.word2leaf, (\n                \"Can't have duplicate words in vocabulary\"\n            )\n            self.word2leaf[word] = last\n\n            self.token_id_to_leaf.append((token_id, last))\n\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n        )\n        self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n        # Renumber the states of the trie so that they are named by a contiguous\n        # range of integers and those integers respect the are topologically\n        # ordering of the trie topology.  This improves the efficiency of the\n        # updating the trie as it improves memory locality.\n        ordering = {}\n        for i, x in enumerate(self._order_full(self.root)):\n            ordering[x] = i\n        self._rename(f=lambda x: ordering[x])\n\n        node2prefix = {self.root: []}\n        for x in reversed(range(len(self.children))):\n            for letter, y in self.children[x].items():\n                if letter is None:\n                    node2prefix[y] = node2prefix[x]\n                else:\n                    node2prefix[y] = node2prefix[x] + [letter]\n        self.node2prefix = node2prefix\n\n    def _rename(self, f):\n        \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n        Args:\n            f (callable): Function that maps old node indices to new node indices\n        \"\"\"\n        N = len(self.children)\n\n        new_children = [{} for _ in range(N)]\n        nodes = range(N)\n\n        for x in nodes:\n            for letter, y in self.children[x].items():\n                new_children[f(x)][letter] = f(y)\n\n        self.root = f(self.root)\n        self.children = new_children\n        self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n        self.token_id_to_leaf = np.array(\n            [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n        )\n\n        self.ordering = np.array([f(x) for x in self.ordering])\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n        )\n\n    def _alloc_weights(self):\n        \"\"\"Allocate an array to store weight values for all nodes.\n\n        Returns:\n            np.ndarray: Zero-initialized array for storing weight values\n        \"\"\"\n        return np.zeros(len(self.children), dtype=np.float64)\n\n    def _preprocess_ws(self, ws):\n        \"\"\"Preprocess the weight vector to ensure it is a numpy array and on the correct device.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight vector\n        \"\"\"\n        if isinstance(ws, torch.Tensor):\n            if ws.device.type != \"cpu\":\n                ws = ws.cpu()\n            ws = ws.numpy()\n        return ws\n\n    def weight_sum(self, ws):\n        \"\"\"Compute weight sum for each node in the trie.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Summed weights for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_sum(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def weight_max(self, ws):\n        \"\"\"Compute weight max for each node in the trie.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight max values for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_max(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batched equivalent of `weight_sum`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_sum(ws) for ws in ws])\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batched equivalent of `weight_max`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_max(ws) for ws in ws])\n\n    def _order(self, node):\n        \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            int: Node indices in topological order\n        \"\"\"\n        for a in self.children[node]:\n            if a is None:\n                pass\n            else:\n                yield from self._order(self.children[node][a])\n        yield node\n\n    def _order_full(self, node):\n        \"\"\"Generate a complete topological ordering including all child nodes.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            (int): Node indices in complete topological order\n        \"\"\"\n        for a in self.children[node]:\n            yield from self._order_full(self.children[node][a])\n        yield node\n\n    def visualize(self, ws=None):\n        \"\"\"Visualize the trie structure using Graphviz.\n\n        Args:\n            ws (np.ndarray|None): Optional weight vector to display at each node.\n                                Should be of length `len(self.children)`.\n\n        Returns:\n            (graphviz.Digraph): The generated graph object\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:  # pragma: no cover\n            raise ImportError(\n                \"Please install graphviz: pip install graphviz\"\n            )  # pragma: no cover\n\n        if ws is not None and len(ws) != len(self.children):\n            raise ValueError(\n                f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n            )\n\n        dot = graphviz.Digraph(comment=\"Token Character Trie\")\n        dot.attr(rankdir=\"LR\")\n\n        # Create a subgraph for the legend\n        with dot.subgraph(name=\"cluster_legend\") as legend:\n            legend.attr(label=\"Legend\", fontsize=\"10\")\n            legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n            # Example internal node\n            legend.node(\n                \"legend_internal\",\n                \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n                shape=\"circle\",\n            )\n\n            # Example leaf node\n            legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n            legend.edge(\n                \"legend_internal\",\n                \"legend_leaf\",\n                label=\"Token item\",\n                fontsize=\"10\",\n            )\n\n            # Align legend horizontally\n            legend.attr(rankdir=\"TB\")\n            legend.attr(rank=\"same\")\n\n        # Add the main trie nodes and edges\n        for node_id in range(len(self.children)):\n            prefix = self.node2prefix[node_id]\n\n            if ws is not None:\n                label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n            else:\n                label = f\"{node_id}\\n'{prefix}'\"\n\n            # Color nodes based on mass if provided\n            if ws is not None:\n                max_ws = ws.max()\n                if max_ws &gt; 0:\n                    intensity = int(255 * (1 - ws[node_id] / max_ws))\n                    color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n                else:\n                    color = \"#ffffff\"  # white for zero mass\n            else:\n                color = \"#ffffff\"  # default white\n\n            if node_id in self.leaf2word:\n                dot.node(\n                    str(node_id),\n                    label,\n                    shape=\"doublecircle\",\n                    style=\"filled\",\n                    fillcolor=color,\n                )\n            else:\n                dot.node(\n                    str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n                )\n\n        for node_id, children in enumerate(self.children):\n            for char, child_id in children.items():\n                if char is not None:\n                    edge_label = str(char)\n                else:\n                    edge_label = \"End-of-Token\"\n\n                dot.edge(str(node_id), str(child_id), label=edge_label)\n\n        return dot\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.TokenCharacterTrie.__init__","title":"<code>__init__(decode)</code>","text":"<p>Initialize a <code>TokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>decode</code> <code>list</code> <p>List representing the token vocabulary. Each element of the list must be iterable.</p> required Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def __init__(self, decode):\n    \"\"\"Initialize a `TokenCharacterTrie`.\n\n    Args:\n        decode (list): List representing the token vocabulary.\n            Each element of the list must be iterable.\n    \"\"\"\n    self.decode = decode\n    self.word2leaf = {}\n    self.children = [{}]  # First node is root\n    self.root = 0\n    self.token_id_to_leaf = []\n\n    for token_id, word in enumerate(self.decode):\n        curr = self.root\n        for letter in word:\n            if letter not in self.children[curr]:\n                self.children[curr][letter] = len(self.children)\n                self.children.append({})\n            curr = self.children[curr][letter]\n\n        self.children[curr][None] = last = len(self.children)\n        self.children.append({})\n        assert word not in self.word2leaf, (\n            \"Can't have duplicate words in vocabulary\"\n        )\n        self.word2leaf[word] = last\n\n        self.token_id_to_leaf.append((token_id, last))\n\n    self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n    self.jump = List(\n        [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n    )\n    self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n    # Renumber the states of the trie so that they are named by a contiguous\n    # range of integers and those integers respect the are topologically\n    # ordering of the trie topology.  This improves the efficiency of the\n    # updating the trie as it improves memory locality.\n    ordering = {}\n    for i, x in enumerate(self._order_full(self.root)):\n        ordering[x] = i\n    self._rename(f=lambda x: ordering[x])\n\n    node2prefix = {self.root: []}\n    for x in reversed(range(len(self.children))):\n        for letter, y in self.children[x].items():\n            if letter is None:\n                node2prefix[y] = node2prefix[x]\n            else:\n                node2prefix[y] = node2prefix[x] + [letter]\n    self.node2prefix = node2prefix\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.TokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Compute weight sum for each node in the trie.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Compute weight sum for each node in the trie.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Summed weights for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_sum(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.TokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Compute weight max for each node in the trie.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Weight max values for each node in the trie.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Compute weight max for each node in the trie.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Weight max values for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_max(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.TokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batched equivalent of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batched equivalent of `weight_sum`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_sum(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.TokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batched equivalent of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight max values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batched equivalent of `weight_max`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_max(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.TokenCharacterTrie.visualize","title":"<code>visualize(ws=None)</code>","text":"<p>Visualize the trie structure using Graphviz.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>ndarray | None</code> <p>Optional weight vector to display at each node.                 Should be of length <code>len(self.children)</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Digraph</code> <p>The generated graph object</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def visualize(self, ws=None):\n    \"\"\"Visualize the trie structure using Graphviz.\n\n    Args:\n        ws (np.ndarray|None): Optional weight vector to display at each node.\n                            Should be of length `len(self.children)`.\n\n    Returns:\n        (graphviz.Digraph): The generated graph object\n    \"\"\"\n    try:\n        import graphviz\n    except ImportError:  # pragma: no cover\n        raise ImportError(\n            \"Please install graphviz: pip install graphviz\"\n        )  # pragma: no cover\n\n    if ws is not None and len(ws) != len(self.children):\n        raise ValueError(\n            f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n        )\n\n    dot = graphviz.Digraph(comment=\"Token Character Trie\")\n    dot.attr(rankdir=\"LR\")\n\n    # Create a subgraph for the legend\n    with dot.subgraph(name=\"cluster_legend\") as legend:\n        legend.attr(label=\"Legend\", fontsize=\"10\")\n        legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n        # Example internal node\n        legend.node(\n            \"legend_internal\",\n            \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n            shape=\"circle\",\n        )\n\n        # Example leaf node\n        legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n        legend.edge(\n            \"legend_internal\",\n            \"legend_leaf\",\n            label=\"Token item\",\n            fontsize=\"10\",\n        )\n\n        # Align legend horizontally\n        legend.attr(rankdir=\"TB\")\n        legend.attr(rank=\"same\")\n\n    # Add the main trie nodes and edges\n    for node_id in range(len(self.children)):\n        prefix = self.node2prefix[node_id]\n\n        if ws is not None:\n            label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n        else:\n            label = f\"{node_id}\\n'{prefix}'\"\n\n        # Color nodes based on mass if provided\n        if ws is not None:\n            max_ws = ws.max()\n            if max_ws &gt; 0:\n                intensity = int(255 * (1 - ws[node_id] / max_ws))\n                color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n            else:\n                color = \"#ffffff\"  # white for zero mass\n        else:\n            color = \"#ffffff\"  # default white\n\n        if node_id in self.leaf2word:\n            dot.node(\n                str(node_id),\n                label,\n                shape=\"doublecircle\",\n                style=\"filled\",\n                fillcolor=color,\n            )\n        else:\n            dot.node(\n                str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n            )\n\n    for node_id, children in enumerate(self.children):\n        for char, child_id in children.items():\n            if char is not None:\n                edge_label = str(char)\n            else:\n                edge_label = \"End-of-Token\"\n\n            dot.edge(str(node_id), str(child_id), label=edge_label)\n\n    return dot\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.ParallelTokenCharacterTrie","title":"<code>ParallelTokenCharacterTrie</code>","text":"<p>               Bases: <code>TokenCharacterTrie</code></p> <p>A GPU-optimized version of <code>TokenCharacterTrie</code> that performs weight sum and max operations in parallel.</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>class ParallelTokenCharacterTrie(TokenCharacterTrie):\n    \"\"\"A GPU-optimized version of `TokenCharacterTrie` that performs weight sum and max operations in parallel.\"\"\"\n\n    def __init__(self, decode, device=None, **kwargs):\n        super().__init__(decode, **kwargs)\n\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if self.device not in [\"cpu\", \"cuda\"]:\n            raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n        self._build_reachability_matrix()\n        self.token_ids = torch.tensor(\n            self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n        )\n\n    def _build_parent_map(self):\n        \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n        Returns:\n            (dict): A dictionary where keys are child nodes and values are their parent nodes.\n        \"\"\"\n        parent = {}\n        for node in range(len(self.children)):\n            for child in self.jump[node]:\n                parent[child] = node\n        return parent\n\n    def _build_reachability_matrix(self):\n        \"\"\"Constructs a sparse reachability matrix for efficient weight propagation.\n\n        The matrix M is constructed such that M[i,j] = 1 if node j is either:\n        - The leaf node i itself (self-connection)\n        - An ancestor of leaf node i in the trie\n        \"\"\"\n        leaf_indices = self.token_id_to_leaf[:, 1]\n        parent = self._build_parent_map()\n\n        rows, cols = [], []\n        for i, node in enumerate(leaf_indices):\n            # self connections\n            rows.append(i)\n            cols.append(node)\n\n            current = node\n            while current in parent:  # Walk up to root\n                ancestor = parent[current]\n                rows.append(i)\n                cols.append(ancestor)\n                current = ancestor\n\n        self.src_indices = torch.tensor(rows, dtype=torch.long, device=self.device)\n        self.dst_indices = torch.tensor(cols, dtype=torch.long, device=self.device)\n\n        indices = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n        values = torch.ones(len(rows), device=self.device)\n\n        self.M = torch.sparse_coo_tensor(\n            indices, values, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n    def _preprocess_ws(self, batch_ws):\n        processed_batch_ws = []\n        for ws in batch_ws:\n            if not isinstance(ws, torch.Tensor):\n                ws = torch.tensor(ws, device=self.device, dtype=torch.float32)\n            elif ws.device != self.device or ws.dtype != torch.float32:\n                ws = ws.to(device=self.device, dtype=torch.float32)\n            assert ws.shape[0] == len(self.decode), [ws.shape[0], len(self.decode)]\n            processed_batch_ws.append(ws)\n        return torch.stack(processed_batch_ws)\n\n    def weight_sum(self, ws):\n        \"\"\"Computes weight sums given token weights.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n        with a pre-computed reachability matrix.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batch version of `weight_sum`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n        return masses.cpu().numpy()\n\n    def weight_max(self, ws):\n        \"\"\"Computes the max weights given the token weights.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n        operations on GPU.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batch version of `weight_max`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n\n        # Get leaf weights\n        leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n        batch_size = leaf_weights.shape[0]\n\n        # Use scatter_reduce to propagate maximum values in parallel\n        result = torch.zeros((batch_size, len(self.children)), device=self.device)\n        result.scatter_reduce_(\n            dim=1,\n            index=self.dst_indices.expand(batch_size, -1),\n            src=leaf_weights[:, self.src_indices],\n            reduce=\"amax\",\n            include_self=False,\n        )\n\n        return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.ParallelTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Computes weight sums given token weights.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using sparse matrix multiplication with a pre-computed reachability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Computes weight sums given token weights.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n    with a pre-computed reachability matrix.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.ParallelTokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batch version of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batch version of `weight_sum`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n    return masses.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.ParallelTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Computes the max weights given the token weights.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using parallel scatter_reduce operations on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Computes the max weights given the token weights.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n    operations on GPU.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.ParallelTokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batch version of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batch version of `weight_max`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n\n    # Get leaf weights\n    leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n    batch_size = leaf_weights.shape[0]\n\n    # Use scatter_reduce to propagate maximum values in parallel\n    result = torch.zeros((batch_size, len(self.children)), device=self.device)\n    result.scatter_reduce_(\n        dim=1,\n        index=self.dst_indices.expand(batch_size, -1),\n        src=leaf_weights[:, self.src_indices],\n        reduce=\"amax\",\n        include_self=False,\n    )\n\n    return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTokenCharacterTrie","title":"<code>AsyncTokenCharacterTrie</code>","text":"<p>An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>class AsyncTokenCharacterTrie:\n    \"\"\"An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.\"\"\"\n\n    def __init__(self, trie):\n        \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n        Args:\n            trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n        \"\"\"\n        self.trie = trie\n        self._queue = None\n        self._task = None\n\n    @classmethod\n    def from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n        \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n        Args:\n            vocab (list): The vocabulary over which the trie will be defined.\n            backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                    Defaults to 'parallel' which uses GPU acceleration when available.\n            **kwargs: Additional arguments passed to the trie constructor\n\n        Returns:\n            (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n        \"\"\"\n        if backend == \"sequential\":\n            trie = TokenCharacterTrie(decode=vocab, **kwargs)\n        elif backend == \"parallel\":\n            trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n        else:\n            raise ValueError(\n                f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n            )\n        return cls(trie)\n\n    async def _queue_request(self, request, op):\n        if not self._task or self._task.done():\n            self.start()\n\n        future = asyncio.Future()\n        await self._queue.put((request, future, op))\n        return future\n\n    async def weight_sum(self, ws):\n        \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated mass sums for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"sum\")\n        result = await future\n        return result\n\n    async def weight_max(self, ws):\n        \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated max weights for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"max\")\n        result = await future\n        return result\n\n    def start(self):\n        \"\"\"Start the background processing task if not already running.\"\"\"\n        if not self._task or self._task.done():\n            self._queue = (\n                asyncio.Queue()\n            )  # Create a new queue so that it is bound to the current event loop\n            self._task = asyncio.create_task(self._background_loop())\n\n    def _do_weight_sums(self, batch_weights):\n        return self.trie.batch_weight_sum(batch_weights)\n\n    def _do_weight_maxs(self, batch_weights):\n        return self.trie.batch_weight_max(batch_weights)\n\n    async def _background_loop(self):\n        \"\"\"Background task that processes queued weight sum and max requests.\n\n        Continuously monitors the queue for new requests and processes them in batches\n        using the underlying trie implementation.\n\n        Raises:\n            Exception: If any error occurs during processing, it is propagated to all\n                      pending futures in the current batch.\n        \"\"\"\n        while True:\n            try:\n                op_groups = defaultdict(list)\n\n                request, future, op = await self._queue.get()\n                op_groups[op].append((request, future))\n\n                while not self._queue.empty():\n                    request, future, op = await self._queue.get()\n                    op_groups[op].append((request, future))\n\n                for op, group in op_groups.items():\n                    requests, futures = zip(*group)\n\n                    if op == \"sum\":\n                        logger.debug(f\"processing {len(requests)} sum requests\")\n                        results = self._do_weight_sums(requests)\n                    elif op == \"max\":\n                        logger.debug(f\"processing {len(requests)} max requests\")\n                        results = self._do_weight_maxs(requests)\n                    else:\n                        raise ValueError(f\"Unknown operation: {op}\")\n\n                    for future, result in zip(futures, results):\n                        future.set_result(result)\n\n            except Exception as e:\n                for group in op_groups.values():\n                    for _, future in group:\n                        if not future.done():\n                            future.set_exception(e)\n                raise\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        if self._task and not self._task.done():\n            self._task.cancel()\n            try:\n                await self._task\n            except asyncio.CancelledError:\n                pass\n            self._task = None\n\n    def shutdown(self):\n        \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n        if self._task is not None:\n            try:\n                self._task.cancel()\n            except RuntimeError:\n                # Ignore runtime errors that might occur if event loop is closed\n                pass\n            self._task = None\n\n    def __del__(self):\n        self.shutdown()\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTokenCharacterTrie.__init__","title":"<code>__init__(trie)</code>","text":"<p>Initialize an <code>AsyncTokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trie</code> <code>TokenCharacterTrie | ParallelTokenCharacterTrie</code> <p>The underlying <code>TokenCharacterTrie</code> or <code>ParallelTokenCharacterTrie</code> instance</p> required Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def __init__(self, trie):\n    \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n    Args:\n        trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n    \"\"\"\n    self.trie = trie\n    self._queue = None\n    self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTokenCharacterTrie.from_vocab","title":"<code>from_vocab(vocab, backend='parallel', **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AsyncTokenCharacterTrie</code> from a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>list</code> <p>The vocabulary over which the trie will be defined.</p> required <code>backend</code> <code>str</code> <p>The trie implementation to use - either 'sequential' or 'parallel'.     Defaults to 'parallel' which uses GPU acceleration when available.</p> <code>'parallel'</code> <code>**kwargs</code> <p>Additional arguments passed to the trie constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTokenCharacterTrie</code> <p>The initialized asynchronous trie instance.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>@classmethod\ndef from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n    \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n    Args:\n        vocab (list): The vocabulary over which the trie will be defined.\n        backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                Defaults to 'parallel' which uses GPU acceleration when available.\n        **kwargs: Additional arguments passed to the trie constructor\n\n    Returns:\n        (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n    \"\"\"\n    if backend == \"sequential\":\n        trie = TokenCharacterTrie(decode=vocab, **kwargs)\n    elif backend == \"parallel\":\n        trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n    else:\n        raise ValueError(\n            f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n        )\n    return cls(trie)\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_sum</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated mass sums for the given distribution.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def weight_sum(self, ws):\n    \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated mass sums for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"sum\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_max</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated max weights for the given distribution.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def weight_max(self, ws):\n    \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated max weights for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"max\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTokenCharacterTrie.start","title":"<code>start()</code>","text":"<p>Start the background processing task if not already running.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def start(self):\n    \"\"\"Start the background processing task if not already running.\"\"\"\n    if not self._task or self._task.done():\n        self._queue = (\n            asyncio.Queue()\n        )  # Create a new queue so that it is bound to the current event loop\n        self._task = asyncio.create_task(self._background_loop())\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTokenCharacterTrie.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    if self._task and not self._task.done():\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/__init__/#genlm.backend.AsyncTokenCharacterTrie.shutdown","title":"<code>shutdown()</code>","text":"<p>Stop the background processing task and cleanup resources.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def shutdown(self):\n    \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n    if self._task is not None:\n        try:\n            self._task.cancel()\n        except RuntimeError:\n            # Ignore runtime errors that might occur if event loop is closed\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/cache/","title":"cache","text":""},{"location":"reference/genlm/backend/cache/#genlm.backend.cache.OutputCache","title":"<code>OutputCache</code>","text":"<p>A cache for storing tensor outputs with optional CPU offloading.</p> <p>This cache stores tensors along with their original devices and can optionally move tensors to CPU to save GPU memory. When retrieving tensors, they are moved back to their original device.</p> <p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of items to store in the cache</p> required <code>move_to_cpu</code> <code>bool</code> <p>If True, tensors will be moved to CPU when cached</p> <code>False</code> Source code in <code>genlm/backend/cache.py</code> <pre><code>class OutputCache:\n    \"\"\"A cache for storing tensor outputs with optional CPU offloading.\n\n    This cache stores tensors along with their original devices and can optionally\n    move tensors to CPU to save GPU memory. When retrieving tensors, they are\n    moved back to their original device.\n\n    Args:\n        maxsize (int): Maximum number of items to store in the cache\n        move_to_cpu (bool): If True, tensors will be moved to CPU when cached\n    \"\"\"\n\n    def __init__(self, maxsize, move_to_cpu=False):\n        self.maxsize = maxsize\n        self.move_to_cpu = move_to_cpu\n        self.cache = OrderedDict()  # stores (device, tensor) tuples\n\n    def __getitem__(self, key):\n        if key in self.cache:\n            device, value = self.cache.pop(key)\n            self.cache[key] = (device, value)\n            return value.to(device) if self.move_to_cpu else value\n        raise KeyError(key)\n\n    def __setitem__(self, key, value):\n        if len(self.cache) &gt;= self.maxsize:\n            old_key, (_, old_tensor) = self.cache.popitem(last=False)\n            del old_tensor\n\n        self.cache[key] = (value.device, value.cpu() if self.move_to_cpu else value)\n\n    def __contains__(self, key):\n        return key in self.cache\n\n    def __len__(self):\n        return len(self.cache)\n\n    def clear(self):\n        self.cache.clear()\n</code></pre>"},{"location":"reference/genlm/backend/cache/#genlm.backend.cache.TokenTrie","title":"<code>TokenTrie</code>","text":"<p>Class used internally to cache language model results.</p> <p>The TokenTrie maintains a tree of token sequences, storing logits and key-value states for each path.</p> Source code in <code>genlm/backend/cache.py</code> <pre><code>class TokenTrie:\n    \"\"\"Class used internally to cache language model results.\n\n    The TokenTrie maintains a tree of token sequences, storing logits and key-value\n    states for each path.\n    \"\"\"\n\n    # maybe TODO: Implement eviction policy\n\n    # Trie of tokens.\n\n    def __init__(self, parent=None, logprobs=None):\n        self.children = {}  # maps token ID to child\n        self.logprobs = logprobs  # for next token\n        self.past_key_values = None\n\n    def __repr__(self):\n        return (\n            f\"{'*' if self.past_key_values is not None else ''}[\"\n            + \", \".join(\n                [\n                    f\"{node_id}: {node.__repr__()}\"\n                    for (node_id, node) in self.children.items()\n                ]\n            )\n            + \"]\"\n        )\n\n    def clear_kv_cache(self):\n        self.past_key_values = None\n        for child, node in self.children.items():\n            node.clear_kv_cache()\n\n    def has_token(self, token_id):\n        return token_id in self.children\n\n    def get_token(self, token_id):\n        return self.children[token_id]\n\n    def add_token(self, token_id, logprobs=None):\n        self.children[token_id] = TokenTrie(self, logprobs)\n        return self.children[token_id]\n\n    def extend_cache(self, next_token_index, token_ids, logits, base):\n        node = self\n\n        for j in range(next_token_index, len(token_ids)):\n            token_id = token_ids[j]\n            token_logits = logits[j - base]\n            token_logprobs = torch.log_softmax(token_logits, 0)\n\n            node = node.add_token(token_id, token_logprobs.cpu())\n\n        return node\n</code></pre>"},{"location":"reference/genlm/backend/cache/#genlm.backend.cache.DynamicTokenTrie","title":"<code>DynamicTokenTrie</code>","text":"<p>               Bases: <code>TokenTrie</code></p> Source code in <code>genlm/backend/cache.py</code> <pre><code>class DynamicTokenTrie(TokenTrie):\n    def __init__(self, parent=None, logprobs=None, past_key_values=None):\n        super().__init__(parent, logprobs)\n        self.past_key_values = past_key_values\n        self.last_access = time()\n        self.kv_size = 0\n        self.parent = parent\n        self.depth = 0 if parent is None else parent.depth + 1\n\n    def touch(self):\n        \"\"\"Update access timestamp recursively upward.\"\"\"\n        t = time()\n        node = self\n        while node:\n            node.last_access = t\n            node = node.parent\n\n    def add_token(self, token_id, logprobs=None, past_key_values=None):\n        if token_id in self.children:\n            child = self.children[token_id]\n            child.store_kv(past_key_values)\n            if child.logprobs is None:\n                child.logprobs = logprobs\n        else:\n            self.children[token_id] = DynamicTokenTrie(\n                parent=self, logprobs=logprobs, past_key_values=past_key_values\n            )\n        self.children[token_id].touch()\n        return self.children[token_id]\n\n    def store_kv(self, past_key_values):\n        \"\"\"Store KV states on this node.\"\"\"\n        if self.past_key_values is not None or past_key_values is None:\n            return\n        self.past_key_values = past_key_values\n\n    def extend_cache(self, next_token_index, token_ids, logprobs=None, kv=None):\n        node = self\n        token_ids_current = token_ids[next_token_index:]\n        if kv is None:\n            kv = [None] * len(token_ids_current)\n        else:\n            kv = [kv[:, :, :, i : i + 1, :] for i in range(len(token_ids_current))]\n\n        for i, token_id in enumerate(token_ids_current):\n            node = node.add_token(token_id, None, kv[i])\n\n        if node.logprobs is None:\n            node.logprobs = logprobs\n\n        return node\n\n    def count_kv_size(self):\n        \"\"\"Recompute how many nodes currently store KVs.\"\"\"\n        total = 1 if self.past_key_values is not None else 0\n        for c in self.children.values():\n            total += c.count_kv_size()\n        self.kv_size = total\n        return total\n\n    def collect_nodes_with_kv(self):\n        \"\"\"Collect nodes that have stored KVs (for eviction decisions).\"\"\"\n        nodes = []\n        if self.past_key_values is not None:\n            nodes.append(self)\n        for c in self.children.values():\n            nodes.extend(c.collect_nodes_with_kv())\n        return nodes\n\n    def evict_lru_kv(self, max_kv):\n        \"\"\"Evict least recently used KV entries (and descendants) until under limit.\"\"\"\n        total = self.count_kv_size()\n        if total &lt;= max_kv:\n            return\n        nodes = self.collect_nodes_with_kv()\n        nodes.sort(key=lambda n: (n.last_access, -n.depth))\n\n        for node in nodes:\n            if self.kv_size &lt;= max_kv:\n                break\n            node._clear_kv_recursive()\n            self.count_kv_size()\n\n    def _clear_kv_recursive(self):\n        \"\"\"Remove KV from this node and all descendants.\"\"\"\n        if self.past_key_values is not None:\n            self.past_key_values = None\n        for c in self.children.values():\n            c._clear_kv_recursive()\n</code></pre>"},{"location":"reference/genlm/backend/cache/#genlm.backend.cache.DynamicTokenTrie.touch","title":"<code>touch()</code>","text":"<p>Update access timestamp recursively upward.</p> Source code in <code>genlm/backend/cache.py</code> <pre><code>def touch(self):\n    \"\"\"Update access timestamp recursively upward.\"\"\"\n    t = time()\n    node = self\n    while node:\n        node.last_access = t\n        node = node.parent\n</code></pre>"},{"location":"reference/genlm/backend/cache/#genlm.backend.cache.DynamicTokenTrie.store_kv","title":"<code>store_kv(past_key_values)</code>","text":"<p>Store KV states on this node.</p> Source code in <code>genlm/backend/cache.py</code> <pre><code>def store_kv(self, past_key_values):\n    \"\"\"Store KV states on this node.\"\"\"\n    if self.past_key_values is not None or past_key_values is None:\n        return\n    self.past_key_values = past_key_values\n</code></pre>"},{"location":"reference/genlm/backend/cache/#genlm.backend.cache.DynamicTokenTrie.count_kv_size","title":"<code>count_kv_size()</code>","text":"<p>Recompute how many nodes currently store KVs.</p> Source code in <code>genlm/backend/cache.py</code> <pre><code>def count_kv_size(self):\n    \"\"\"Recompute how many nodes currently store KVs.\"\"\"\n    total = 1 if self.past_key_values is not None else 0\n    for c in self.children.values():\n        total += c.count_kv_size()\n    self.kv_size = total\n    return total\n</code></pre>"},{"location":"reference/genlm/backend/cache/#genlm.backend.cache.DynamicTokenTrie.collect_nodes_with_kv","title":"<code>collect_nodes_with_kv()</code>","text":"<p>Collect nodes that have stored KVs (for eviction decisions).</p> Source code in <code>genlm/backend/cache.py</code> <pre><code>def collect_nodes_with_kv(self):\n    \"\"\"Collect nodes that have stored KVs (for eviction decisions).\"\"\"\n    nodes = []\n    if self.past_key_values is not None:\n        nodes.append(self)\n    for c in self.children.values():\n        nodes.extend(c.collect_nodes_with_kv())\n    return nodes\n</code></pre>"},{"location":"reference/genlm/backend/cache/#genlm.backend.cache.DynamicTokenTrie.evict_lru_kv","title":"<code>evict_lru_kv(max_kv)</code>","text":"<p>Evict least recently used KV entries (and descendants) until under limit.</p> Source code in <code>genlm/backend/cache.py</code> <pre><code>def evict_lru_kv(self, max_kv):\n    \"\"\"Evict least recently used KV entries (and descendants) until under limit.\"\"\"\n    total = self.count_kv_size()\n    if total &lt;= max_kv:\n        return\n    nodes = self.collect_nodes_with_kv()\n    nodes.sort(key=lambda n: (n.last_access, -n.depth))\n\n    for node in nodes:\n        if self.kv_size &lt;= max_kv:\n            break\n        node._clear_kv_recursive()\n        self.count_kv_size()\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/","title":"llm","text":""},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM","title":"<code>AsyncVirtualLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>class AsyncVirtualLM(AsyncLM):\n    default_params = {\n        \"max_tokens\": 1,\n        \"n\": 1,\n        \"detokenize\": False,\n        \"stop\": None,\n        \"ignore_eos\": True,\n    }\n\n    def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n        \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n        Args:\n            async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n            cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n            cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm.backend.cache.OutputCache] constructor. Defaults to {}.\n\n        Note:\n            The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n        \"\"\"\n        self.async_llm_engine = async_llm_engine\n        self.tokenizer = async_llm_engine.engine.get_tokenizer()\n        self.request_counter = Counter()\n        self.cache = (\n            OutputCache(maxsize=cache_size, **cache_opts)\n            if cache_size &gt; 0\n            else None\n        )\n\n        async_llm_engine.engine.log_stats = False\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    @classmethod\n    def from_name(cls, model_name, engine_opts=None, **kwargs):\n        \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n                configured with prefix caching enabled and async output processing disabled by default.\n            **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n        Returns:\n            (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n        \"\"\"\n        if not HAS_VLLM:\n            raise ImportError(  # pragma: no cover\n                \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n            )\n\n        if engine_opts is not None and \"enable_chunked_prefill\" in engine_opts:\n            if engine_opts[\"enable_chunked_prefill\"]:\n                warnings.warn(  # pragma: no cover\n                    \"Setting enable_chunked_prefill to True may interfere with AsyncVirtualLM's \"\n                    \"custom sampling functionality.\"\n                )\n\n        engine_opts = {\n            \"enable_prefix_caching\": True,\n            \"disable_log_requests\": True,\n            \"disable_async_output_proc\": True,  # This parameter forces vLLM to use v0, which is currently what we want to do.\n            **(engine_opts or {}),\n        }\n\n        engine = AsyncLLMEngine.from_engine_args(\n            AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n        )\n\n        return cls(engine, **kwargs)\n\n    @property\n    def underlying_model(self):\n        return self.async_llm_engine.engine.model_executor.driver_worker.model_runner.model\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            result (torch.Tensor): Normalized log probability tensor.\n\n        Warning:\n            Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n            For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n        \"\"\"\n        key = tuple(token_ids)\n\n        if self.cache is not None and key in self.cache:\n            return self.cache[key]\n\n        result = await self._next_token_logprobs(key)\n\n        if self.cache is not None:\n            self.cache[key] = result\n\n        return result\n\n    async def _next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        req_id = str(next(self.request_counter))\n        prompt = TokensPrompt(prompt_token_ids=token_ids)\n\n        outputs = []\n        processor = PassThroughLogitsProcessor()\n        async for output in self.async_llm_engine.generate(\n            prompt=prompt,\n            sampling_params=SamplingParams(\n                **self.default_params, logits_processors=[processor]\n            ),\n            request_id=req_id,\n        ):\n            if output.finished:\n                outputs.append(output)\n\n        assert processor.log_probs is not None, (\n            \"Log probs should be set by the logits processor.\"\n        )\n        return processor.log_probs\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self.batch_next_token_logprobs_sync([token_ids])[0]\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"\n        Request log probabilities of next tokens in a batch synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n        \"\"\"\n        req_ids = []\n        req_id2processors = {}\n        for token_ids in token_ids_list:\n            req_id = str(next(self.request_counter))\n            req_ids.append(req_id)\n            processor = PassThroughLogitsProcessor()\n            req_id2processors[req_id] = processor\n            self.async_llm_engine.engine.add_request(\n                prompt=TokensPrompt(prompt_token_ids=token_ids),\n                params=SamplingParams(\n                    **self.default_params, logits_processors=[processor]\n                ),\n                request_id=req_id,\n            )\n\n        while self.async_llm_engine.engine.has_unfinished_requests():\n            output = self.async_llm_engine.engine.step()\n            for out in output:\n                if out.finished:\n                    assert out.request_id in req_id2processors, (\n                        f\"{out.request_id} not in requested IDs\"\n                    )\n\n        return torch.stack(\n            [req_id2processors[req_id].log_probs for req_id in req_ids]\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear output cache.\"\"\"\n        if self.cache:\n            self.cache.clear()\n\n    def __del__(self):\n        \"\"\"Clean up resources on deletion.\"\"\"\n        self._cleanup_engine()\n\n    def _cleanup_engine(self):\n        \"\"\"Clean up the vLLM engine and associated resources.\"\"\"\n        if async_engine := getattr(self, \"async_llm_engine\", None):\n            async_engine.shutdown_background_loop()\n            destroy_model_parallel()\n            destroy_distributed_environment()\n\n    async def sample(\n        self,\n        prompt_token_ids,\n        max_tokens,\n        eos_token_ids,\n        temperature=1.0,\n        seed=None,\n    ):\n        \"\"\"Sample from the language model.\n\n        Args:\n            prompt_token_ids (list[int]): The token IDs of the prompt.\n            eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n            temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n            max_tokens (int): The maximum number of tokens to generate.\n            seed (int, optional): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            (list[int]): The sampled token IDs.\n        \"\"\"\n        async for output in self.async_llm_engine.generate(\n            prompt=TokensPrompt(prompt_token_ids=prompt_token_ids),\n            sampling_params=SamplingParams(\n                n=1,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                seed=seed,\n                stop=[self.byte_vocab[i].decode() for i in eos_token_ids],\n            ),\n            request_id=str(next(self.request_counter)),\n        ):\n            if output.finished:\n                assert len(output.outputs) == 1, (\n                    \"Expected exactly one sequence group\"\n                )\n                token_ids = list(output.outputs[0].token_ids)\n                if token_ids[-1] in eos_token_ids:\n                    token_ids = token_ids[:-1]\n                return token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM.__init__","title":"<code>__init__(async_llm_engine, cache_size=0, cache_opts={})</code>","text":"<p>Initialize an <code>AsyncVirtualLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>async_llm_engine</code> <code>AsyncLLMEngine</code> <p>The async vLLM engine instance.</p> required <code>cache_size</code> <code>int</code> <p>Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.</p> <code>0</code> <code>cache_opts</code> <code>dict</code> <p>Additional options to pass to the <code>OutputCache</code> constructor. Defaults to {}.</p> <code>{}</code> Note <p>The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n    \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n    Args:\n        async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n        cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n        cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm.backend.cache.OutputCache] constructor. Defaults to {}.\n\n    Note:\n        The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n    \"\"\"\n    self.async_llm_engine = async_llm_engine\n    self.tokenizer = async_llm_engine.engine.get_tokenizer()\n    self.request_counter = Counter()\n    self.cache = (\n        OutputCache(maxsize=cache_size, **cache_opts)\n        if cache_size &gt; 0\n        else None\n    )\n\n    async_llm_engine.engine.log_stats = False\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM.from_name","title":"<code>from_name(model_name, engine_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>AsyncVirtualLM</code> instance from a model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>engine_opts</code> <code>dict</code> <p>Additional options to pass to the <code>AsyncLLMEngine</code>. The engine will be configured with prefix caching enabled and async output processing disabled by default.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to <code>AsyncVirtualLM</code> constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncVirtualLM</code> <p>An <code>AsyncVirtualLM</code> instance.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, engine_opts=None, **kwargs):\n    \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n            configured with prefix caching enabled and async output processing disabled by default.\n        **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n    Returns:\n        (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n    \"\"\"\n    if not HAS_VLLM:\n        raise ImportError(  # pragma: no cover\n            \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n        )\n\n    if engine_opts is not None and \"enable_chunked_prefill\" in engine_opts:\n        if engine_opts[\"enable_chunked_prefill\"]:\n            warnings.warn(  # pragma: no cover\n                \"Setting enable_chunked_prefill to True may interfere with AsyncVirtualLM's \"\n                \"custom sampling functionality.\"\n            )\n\n    engine_opts = {\n        \"enable_prefix_caching\": True,\n        \"disable_log_requests\": True,\n        \"disable_async_output_proc\": True,  # This parameter forces vLLM to use v0, which is currently what we want to do.\n        **(engine_opts or {}),\n    }\n\n    engine = AsyncLLMEngine.from_engine_args(\n        AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n    )\n\n    return cls(engine, **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token asynchronously with output caching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>Tensor</code> <p>Normalized log probability tensor.</p> Warning <p>Do not use <code>asyncio.run(next_token_logprobs())</code> as it may interfere with vLLM's background loop. For synchronous usage, use the <code>next_token_logprobs_sync()</code> method instead.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        result (torch.Tensor): Normalized log probability tensor.\n\n    Warning:\n        Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n        For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n    \"\"\"\n    key = tuple(token_ids)\n\n    if self.cache is not None and key in self.cache:\n        return self.cache[key]\n\n    result = await self._next_token_logprobs(key)\n\n    if self.cache is not None:\n        self.cache[key] = result\n\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self.batch_next_token_logprobs_sync([token_ids])[0]\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Request log probabilities of next tokens in a batch synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists, each representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of normalized log probability tensors, one for each prompt in the input list.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"\n    Request log probabilities of next tokens in a batch synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n    \"\"\"\n    req_ids = []\n    req_id2processors = {}\n    for token_ids in token_ids_list:\n        req_id = str(next(self.request_counter))\n        req_ids.append(req_id)\n        processor = PassThroughLogitsProcessor()\n        req_id2processors[req_id] = processor\n        self.async_llm_engine.engine.add_request(\n            prompt=TokensPrompt(prompt_token_ids=token_ids),\n            params=SamplingParams(\n                **self.default_params, logits_processors=[processor]\n            ),\n            request_id=req_id,\n        )\n\n    while self.async_llm_engine.engine.has_unfinished_requests():\n        output = self.async_llm_engine.engine.step()\n        for out in output:\n            if out.finished:\n                assert out.request_id in req_id2processors, (\n                    f\"{out.request_id} not in requested IDs\"\n                )\n\n    return torch.stack(\n        [req_id2processors[req_id].log_probs for req_id in req_ids]\n    )\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear output cache.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear output cache.\"\"\"\n    if self.cache:\n        self.cache.clear()\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM.__del__","title":"<code>__del__()</code>","text":"<p>Clean up resources on deletion.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up resources on deletion.\"\"\"\n    self._cleanup_engine()\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncVirtualLM.sample","title":"<code>sample(prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids</code> <code>list[int]</code> <p>The token IDs of the prompt.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs of the end-of-sequence tokens.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use to rescale the logits. Defaults to 1.0.</p> <code>1.0</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>async def sample(\n    self,\n    prompt_token_ids,\n    max_tokens,\n    eos_token_ids,\n    temperature=1.0,\n    seed=None,\n):\n    \"\"\"Sample from the language model.\n\n    Args:\n        prompt_token_ids (list[int]): The token IDs of the prompt.\n        eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n        temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n        max_tokens (int): The maximum number of tokens to generate.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        (list[int]): The sampled token IDs.\n    \"\"\"\n    async for output in self.async_llm_engine.generate(\n        prompt=TokensPrompt(prompt_token_ids=prompt_token_ids),\n        sampling_params=SamplingParams(\n            n=1,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            seed=seed,\n            stop=[self.byte_vocab[i].decode() for i in eos_token_ids],\n        ),\n        request_id=str(next(self.request_counter)),\n    ):\n        if output.finished:\n            assert len(output.outputs) == 1, (\n                \"Expected exactly one sequence group\"\n            )\n            token_ids = list(output.outputs[0].token_ids)\n            if token_ids[-1] in eos_token_ids:\n                token_ids = token_ids[:-1]\n            return token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer","title":"<code>AsyncTransformer</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Asynchronous wrapper around a HuggingFace causal language model with caching support.</p> <p>This class provides an asynchronous interface to HuggingFace language models with automatic batching and caching (output and KV) for improved efficiency.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>class AsyncTransformer(AsyncLM):\n    \"\"\"Asynchronous wrapper around a HuggingFace causal language model with caching support.\n\n    This class provides an asynchronous interface to HuggingFace language models with automatic batching\n    and caching (output and KV) for improved efficiency.\n    \"\"\"\n\n    @classmethod\n    def from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n        \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n        Args:\n            model_id (str): Model identifier in HuggingFace's model hub.\n            bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n                Defaults to None.\n            hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n                Defaults to None.\n            **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n        Returns:\n            (AsyncTransformer): An initialized `AsyncTransformer` instance.\n        \"\"\"\n        if bitsandbytes_opts:\n            bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n        else:\n            bnb_config = None\n\n        _hf_opts = {\n            \"device_map\": \"auto\",\n            \"torch_dtype\": \"auto\",\n        }\n        if hf_opts:\n            _hf_opts.update(hf_opts)\n\n        tok = AutoTokenizer.from_pretrained(model_id)\n        mod = AutoModelForCausalLM.from_pretrained(\n            model_id, quantization_config=bnb_config, **_hf_opts\n        )\n\n        return cls(mod, tok, **kwargs)\n\n    @torch.no_grad()\n    def __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n        \"\"\"Initialize an AsyncTransformer instance.\n\n        Args:\n            hf_model: A HuggingFace CausalLM model instance.\n            hf_tokenizer: A HuggingFace Tokenizer.\n            batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n                Defaults to 20.\n            timeout (float, optional): Seconds to wait since last query before processing current batch.\n                Defaults to 0.02.\n        \"\"\"\n        self.model = hf_model\n        self.tokenizer = hf_tokenizer\n        self.device = hf_model.device\n        self.cache = TokenTrie()\n\n        # Queries to be batched. Each query is a sequence of tokens,\n        # and a Future to be called when the query is resolved.\n        self.queries = []\n        self.batch_size = batch_size\n        self.timeout = timeout\n        self.timer = None\n\n        self.model.eval()\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    def clear_cache(self):\n        \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n        self.cache = TokenTrie()\n\n    def clear_kv_cache(self):\n        \"\"\"Clear any key and value vectors from the cache.\"\"\"\n        self.cache.clear_kv_cache()\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n        to completion.\"\"\"\n        self.queries = []\n\n    @torch.no_grad()\n    def cache_kv(self, prompt_tokens):\n        \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n        Args:\n            prompt_tokens (list[int]): token ids for the prompt to cache.\n        \"\"\"\n        result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n        node = self.cache.extend_cache(0, prompt_tokens, result.logits[0], 0)\n        node.past_key_values = result.past_key_values\n\n    @torch.no_grad()\n    def batch_evaluate_queries(self):\n        \"\"\"\n        Process a batch of queued language model queries.\n\n        This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n        \"\"\"\n\n        queries, self.queries = self.queries, []\n        if len(queries) == 0:\n            return\n\n        query_groups = defaultdict(list)\n        for query in queries:\n            key = tuple(query.prompt)  # XXX: cache based on past_len too?\n            query_groups[key].append(query)\n\n        # Use one representative query from each group\n        unique_queries = [group[0] for group in query_groups.values()]\n\n        past_example = next((q.past for q in unique_queries if q.past), False)\n        max_past_length = max(q.past_len for q in unique_queries)\n        max_query_length = max(len(q.prompt) for q in unique_queries)\n\n        padding_token_id = (\n            self.tokenizer.pad_token_id\n            if self.tokenizer.pad_token_id is not None\n            else 0\n        )\n\n        input_ids = torch.tensor(\n            [\n                q.prompt_padded(padding_token_id, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        attn_masks = torch.tensor(\n            [\n                q.attention_mask(max_past_length, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        posn_ids = torch.tensor(\n            [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n        ).to(self.device)\n        if past_example:\n            pasts = [\n                [\n                    torch.cat(\n                        (\n                            *(\n                                q.past_padded(\n                                    layer,\n                                    j,\n                                    max_past_length,\n                                    past_example[0][0].dtype,\n                                    self.device,\n                                    past_example[0][0].shape,\n                                )\n                                for q in unique_queries\n                            ),\n                        ),\n                        dim=0,\n                    )\n                    for j in range(2)\n                ]\n                for layer in range(len(past_example))\n            ]\n        else:\n            pasts = None\n\n        pasts = DynamicCache.from_legacy_cache(pasts)\n\n        results = self.model(\n            input_ids,\n            attention_mask=attn_masks,\n            position_ids=posn_ids,\n            past_key_values=pasts,\n            use_cache=pasts is not None,\n        )\n\n        assert len(results.logits) == len(unique_queries)\n\n        for i, q in enumerate(unique_queries):\n            result = results.logits[i]\n            for dup_query in query_groups[tuple(q.prompt)]:\n                dup_query.future.set_result(result)\n\n    @torch.no_grad()\n    def add_query(self, query, future, past):\n        \"\"\"Add a query to be evaluated in the next batch.\n\n        This method is called internally when a `next_token_logprobs` request is made.\n\n        Args:\n            query (list[int]): Token IDs representing the query prompt\n            future (asyncio.Future): Future to store the result in\n            past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n                or None if this is a new query\n        \"\"\"\n        self.queries.append(Query(query, future, past))\n\n        if self.timer:\n            self.timer.cancel()\n            self.timer = None\n        if len(self.queries) &gt;= self.batch_size:\n            self.batch_evaluate_queries()\n        else:\n            self.timer = asyncio.get_running_loop().call_later(\n                self.timeout, lambda: self.batch_evaluate_queries()\n            )\n\n    def walk_cache(self, token_ids):\n        \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n        Args:\n            token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n        Returns:\n            tuple:\n                - CacheNode: The deepest node in the cache tree that matches the token sequence\n                - int: Number of tokens matched from the start of token_ids\n                - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                    or None if no cached states were found\n                - int: Base index indicating where the past states start in token_ids\n        \"\"\"\n        # Walk while tokens can be found\n        node = self.cache\n        next_token_index = 0\n\n        past = None\n        base = 0\n        while next_token_index &lt; len(token_ids):\n            if node.past_key_values is not None:\n                past = node.past_key_values\n                base = next_token_index\n            if node.has_token(token_ids[next_token_index]):\n                node = node.get_token(token_ids[next_token_index])\n                next_token_index += 1\n            else:\n                break\n\n        return node, next_token_index, past, base\n\n    @torch.no_grad()\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        # If we processed all tokens, then we're done.\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        # Create a future with the prompt\n        future = asyncio.get_running_loop().create_future()\n        self.add_query(token_ids[base:], future, past)\n        logits = await future\n\n        # Create new nodes\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    @torch.no_grad()\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        # Walk while tokens can be found\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        logits = self.model(\n            torch.tensor([token_ids[base:]]).to(self.device),\n            past_key_values=node.past_key_values,\n            use_cache=node.past_key_values is not None,\n        ).logits[0]\n\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    def next_token_logprobs_uncached(self, token_ids):\n        \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        with torch.no_grad():\n            logits = self.model(\n                torch.tensor([token_ids]).to(self.device),\n                past_key_values=None,\n                use_cache=False,\n            ).logits[0]\n            return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.from_name","title":"<code>from_name(model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an AsyncTransformer instance from a pretrained HuggingFace model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Model identifier in HuggingFace's model hub.</p> required <code>bitsandbytes_opts</code> <code>dict</code> <p>Additional configuration options for bitsandbytes quantization. Defaults to None.</p> <code>None</code> <code>hf_opts</code> <code>dict</code> <p>Additional configuration options for loading the HuggingFace model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the <code>AsyncTransformer</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTransformer</code> <p>An initialized <code>AsyncTransformer</code> instance.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@classmethod\ndef from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n    \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n    Args:\n        model_id (str): Model identifier in HuggingFace's model hub.\n        bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n            Defaults to None.\n        hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n            Defaults to None.\n        **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n    Returns:\n        (AsyncTransformer): An initialized `AsyncTransformer` instance.\n    \"\"\"\n    if bitsandbytes_opts:\n        bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n    else:\n        bnb_config = None\n\n    _hf_opts = {\n        \"device_map\": \"auto\",\n        \"torch_dtype\": \"auto\",\n    }\n    if hf_opts:\n        _hf_opts.update(hf_opts)\n\n    tok = AutoTokenizer.from_pretrained(model_id)\n    mod = AutoModelForCausalLM.from_pretrained(\n        model_id, quantization_config=bnb_config, **_hf_opts\n    )\n\n    return cls(mod, tok, **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.__init__","title":"<code>__init__(hf_model, hf_tokenizer, batch_size=20, timeout=0.02)</code>","text":"<p>Initialize an AsyncTransformer instance.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <p>A HuggingFace CausalLM model instance.</p> required <code>hf_tokenizer</code> <p>A HuggingFace Tokenizer.</p> required <code>batch_size</code> <code>int</code> <p>Maximum queries to process in one batch during auto-batching. Defaults to 20.</p> <code>20</code> <code>timeout</code> <code>float</code> <p>Seconds to wait since last query before processing current batch. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n    \"\"\"Initialize an AsyncTransformer instance.\n\n    Args:\n        hf_model: A HuggingFace CausalLM model instance.\n        hf_tokenizer: A HuggingFace Tokenizer.\n        batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n            Defaults to 20.\n        timeout (float, optional): Seconds to wait since last query before processing current batch.\n            Defaults to 0.02.\n    \"\"\"\n    self.model = hf_model\n    self.tokenizer = hf_tokenizer\n    self.device = hf_model.device\n    self.cache = TokenTrie()\n\n    # Queries to be batched. Each query is a sequence of tokens,\n    # and a Future to be called when the query is resolved.\n    self.queries = []\n    self.batch_size = batch_size\n    self.timeout = timeout\n    self.timer = None\n\n    self.model.eval()\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache of log probabilities and key/value pairs.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n    self.cache = TokenTrie()\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.clear_kv_cache","title":"<code>clear_kv_cache()</code>","text":"<p>Clear any key and value vectors from the cache.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def clear_kv_cache(self):\n    \"\"\"Clear any key and value vectors from the cache.\"\"\"\n    self.cache.clear_kv_cache()\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing to completion.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n    to completion.\"\"\"\n    self.queries = []\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.cache_kv","title":"<code>cache_kv(prompt_tokens)</code>","text":"<p>Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_tokens</code> <code>list[int]</code> <p>token ids for the prompt to cache.</p> required Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef cache_kv(self, prompt_tokens):\n    \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n    Args:\n        prompt_tokens (list[int]): token ids for the prompt to cache.\n    \"\"\"\n    result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n    node = self.cache.extend_cache(0, prompt_tokens, result.logits[0], 0)\n    node.past_key_values = result.past_key_values\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.batch_evaluate_queries","title":"<code>batch_evaluate_queries()</code>","text":"<p>Process a batch of queued language model queries.</p> <p>This method is called internally when the <code>batch_size</code> has been met or the <code>timeout</code> has expired.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef batch_evaluate_queries(self):\n    \"\"\"\n    Process a batch of queued language model queries.\n\n    This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n    \"\"\"\n\n    queries, self.queries = self.queries, []\n    if len(queries) == 0:\n        return\n\n    query_groups = defaultdict(list)\n    for query in queries:\n        key = tuple(query.prompt)  # XXX: cache based on past_len too?\n        query_groups[key].append(query)\n\n    # Use one representative query from each group\n    unique_queries = [group[0] for group in query_groups.values()]\n\n    past_example = next((q.past for q in unique_queries if q.past), False)\n    max_past_length = max(q.past_len for q in unique_queries)\n    max_query_length = max(len(q.prompt) for q in unique_queries)\n\n    padding_token_id = (\n        self.tokenizer.pad_token_id\n        if self.tokenizer.pad_token_id is not None\n        else 0\n    )\n\n    input_ids = torch.tensor(\n        [\n            q.prompt_padded(padding_token_id, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    attn_masks = torch.tensor(\n        [\n            q.attention_mask(max_past_length, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    posn_ids = torch.tensor(\n        [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n    ).to(self.device)\n    if past_example:\n        pasts = [\n            [\n                torch.cat(\n                    (\n                        *(\n                            q.past_padded(\n                                layer,\n                                j,\n                                max_past_length,\n                                past_example[0][0].dtype,\n                                self.device,\n                                past_example[0][0].shape,\n                            )\n                            for q in unique_queries\n                        ),\n                    ),\n                    dim=0,\n                )\n                for j in range(2)\n            ]\n            for layer in range(len(past_example))\n        ]\n    else:\n        pasts = None\n\n    pasts = DynamicCache.from_legacy_cache(pasts)\n\n    results = self.model(\n        input_ids,\n        attention_mask=attn_masks,\n        position_ids=posn_ids,\n        past_key_values=pasts,\n        use_cache=pasts is not None,\n    )\n\n    assert len(results.logits) == len(unique_queries)\n\n    for i, q in enumerate(unique_queries):\n        result = results.logits[i]\n        for dup_query in query_groups[tuple(q.prompt)]:\n            dup_query.future.set_result(result)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.add_query","title":"<code>add_query(query, future, past)</code>","text":"<p>Add a query to be evaluated in the next batch.</p> <p>This method is called internally when a <code>next_token_logprobs</code> request is made.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[int]</code> <p>Token IDs representing the query prompt</p> required <code>future</code> <code>Future</code> <p>Future to store the result in</p> required <code>past</code> <code>list[tuple[Tensor]] | None</code> <p>Past key/value states from previous evaluation, or None if this is a new query</p> required Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef add_query(self, query, future, past):\n    \"\"\"Add a query to be evaluated in the next batch.\n\n    This method is called internally when a `next_token_logprobs` request is made.\n\n    Args:\n        query (list[int]): Token IDs representing the query prompt\n        future (asyncio.Future): Future to store the result in\n        past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n            or None if this is a new query\n    \"\"\"\n    self.queries.append(Query(query, future, past))\n\n    if self.timer:\n        self.timer.cancel()\n        self.timer = None\n    if len(self.queries) &gt;= self.batch_size:\n        self.batch_evaluate_queries()\n    else:\n        self.timer = asyncio.get_running_loop().call_later(\n            self.timeout, lambda: self.batch_evaluate_queries()\n        )\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.walk_cache","title":"<code>walk_cache(token_ids)</code>","text":"<p>Walk the cache tree to find the deepest node matching a sequence of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Sequence of token IDs to follow in the cache tree</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>CacheNode: The deepest node in the cache tree that matches the token sequence</li> <li>int: Number of tokens matched from the start of token_ids</li> <li>list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,     or None if no cached states were found</li> <li>int: Base index indicating where the past states start in token_ids</li> </ul> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def walk_cache(self, token_ids):\n    \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n    Args:\n        token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n    Returns:\n        tuple:\n            - CacheNode: The deepest node in the cache tree that matches the token sequence\n            - int: Number of tokens matched from the start of token_ids\n            - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                or None if no cached states were found\n            - int: Base index indicating where the past states start in token_ids\n    \"\"\"\n    # Walk while tokens can be found\n    node = self.cache\n    next_token_index = 0\n\n    past = None\n    base = 0\n    while next_token_index &lt; len(token_ids):\n        if node.past_key_values is not None:\n            past = node.past_key_values\n            base = next_token_index\n        if node.has_token(token_ids[next_token_index]):\n            node = node.get_token(token_ids[next_token_index])\n            next_token_index += 1\n        else:\n            break\n\n    return node, next_token_index, past, base\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    # If we processed all tokens, then we're done.\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    # Create a future with the prompt\n    future = asyncio.get_running_loop().create_future()\n    self.add_query(token_ids[base:], future, past)\n    logits = await future\n\n    # Create new nodes\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token. Not asynchronous, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    # Walk while tokens can be found\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    logits = self.model(\n        torch.tensor([token_ids[base:]]).to(self.device),\n        past_key_values=node.past_key_values,\n        use_cache=node.past_key_values is not None,\n    ).logits[0]\n\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncTransformer.next_token_logprobs_uncached","title":"<code>next_token_logprobs_uncached(token_ids)</code>","text":"<p>Request log probabilities of next token. No KV or output caching, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def next_token_logprobs_uncached(self, token_ids):\n    \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    with torch.no_grad():\n        logits = self.model(\n            torch.tensor([token_ids]).to(self.device),\n            past_key_values=None,\n            use_cache=False,\n        ).logits[0]\n        return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncLM","title":"<code>AsyncLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for asynchronous language models.</p> <p>This class provides an interface for language models that can generate token probabilities asynchronously. It handles tokenization and vocabulary management.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance compatible with the language model</p> required Source code in <code>genlm/backend/llm/base.py</code> <pre><code>class AsyncLM(ABC):\n    \"\"\"Abstract base class for asynchronous language models.\n\n    This class provides an interface for language models that can generate token probabilities\n    asynchronously. It handles tokenization and vocabulary management.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance compatible with the language model\n    \"\"\"\n\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.byte_vocab, self.str_vocab = decode_vocab(self.tokenizer)\n\n    @abstractmethod\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs representing the prompt.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs representing the prompt.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        pass\n\n    async def batch_next_token_logprobs(self, token_ids_list):\n        \"\"\"Batch request log probabilities for multiple token sequences asynchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists.\n\n        Returns:\n            (torch.Tensor): A tensor of log probability tensors.\n        \"\"\"\n        logprobs = await asyncio.gather(\n            *[self.next_token_logprobs(token_ids) for token_ids in token_ids_list]\n        )\n\n        return torch.stack(logprobs)\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"Batch request log probabilities for multiple token sequences synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists.\n\n        Returns:\n            (torch.Tensor): A tensor of log probability tensors.\n        \"\"\"\n        return torch.stack(\n            [self.next_token_logprobs_sync(token_ids) for token_ids in token_ids_list]\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear any caches used by the language model. No-op in base class.\"\"\"\n        pass  # pragma: no cover\n\n    async def sample(\n        self, prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None\n    ):\n        \"\"\"Sample from the language model.\n\n        Args:\n            prompt_token_ids (list[int]): The token IDs of the prompt.\n            eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n            temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n            max_tokens (int): The maximum number of tokens to generate.\n            seed (int, optional): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            (list[int]): The sampled token IDs.\n        \"\"\"\n        if seed is not None:\n            generator = torch.Generator()\n            generator.manual_seed(seed)\n        else:\n            generator = None\n\n        generated_token_ids = []\n        for _ in range(max_tokens):\n            logprobs = await self.next_token_logprobs(\n                prompt_token_ids + generated_token_ids\n            )\n            probs = torch.softmax(logprobs / temperature, dim=-1)\n            next_token_id = torch.multinomial(\n                probs.cpu() if seed is not None else probs,\n                num_samples=1,\n                generator=generator,\n            ).item()\n            if next_token_id in eos_token_ids:\n                break\n            generated_token_ids.append(next_token_id)\n\n        return generated_token_ids\n\n    async def batch_sample(\n        self,\n        prompt_token_ids_list,\n        max_tokens,\n        eos_token_ids,\n        temperature=1.0,\n        seed=None,\n    ):\n        \"\"\"Batch sample from the language model.\n\n        Args:\n            prompt_token_ids_list (list[list[int]]): The token IDs of the prompts.\n            max_tokens (int): The maximum number of tokens to generate.\n            eos_token_ids (list[int]): The token IDs of the end-of-sequence token.\n            temperature (float): The temperature to use for the logits.\n            seed (int, optional): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            (list[list[int]]): The sampled token IDs.\n        \"\"\"\n        return await asyncio.gather(\n            *[\n                self.sample(\n                    prompt_token_ids=prompt_token_ids,\n                    max_tokens=max_tokens,\n                    eos_token_ids=eos_token_ids,\n                    temperature=temperature,\n                    seed=seed,\n                )\n                for prompt_token_ids in prompt_token_ids_list\n            ]\n        )\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Request log probabilities of next token asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs representing the prompt.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>@abstractmethod\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs representing the prompt.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>  <code>abstractmethod</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs representing the prompt.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>@abstractmethod\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs representing the prompt.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncLM.batch_next_token_logprobs","title":"<code>batch_next_token_logprobs(token_ids_list)</code>  <code>async</code>","text":"<p>Batch request log probabilities for multiple token sequences asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of log probability tensors.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>async def batch_next_token_logprobs(self, token_ids_list):\n    \"\"\"Batch request log probabilities for multiple token sequences asynchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists.\n\n    Returns:\n        (torch.Tensor): A tensor of log probability tensors.\n    \"\"\"\n    logprobs = await asyncio.gather(\n        *[self.next_token_logprobs(token_ids) for token_ids in token_ids_list]\n    )\n\n    return torch.stack(logprobs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Batch request log probabilities for multiple token sequences synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of log probability tensors.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"Batch request log probabilities for multiple token sequences synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists.\n\n    Returns:\n        (torch.Tensor): A tensor of log probability tensors.\n    \"\"\"\n    return torch.stack(\n        [self.next_token_logprobs_sync(token_ids) for token_ids in token_ids_list]\n    )\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear any caches used by the language model. No-op in base class.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear any caches used by the language model. No-op in base class.\"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncLM.sample","title":"<code>sample(prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids</code> <code>list[int]</code> <p>The token IDs of the prompt.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs of the end-of-sequence tokens.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use to rescale the logits. Defaults to 1.0.</p> <code>1.0</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>async def sample(\n    self, prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None\n):\n    \"\"\"Sample from the language model.\n\n    Args:\n        prompt_token_ids (list[int]): The token IDs of the prompt.\n        eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n        temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n        max_tokens (int): The maximum number of tokens to generate.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        (list[int]): The sampled token IDs.\n    \"\"\"\n    if seed is not None:\n        generator = torch.Generator()\n        generator.manual_seed(seed)\n    else:\n        generator = None\n\n    generated_token_ids = []\n    for _ in range(max_tokens):\n        logprobs = await self.next_token_logprobs(\n            prompt_token_ids + generated_token_ids\n        )\n        probs = torch.softmax(logprobs / temperature, dim=-1)\n        next_token_id = torch.multinomial(\n            probs.cpu() if seed is not None else probs,\n            num_samples=1,\n            generator=generator,\n        ).item()\n        if next_token_id in eos_token_ids:\n            break\n        generated_token_ids.append(next_token_id)\n\n    return generated_token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncLM.batch_sample","title":"<code>batch_sample(prompt_token_ids_list, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Batch sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids_list</code> <code>list[list[int]]</code> <p>The token IDs of the prompts.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs of the end-of-sequence token.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use for the logits.</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[int]]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>async def batch_sample(\n    self,\n    prompt_token_ids_list,\n    max_tokens,\n    eos_token_ids,\n    temperature=1.0,\n    seed=None,\n):\n    \"\"\"Batch sample from the language model.\n\n    Args:\n        prompt_token_ids_list (list[list[int]]): The token IDs of the prompts.\n        max_tokens (int): The maximum number of tokens to generate.\n        eos_token_ids (list[int]): The token IDs of the end-of-sequence token.\n        temperature (float): The temperature to use for the logits.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        (list[list[int]]): The sampled token IDs.\n    \"\"\"\n    return await asyncio.gather(\n        *[\n            self.sample(\n                prompt_token_ids=prompt_token_ids,\n                max_tokens=max_tokens,\n                eos_token_ids=eos_token_ids,\n                temperature=temperature,\n                seed=seed,\n            )\n            for prompt_token_ids in prompt_token_ids_list\n        ]\n    )\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.MockAsyncLM","title":"<code>MockAsyncLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Mock implementation of AsyncLM used for testing.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>class MockAsyncLM(AsyncLM):\n    \"\"\"Mock implementation of AsyncLM used for testing.\"\"\"\n\n    def __init__(self, tokenizer):\n        \"\"\"Initialize a `MockAsyncLM` instance.\n\n        Args:\n            tokenizer: Hugging Face tokenizer instance\n        \"\"\"\n        super().__init__(tokenizer)\n        self._rng = np.random.RandomState(42)\n\n    @classmethod\n    def from_name(cls, model_name, **kwargs):\n        \"\"\"Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.\n\n        Args:\n            model_name (str): Name of pretrained model to load tokenizer from\n            **kwargs: Additional arguments passed to `MockAsyncLM` constructor\n\n        Returns:\n            (MockAsyncLM): `MockAsyncLM` instance initialized with tokenizer from `model_name`\n        \"\"\"\n        from transformers import AutoTokenizer\n\n        return cls(AutoTokenizer.from_pretrained(model_name), **kwargs)\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Get next token log probabilities asynchronously.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self._get_logprobs(token_ids)\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Get next token log probabilities synchronously.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self._get_logprobs(token_ids)\n\n    def _get_logprobs(self, token_ids):\n        \"\"\"Generate random but deterministic log probabilities for given tokens.\n\n        Uses token_ids to seed the random generator, ensuring same inputs produce same outputs.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        seed = sum([(i + 1) * t for i, t in enumerate(token_ids)])\n        self._rng.seed(seed)\n        logits = torch.from_numpy(\n            self._rng.rand(len(self.tokenizer)).astype(np.float32)\n        )\n        return torch.log_softmax(logits, dim=-1)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.MockAsyncLM.__init__","title":"<code>__init__(tokenizer)</code>","text":"<p>Initialize a <code>MockAsyncLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>Hugging Face tokenizer instance</p> required Source code in <code>genlm/backend/llm/base.py</code> <pre><code>def __init__(self, tokenizer):\n    \"\"\"Initialize a `MockAsyncLM` instance.\n\n    Args:\n        tokenizer: Hugging Face tokenizer instance\n    \"\"\"\n    super().__init__(tokenizer)\n    self._rng = np.random.RandomState(42)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.MockAsyncLM.from_name","title":"<code>from_name(model_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of pretrained model to load tokenizer from</p> required <code>**kwargs</code> <p>Additional arguments passed to <code>MockAsyncLM</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>MockAsyncLM</code> <p><code>MockAsyncLM</code> instance initialized with tokenizer from <code>model_name</code></p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, **kwargs):\n    \"\"\"Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.\n\n    Args:\n        model_name (str): Name of pretrained model to load tokenizer from\n        **kwargs: Additional arguments passed to `MockAsyncLM` constructor\n\n    Returns:\n        (MockAsyncLM): `MockAsyncLM` instance initialized with tokenizer from `model_name`\n    \"\"\"\n    from transformers import AutoTokenizer\n\n    return cls(AutoTokenizer.from_pretrained(model_name), **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.MockAsyncLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Get next token log probabilities asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Get next token log probabilities asynchronously.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self._get_logprobs(token_ids)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.MockAsyncLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Get next token log probabilities synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Get next token log probabilities synchronously.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self._get_logprobs(token_ids)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM","title":"<code>AsyncMlxLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Asynchronous MLX-based language model wrapper.</p> <p>This class provides an async interface to MLX language models with automatic batching, caching, and KV cache management. It extends AsyncLM to provide efficient batched inference with prefix caching.</p> <p>The model automatically batches concurrent requests and uses a trie-based cache to store computed log probabilities and KV states for reuse.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>class AsyncMlxLM(AsyncLM):\n    \"\"\"Asynchronous MLX-based language model wrapper.\n\n    This class provides an async interface to MLX language models with\n    automatic batching, caching, and KV cache management. It extends\n    AsyncLM to provide efficient batched inference with prefix caching.\n\n    The model automatically batches concurrent requests and uses a trie-based\n    cache to store computed log probabilities and KV states for reuse.\n    \"\"\"\n\n    def __init__(\n        self,\n        mlx_lm_model,\n        tokenizer,\n        batch_size=5,\n        timeout=0.001,\n        prefill_step_size=2048,\n        cache_size=400,\n    ):\n        \"\"\"Initialize an `AsyncMlxLM` instance.\n\n        Args:\n            mlx_lm_model: The MLX language model instance.\n            tokenizer: The tokenizer for encoding/decoding text.\n            batch_size (int, optional): Maximum number of queries to batch\n                together.\n            timeout (float, optional): Maximum time in seconds to wait\n                before processing a batch, even if batch_size is not met.\n            prefill_step_size (int, optional): Number of tokens to process\n                per step during prompt prefilling.\n            cache_size (int, optional): Maximum number of KV cache entries\n                to keep in memory.\n        \"\"\"\n        self.mlx_lm_model = mlx_lm_model\n        self.tokenizer = tokenizer\n        self.cache = DynamicTokenTrie()\n        self.generation_stream = mx.new_stream(mx.default_device())\n        self.queries = []\n        self.timeout = timeout\n        self.timer = None\n        self.prefill_step_size = prefill_step_size\n        self.cache_size = cache_size\n\n        self.batch_size = batch_size\n        self.kv_cachable = self._kv_cachable(self.mlx_lm_model)\n        if not self.kv_cachable:\n            warnings.warn(\n                f\"Model {type(self.mlx_lm_model).__name__} does not support KV caching; \"\n                f\"prefix caching will be disabled.\",\n                UserWarning,\n                stacklevel=2,\n            )\n        super().__init__(tokenizer=self.tokenizer)\n\n    @classmethod\n    def from_name(cls, model_name, **kwargs):\n        \"\"\"Create an `AsyncMlxLM` instance from a model name.\n\n        Args:\n            model_name (str): Name of the model to load. Can be a Hugging Face\n                model identifier or local path.\n            **kwargs: Additional arguments passed to `AsyncMlxLM` constructor,\n                such as `batch_size`, `timeout`, `prefill_step_size`, `cache_size`.\n\n        Returns:\n            AsyncMlxLM: An `AsyncMlxLM` instance with the loaded model and tokenizer.\n        \"\"\"\n\n        model, tokenizer = mlx_lm.load(model_name)\n        return cls(model, tokenizer, **kwargs)\n\n    @staticmethod\n    def _to_torch(logprobs):\n        \"\"\"Convert MLX arrays into PyTorch tensors.\"\"\"\n        if logprobs.dtype == mx.bfloat16:\n            logprobs = logprobs.astype(mx.float16)\n        return torch.tensor(logprobs)\n\n    @staticmethod\n    def _kv_cachable(mlx_lm_model):\n        \"\"\"Check if an MLX model supports KV cache storage.\n\n        A model is KV-cacheable if all its cache layers are KVCache or\n        RotatingKVCache with keep=0.\n        \"\"\"\n        if not hasattr(mlx_lm_model, \"make_cache\"):\n            return True\n        cache = mlx_lm_model.make_cache()\n        return all(\n            isinstance(c, KVCache)\n            or (isinstance(c, RotatingKVCache) and c.keep == 0)\n            for c in cache\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear the output cache and MLX device cache.\n\n        This method resets the internal token trie cache and clears\n        any cached arrays on the MLX device to free memory.\n        \"\"\"\n        if self.cache is not None:\n            self.cache = DynamicTokenTrie()\n        mx.clear_cache()\n\n    def walk_cache(self, token_ids):\n        \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n        Args:\n            token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n        Returns:\n            tuple: A 5-tuple containing:\n                - node: The deepest node in the cache tree that matches\n                    the token sequence, irregardless of whether its kv is cached or not\n                - next_token_index: Number of tokens matched from the start of token_ids\n                - past_kvs: Past key/value states concatenated from cached nodes, or None if no cached states were found\n                - kv_node: The cache node where KV states start\n                - kv_next_token_index: Number of tokens matched from the start of token_ids for the KV states\n        \"\"\"\n        # Walk while tokens can be found\n        node = self.cache\n        kv_next_token_index = 0\n        kv_node = node\n        collecting = True\n        next_token_index = 0\n        past_kvs = []\n\n        while next_token_index &lt; len(token_ids):\n            if node.past_key_values is not None and collecting:\n                past_kvs.append(node.past_key_values)\n                kv_node = node\n                kv_next_token_index = next_token_index\n            elif next_token_index &gt; 0:\n                collecting = False\n            if node.has_token(token_ids[next_token_index]):\n                node = node.get_token(token_ids[next_token_index])\n                next_token_index += 1\n            else:\n                break\n\n        past_kvs = None if len(past_kvs) == 0 else mx.concatenate(past_kvs, axis=3)\n\n        return node, next_token_index, past_kvs, kv_node, kv_next_token_index\n\n    def cache_kv(self, token_ids):\n        \"\"\"Pre-compute and cache KV states for a given token sequence.\"\"\"\n        query = Query(token_ids, None, None, self.cache, 0)\n        self._batch_logits_custom([query])\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n        to completion.\"\"\"\n        self.queries = []\n\n    def add_to_cache(self, queries, prompt_cache=None, logprobs=None):\n        \"\"\"Add computed log probabilities and KV states to the cache tree.\"\"\"\n        left_paddings = prompt_cache[0].left_padding.tolist()\n        for i, query in enumerate(queries):\n            token_ids, node, next_token_index = (\n                query.prompt,\n                query.node,\n                query.next_token_index,\n            )\n            if node is None or next_token_index is None:\n                node = self.cache\n                next_token_index = 0\n            lp = left_paddings[i]\n            if prompt_cache is not None and self.kv_cachable:\n                keys = [\n                    c.keys[i, :, lp + next_token_index : lp + len(token_ids), :]\n                    for c in prompt_cache\n                ]\n                values = [\n                    c.values[i, :, lp + next_token_index : lp + len(token_ids), :]\n                    for c in prompt_cache\n                ]\n                keys = mx.stack(keys, axis=0)\n                values = mx.stack(values, axis=0)\n                keys_values = mx.stack([keys, values], axis=0)\n                node.extend_cache(\n                    next_token_index, token_ids, logprobs[i], keys_values\n                )\n            else:\n                node.extend_cache(next_token_index, token_ids, logprobs[i])\n\n        self.cache.evict_lru_kv(self.cache_size)\n\n    def _process_kv(self, left_paddings, prompt_cache, pasts=None, step_size=256):\n        \"\"\"Process and integrate past KV cache states into prompt cache.\n\n        This method takes past key-value cache states from the cache tree\n        and integrates them into the prompt cache for efficient prefix\n        reuse. It handles padding and alignment of cache states across\n        different query lengths.\n\n        Args:\n            left_paddings (list[int]): Left padding amounts for each query\n                in the batch.\n            prompt_cache (list): List of cache objects to update with\n                past states.\n            pasts (list[mx.array], optional): List of past KV cache states,\n                one per query.\n            step_size (int, optional): Step size for cache size alignment.\n\n        Returns:\n            tuple: A 2-tuple containing:\n                - list: Updated prompt_cache objects\n                - cached_len: Number of tokens that were cached\n        \"\"\"\n        if pasts is None or all(past is None for past in pasts):\n            return prompt_cache, 0\n        max_match_lengths = [0 if past is None else past.shape[3] for past in pasts]\n        min_pos_cached = min(\n            ml + lp for ml, lp in zip(max_match_lengths, left_paddings)\n        )\n        cache_grabs = [max(min_pos_cached - lp, 0) for lp in left_paddings]\n        non_zero_index = next(\n            (i for i, grab in enumerate(cache_grabs) if grab), None\n        )\n        if non_zero_index is None:\n            return prompt_cache, 0\n        _, num_layers, N, _, D = pasts[non_zero_index].shape\n        cache_size = (step_size + min_pos_cached - 1) // step_size * step_size\n        right_paddings = [\n            max(cache_size - lp - max_len, 0)\n            for lp, max_len in zip(left_paddings, max_match_lengths)\n        ]\n        padded_pasts = []\n        for past, lp, rp in zip(pasts, left_paddings, right_paddings):\n            if past is None:\n                padded_pasts.append(mx.zeros((2, num_layers, N, cache_size, D)))\n            else:\n                padded_pasts.append(\n                    mx.pad(\n                        past[:, :, :, : cache_size - lp, :],\n                        ((0, 0), (0, 0), (0, 0), (lp, rp), (0, 0)),\n                    )\n                )\n\n        padded_pasts = mx.stack(padded_pasts, axis=2)\n        for i, cache in enumerate(prompt_cache):\n            cache.keys = padded_pasts[0, i]\n            cache.values = padded_pasts[1, i]\n            cache.offset += min_pos_cached\n            cache._idx += min_pos_cached\n        return prompt_cache, min_pos_cached\n\n    def _process_prompts(self, queries):\n        \"\"\"Process a batch of prompts and compute next-token log probabilities.\"\"\"\n        inputs = [q.prompt for q in queries]\n        pasts = [q.past for q in queries]\n        lengths = [len(p) for p in inputs]\n        max_length = max(lengths)\n        left_padding = [max_length - length for length in lengths]\n        prompt_cache = _make_cache(self.mlx_lm_model, left_padding)\n        inputs_padded = _left_pad_prompts(inputs, max_length=max_length)\n\n        if self.kv_cachable:\n            prompt_cache, cached_len = self._process_kv(\n                left_padding, prompt_cache, pasts\n            )\n        else:\n            cached_len = 0\n        inputs_padded = inputs_padded[:, cached_len:]\n\n        while inputs_padded.shape[1] &gt; 1:\n            n_to_process = min(self.prefill_step_size, inputs_padded.shape[1] - 1)\n            self.mlx_lm_model(inputs_padded[:, :n_to_process], cache=prompt_cache)\n            mx.eval([c.state for c in prompt_cache])\n            inputs_padded = inputs_padded[:, n_to_process:]\n\n        logits = self.mlx_lm_model(inputs_padded, cache=prompt_cache)\n        logits = logits[:, -1, :]\n        logprobs = logits - mx.logsumexp(logits, axis=-1, keepdims=True)\n        mx.async_eval(logprobs)\n\n        return logprobs, prompt_cache\n\n    def _batch_logits_custom(\n        self,\n        queries,\n    ):\n        \"\"\"Compute next-token log probabilities for each query in a batch and add to cache.\n        Args:\n            queries (list[Query]): List of query objects to process.\n        Returns:\n            logprobs (list[torch.Tensor]): List of normalized log probability tensors.\"\"\"\n        with wired_limit(self.mlx_lm_model, [self.generation_stream]):\n            logprobs, prompt_cache = self._process_prompts(queries)\n            logprobs = AsyncMlxLM._to_torch(logprobs)\n        mx.clear_cache()\n        self.add_to_cache(queries, prompt_cache, logprobs)\n        return logprobs\n\n    def batch_evaluate_queries(self):\n        \"\"\"Process a batch of queued language model queries.\"\"\"\n\n        queries, self.queries = self.queries, []\n        if len(queries) == 0:\n            return\n\n        query_groups = defaultdict(list)\n        for query in queries:\n            key = tuple(query.prompt)\n            query_groups[key].append(query)\n\n        # Use one representative query from each group\n        unique_queries = [group[0] for group in query_groups.values()]\n\n        results = self._batch_logits_custom(unique_queries)\n\n        assert len(results) == len(unique_queries)\n\n        for i, q in enumerate(unique_queries):\n            for dup_query in query_groups[tuple(q.prompt)]:\n                dup_query.future.set_result(results[i])\n\n    def add_query(self, query):\n        \"\"\"Add a query to be evaluated in the next batch and reset the timeout.\"\"\"\n        self.queries.append(query)\n\n        if self.timer:\n            self.timer.cancel()\n            self.timer = None\n        if len(self.queries) &gt;= self.batch_size:\n            self.batch_evaluate_queries()\n        else:\n            self.timer = asyncio.get_running_loop().call_later(\n                self.timeout, lambda: self.batch_evaluate_queries()\n            )\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, kv_node, kv_next_token_index = (\n            self.walk_cache(token_ids)\n        )\n        if next_token_index == len(token_ids) and node.logprobs is not None:\n            return node.logprobs\n\n        future = asyncio.get_running_loop().create_future()\n        query = Query(token_ids, future, past, kv_node, kv_next_token_index)\n        self.add_query(query)\n        logprobs = await future\n\n        return logprobs\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, kv_node, kv_next_token_index = (\n            self.walk_cache(token_ids)\n        )\n        if next_token_index == len(token_ids) and node.logprobs is not None:\n            return node.logprobs\n\n        query = Query(token_ids, None, past, kv_node, kv_next_token_index)\n        logprobs = self._batch_logits_custom([query])[0]\n\n        return logprobs\n\n    async def sample(\n        self,\n        prompt_token_ids,\n        max_tokens,\n        eos_token_ids,\n        temperature=1.0,\n        seed=None,\n    ):\n        \"\"\"Sample from the language model.\n\n        Args:\n            prompt_token_ids (list[int]): The token IDs of the prompt to\n                start generation from.\n            max_tokens (int): The maximum number of tokens to generate.\n            eos_token_ids (list[int]): The token IDs that signal\n                end-of-sequence. Generation stops when one of these is\n                sampled.\n            temperature (float, optional): The temperature to use for\n                sampling. Higher values make the distribution more uniform,\n                lower values make it more peaked. Defaults to 1.0.\n            seed (int, optional): The seed for the random number generator.\n                If provided, sets the random seed before sampling.\n                Defaults to None.\n\n        Returns:\n            (list[int]): The sampled token IDs.\n        \"\"\"\n\n        if seed is not None:\n            mx.random.seed(seed)\n\n        sampler = make_sampler(temp=temperature)\n        prompt_token_ids_array = mx.array(prompt_token_ids)\n        token_generator = generate_step(\n            prompt_token_ids_array,\n            self.mlx_lm_model,\n            max_tokens=max_tokens,\n            sampler=sampler,\n        )\n        generated_token_ids = []\n        for sampled, _ in token_generator:\n            if sampled in eos_token_ids:\n                break\n            generated_token_ids.append(sampled)\n        return generated_token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.__init__","title":"<code>__init__(mlx_lm_model, tokenizer, batch_size=5, timeout=0.001, prefill_step_size=2048, cache_size=400)</code>","text":"<p>Initialize an <code>AsyncMlxLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>mlx_lm_model</code> <p>The MLX language model instance.</p> required <code>tokenizer</code> <p>The tokenizer for encoding/decoding text.</p> required <code>batch_size</code> <code>int</code> <p>Maximum number of queries to batch together.</p> <code>5</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait before processing a batch, even if batch_size is not met.</p> <code>0.001</code> <code>prefill_step_size</code> <code>int</code> <p>Number of tokens to process per step during prompt prefilling.</p> <code>2048</code> <code>cache_size</code> <code>int</code> <p>Maximum number of KV cache entries to keep in memory.</p> <code>400</code> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def __init__(\n    self,\n    mlx_lm_model,\n    tokenizer,\n    batch_size=5,\n    timeout=0.001,\n    prefill_step_size=2048,\n    cache_size=400,\n):\n    \"\"\"Initialize an `AsyncMlxLM` instance.\n\n    Args:\n        mlx_lm_model: The MLX language model instance.\n        tokenizer: The tokenizer for encoding/decoding text.\n        batch_size (int, optional): Maximum number of queries to batch\n            together.\n        timeout (float, optional): Maximum time in seconds to wait\n            before processing a batch, even if batch_size is not met.\n        prefill_step_size (int, optional): Number of tokens to process\n            per step during prompt prefilling.\n        cache_size (int, optional): Maximum number of KV cache entries\n            to keep in memory.\n    \"\"\"\n    self.mlx_lm_model = mlx_lm_model\n    self.tokenizer = tokenizer\n    self.cache = DynamicTokenTrie()\n    self.generation_stream = mx.new_stream(mx.default_device())\n    self.queries = []\n    self.timeout = timeout\n    self.timer = None\n    self.prefill_step_size = prefill_step_size\n    self.cache_size = cache_size\n\n    self.batch_size = batch_size\n    self.kv_cachable = self._kv_cachable(self.mlx_lm_model)\n    if not self.kv_cachable:\n        warnings.warn(\n            f\"Model {type(self.mlx_lm_model).__name__} does not support KV caching; \"\n            f\"prefix caching will be disabled.\",\n            UserWarning,\n            stacklevel=2,\n        )\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.from_name","title":"<code>from_name(model_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an <code>AsyncMlxLM</code> instance from a model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load. Can be a Hugging Face model identifier or local path.</p> required <code>**kwargs</code> <p>Additional arguments passed to <code>AsyncMlxLM</code> constructor, such as <code>batch_size</code>, <code>timeout</code>, <code>prefill_step_size</code>, <code>cache_size</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AsyncMlxLM</code> <p>An <code>AsyncMlxLM</code> instance with the loaded model and tokenizer.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, **kwargs):\n    \"\"\"Create an `AsyncMlxLM` instance from a model name.\n\n    Args:\n        model_name (str): Name of the model to load. Can be a Hugging Face\n            model identifier or local path.\n        **kwargs: Additional arguments passed to `AsyncMlxLM` constructor,\n            such as `batch_size`, `timeout`, `prefill_step_size`, `cache_size`.\n\n    Returns:\n        AsyncMlxLM: An `AsyncMlxLM` instance with the loaded model and tokenizer.\n    \"\"\"\n\n    model, tokenizer = mlx_lm.load(model_name)\n    return cls(model, tokenizer, **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the output cache and MLX device cache.</p> <p>This method resets the internal token trie cache and clears any cached arrays on the MLX device to free memory.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the output cache and MLX device cache.\n\n    This method resets the internal token trie cache and clears\n    any cached arrays on the MLX device to free memory.\n    \"\"\"\n    if self.cache is not None:\n        self.cache = DynamicTokenTrie()\n    mx.clear_cache()\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.walk_cache","title":"<code>walk_cache(token_ids)</code>","text":"<p>Walk the cache tree to find the deepest node matching a sequence of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Sequence of token IDs to follow in the cache tree</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A 5-tuple containing: - node: The deepest node in the cache tree that matches     the token sequence, irregardless of whether its kv is cached or not - next_token_index: Number of tokens matched from the start of token_ids - past_kvs: Past key/value states concatenated from cached nodes, or None if no cached states were found - kv_node: The cache node where KV states start - kv_next_token_index: Number of tokens matched from the start of token_ids for the KV states</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def walk_cache(self, token_ids):\n    \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n    Args:\n        token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n    Returns:\n        tuple: A 5-tuple containing:\n            - node: The deepest node in the cache tree that matches\n                the token sequence, irregardless of whether its kv is cached or not\n            - next_token_index: Number of tokens matched from the start of token_ids\n            - past_kvs: Past key/value states concatenated from cached nodes, or None if no cached states were found\n            - kv_node: The cache node where KV states start\n            - kv_next_token_index: Number of tokens matched from the start of token_ids for the KV states\n    \"\"\"\n    # Walk while tokens can be found\n    node = self.cache\n    kv_next_token_index = 0\n    kv_node = node\n    collecting = True\n    next_token_index = 0\n    past_kvs = []\n\n    while next_token_index &lt; len(token_ids):\n        if node.past_key_values is not None and collecting:\n            past_kvs.append(node.past_key_values)\n            kv_node = node\n            kv_next_token_index = next_token_index\n        elif next_token_index &gt; 0:\n            collecting = False\n        if node.has_token(token_ids[next_token_index]):\n            node = node.get_token(token_ids[next_token_index])\n            next_token_index += 1\n        else:\n            break\n\n    past_kvs = None if len(past_kvs) == 0 else mx.concatenate(past_kvs, axis=3)\n\n    return node, next_token_index, past_kvs, kv_node, kv_next_token_index\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.cache_kv","title":"<code>cache_kv(token_ids)</code>","text":"<p>Pre-compute and cache KV states for a given token sequence.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def cache_kv(self, token_ids):\n    \"\"\"Pre-compute and cache KV states for a given token sequence.\"\"\"\n    query = Query(token_ids, None, None, self.cache, 0)\n    self._batch_logits_custom([query])\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing to completion.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n    to completion.\"\"\"\n    self.queries = []\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.add_to_cache","title":"<code>add_to_cache(queries, prompt_cache=None, logprobs=None)</code>","text":"<p>Add computed log probabilities and KV states to the cache tree.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def add_to_cache(self, queries, prompt_cache=None, logprobs=None):\n    \"\"\"Add computed log probabilities and KV states to the cache tree.\"\"\"\n    left_paddings = prompt_cache[0].left_padding.tolist()\n    for i, query in enumerate(queries):\n        token_ids, node, next_token_index = (\n            query.prompt,\n            query.node,\n            query.next_token_index,\n        )\n        if node is None or next_token_index is None:\n            node = self.cache\n            next_token_index = 0\n        lp = left_paddings[i]\n        if prompt_cache is not None and self.kv_cachable:\n            keys = [\n                c.keys[i, :, lp + next_token_index : lp + len(token_ids), :]\n                for c in prompt_cache\n            ]\n            values = [\n                c.values[i, :, lp + next_token_index : lp + len(token_ids), :]\n                for c in prompt_cache\n            ]\n            keys = mx.stack(keys, axis=0)\n            values = mx.stack(values, axis=0)\n            keys_values = mx.stack([keys, values], axis=0)\n            node.extend_cache(\n                next_token_index, token_ids, logprobs[i], keys_values\n            )\n        else:\n            node.extend_cache(next_token_index, token_ids, logprobs[i])\n\n    self.cache.evict_lru_kv(self.cache_size)\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.batch_evaluate_queries","title":"<code>batch_evaluate_queries()</code>","text":"<p>Process a batch of queued language model queries.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def batch_evaluate_queries(self):\n    \"\"\"Process a batch of queued language model queries.\"\"\"\n\n    queries, self.queries = self.queries, []\n    if len(queries) == 0:\n        return\n\n    query_groups = defaultdict(list)\n    for query in queries:\n        key = tuple(query.prompt)\n        query_groups[key].append(query)\n\n    # Use one representative query from each group\n    unique_queries = [group[0] for group in query_groups.values()]\n\n    results = self._batch_logits_custom(unique_queries)\n\n    assert len(results) == len(unique_queries)\n\n    for i, q in enumerate(unique_queries):\n        for dup_query in query_groups[tuple(q.prompt)]:\n            dup_query.future.set_result(results[i])\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.add_query","title":"<code>add_query(query)</code>","text":"<p>Add a query to be evaluated in the next batch and reset the timeout.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def add_query(self, query):\n    \"\"\"Add a query to be evaluated in the next batch and reset the timeout.\"\"\"\n    self.queries.append(query)\n\n    if self.timer:\n        self.timer.cancel()\n        self.timer = None\n    if len(self.queries) &gt;= self.batch_size:\n        self.batch_evaluate_queries()\n    else:\n        self.timer = asyncio.get_running_loop().call_later(\n            self.timeout, lambda: self.batch_evaluate_queries()\n        )\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, kv_node, kv_next_token_index = (\n        self.walk_cache(token_ids)\n    )\n    if next_token_index == len(token_ids) and node.logprobs is not None:\n        return node.logprobs\n\n    future = asyncio.get_running_loop().create_future()\n    query = Query(token_ids, future, past, kv_node, kv_next_token_index)\n    self.add_query(query)\n    logprobs = await future\n\n    return logprobs\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, kv_node, kv_next_token_index = (\n        self.walk_cache(token_ids)\n    )\n    if next_token_index == len(token_ids) and node.logprobs is not None:\n        return node.logprobs\n\n    query = Query(token_ids, None, past, kv_node, kv_next_token_index)\n    logprobs = self._batch_logits_custom([query])[0]\n\n    return logprobs\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.AsyncMlxLM.sample","title":"<code>sample(prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids</code> <code>list[int]</code> <p>The token IDs of the prompt to start generation from.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs that signal end-of-sequence. Generation stops when one of these is sampled.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use for sampling. Higher values make the distribution more uniform, lower values make it more peaked. Defaults to 1.0.</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator. If provided, sets the random seed before sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>async def sample(\n    self,\n    prompt_token_ids,\n    max_tokens,\n    eos_token_ids,\n    temperature=1.0,\n    seed=None,\n):\n    \"\"\"Sample from the language model.\n\n    Args:\n        prompt_token_ids (list[int]): The token IDs of the prompt to\n            start generation from.\n        max_tokens (int): The maximum number of tokens to generate.\n        eos_token_ids (list[int]): The token IDs that signal\n            end-of-sequence. Generation stops when one of these is\n            sampled.\n        temperature (float, optional): The temperature to use for\n            sampling. Higher values make the distribution more uniform,\n            lower values make it more peaked. Defaults to 1.0.\n        seed (int, optional): The seed for the random number generator.\n            If provided, sets the random seed before sampling.\n            Defaults to None.\n\n    Returns:\n        (list[int]): The sampled token IDs.\n    \"\"\"\n\n    if seed is not None:\n        mx.random.seed(seed)\n\n    sampler = make_sampler(temp=temperature)\n    prompt_token_ids_array = mx.array(prompt_token_ids)\n    token_generator = generate_step(\n        prompt_token_ids_array,\n        self.mlx_lm_model,\n        max_tokens=max_tokens,\n        sampler=sampler,\n    )\n    generated_token_ids = []\n    for sampled, _ in token_generator:\n        if sampled in eos_token_ids:\n            break\n        generated_token_ids.append(sampled)\n    return generated_token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/__init__/#genlm.backend.llm.load_model_by_name","title":"<code>load_model_by_name(name, backend=None, llm_opts=None)</code>","text":"<p>Load a language model by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Hugging Face model name (e.g. \"gpt2\", \"meta-llama/Llama-3.2-1B-Instruct\")</p> required <code>backend</code> <code>str</code> <p>Backend to use for inference. Can be \"vllm\", \"hf\" or \"mock\". If None, defaults to \"vllm\" if CUDA is available, otherwise \"hf\".</p> <code>None</code> <code>llm_opts</code> <code>dict</code> <p>Additional options to pass to the backend constructor. See AsyncVirtualLM and AsyncTransformer documentation for details.</p> <code>None</code> <p>Returns:</p> Type Description <code>AsyncLM</code> <p>An asynchronous language model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid backend is specified.</p> Source code in <code>genlm/backend/llm/__init__.py</code> <pre><code>def load_model_by_name(name, backend=None, llm_opts=None):\n    \"\"\"Load a language model by name.\n\n    Args:\n        name (str): Hugging Face model name (e.g. \"gpt2\", \"meta-llama/Llama-3.2-1B-Instruct\")\n        backend (str, optional): Backend to use for inference. Can be \"vllm\", \"hf\" or \"mock\".\n            If None, defaults to \"vllm\" if CUDA is available, otherwise \"hf\".\n        llm_opts (dict, optional): Additional options to pass to the backend constructor.\n            See AsyncVirtualLM and AsyncTransformer documentation for details.\n\n    Returns:\n        (AsyncLM): An asynchronous language model.\n\n    Raises:\n        (ValueError): If an invalid backend is specified.\n    \"\"\"\n    if backend is None:\n        backend = \"vllm\" if torch.cuda.is_available() else \"hf\"\n\n    if llm_opts is None:\n        llm_opts = {}\n\n    if backend == \"vllm\":\n        return AsyncVirtualLM.from_name(name, **llm_opts)\n    elif backend == \"hf\":\n        return AsyncTransformer.from_name(name, **llm_opts)\n    elif backend == \"mock\":\n        return MockAsyncLM.from_name(name, **llm_opts)\n    elif backend == \"mlx\":\n        return AsyncMlxLM.from_name(name, **llm_opts)\n    else:\n        raise ValueError(f\"Invalid backend: {backend}\")\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/","title":"base","text":""},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.AsyncLM","title":"<code>AsyncLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for asynchronous language models.</p> <p>This class provides an interface for language models that can generate token probabilities asynchronously. It handles tokenization and vocabulary management.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance compatible with the language model</p> required Source code in <code>genlm/backend/llm/base.py</code> <pre><code>class AsyncLM(ABC):\n    \"\"\"Abstract base class for asynchronous language models.\n\n    This class provides an interface for language models that can generate token probabilities\n    asynchronously. It handles tokenization and vocabulary management.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance compatible with the language model\n    \"\"\"\n\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.byte_vocab, self.str_vocab = decode_vocab(self.tokenizer)\n\n    @abstractmethod\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs representing the prompt.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs representing the prompt.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        pass\n\n    async def batch_next_token_logprobs(self, token_ids_list):\n        \"\"\"Batch request log probabilities for multiple token sequences asynchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists.\n\n        Returns:\n            (torch.Tensor): A tensor of log probability tensors.\n        \"\"\"\n        logprobs = await asyncio.gather(\n            *[self.next_token_logprobs(token_ids) for token_ids in token_ids_list]\n        )\n\n        return torch.stack(logprobs)\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"Batch request log probabilities for multiple token sequences synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists.\n\n        Returns:\n            (torch.Tensor): A tensor of log probability tensors.\n        \"\"\"\n        return torch.stack(\n            [self.next_token_logprobs_sync(token_ids) for token_ids in token_ids_list]\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear any caches used by the language model. No-op in base class.\"\"\"\n        pass  # pragma: no cover\n\n    async def sample(\n        self, prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None\n    ):\n        \"\"\"Sample from the language model.\n\n        Args:\n            prompt_token_ids (list[int]): The token IDs of the prompt.\n            eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n            temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n            max_tokens (int): The maximum number of tokens to generate.\n            seed (int, optional): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            (list[int]): The sampled token IDs.\n        \"\"\"\n        if seed is not None:\n            generator = torch.Generator()\n            generator.manual_seed(seed)\n        else:\n            generator = None\n\n        generated_token_ids = []\n        for _ in range(max_tokens):\n            logprobs = await self.next_token_logprobs(\n                prompt_token_ids + generated_token_ids\n            )\n            probs = torch.softmax(logprobs / temperature, dim=-1)\n            next_token_id = torch.multinomial(\n                probs.cpu() if seed is not None else probs,\n                num_samples=1,\n                generator=generator,\n            ).item()\n            if next_token_id in eos_token_ids:\n                break\n            generated_token_ids.append(next_token_id)\n\n        return generated_token_ids\n\n    async def batch_sample(\n        self,\n        prompt_token_ids_list,\n        max_tokens,\n        eos_token_ids,\n        temperature=1.0,\n        seed=None,\n    ):\n        \"\"\"Batch sample from the language model.\n\n        Args:\n            prompt_token_ids_list (list[list[int]]): The token IDs of the prompts.\n            max_tokens (int): The maximum number of tokens to generate.\n            eos_token_ids (list[int]): The token IDs of the end-of-sequence token.\n            temperature (float): The temperature to use for the logits.\n            seed (int, optional): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            (list[list[int]]): The sampled token IDs.\n        \"\"\"\n        return await asyncio.gather(\n            *[\n                self.sample(\n                    prompt_token_ids=prompt_token_ids,\n                    max_tokens=max_tokens,\n                    eos_token_ids=eos_token_ids,\n                    temperature=temperature,\n                    seed=seed,\n                )\n                for prompt_token_ids in prompt_token_ids_list\n            ]\n        )\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.AsyncLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Request log probabilities of next token asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs representing the prompt.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>@abstractmethod\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs representing the prompt.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.AsyncLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>  <code>abstractmethod</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs representing the prompt.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>@abstractmethod\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs representing the prompt.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.AsyncLM.batch_next_token_logprobs","title":"<code>batch_next_token_logprobs(token_ids_list)</code>  <code>async</code>","text":"<p>Batch request log probabilities for multiple token sequences asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of log probability tensors.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>async def batch_next_token_logprobs(self, token_ids_list):\n    \"\"\"Batch request log probabilities for multiple token sequences asynchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists.\n\n    Returns:\n        (torch.Tensor): A tensor of log probability tensors.\n    \"\"\"\n    logprobs = await asyncio.gather(\n        *[self.next_token_logprobs(token_ids) for token_ids in token_ids_list]\n    )\n\n    return torch.stack(logprobs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.AsyncLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Batch request log probabilities for multiple token sequences synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of log probability tensors.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"Batch request log probabilities for multiple token sequences synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists.\n\n    Returns:\n        (torch.Tensor): A tensor of log probability tensors.\n    \"\"\"\n    return torch.stack(\n        [self.next_token_logprobs_sync(token_ids) for token_ids in token_ids_list]\n    )\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.AsyncLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear any caches used by the language model. No-op in base class.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear any caches used by the language model. No-op in base class.\"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.AsyncLM.sample","title":"<code>sample(prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids</code> <code>list[int]</code> <p>The token IDs of the prompt.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs of the end-of-sequence tokens.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use to rescale the logits. Defaults to 1.0.</p> <code>1.0</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>async def sample(\n    self, prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None\n):\n    \"\"\"Sample from the language model.\n\n    Args:\n        prompt_token_ids (list[int]): The token IDs of the prompt.\n        eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n        temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n        max_tokens (int): The maximum number of tokens to generate.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        (list[int]): The sampled token IDs.\n    \"\"\"\n    if seed is not None:\n        generator = torch.Generator()\n        generator.manual_seed(seed)\n    else:\n        generator = None\n\n    generated_token_ids = []\n    for _ in range(max_tokens):\n        logprobs = await self.next_token_logprobs(\n            prompt_token_ids + generated_token_ids\n        )\n        probs = torch.softmax(logprobs / temperature, dim=-1)\n        next_token_id = torch.multinomial(\n            probs.cpu() if seed is not None else probs,\n            num_samples=1,\n            generator=generator,\n        ).item()\n        if next_token_id in eos_token_ids:\n            break\n        generated_token_ids.append(next_token_id)\n\n    return generated_token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.AsyncLM.batch_sample","title":"<code>batch_sample(prompt_token_ids_list, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Batch sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids_list</code> <code>list[list[int]]</code> <p>The token IDs of the prompts.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs of the end-of-sequence token.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use for the logits.</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[int]]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>async def batch_sample(\n    self,\n    prompt_token_ids_list,\n    max_tokens,\n    eos_token_ids,\n    temperature=1.0,\n    seed=None,\n):\n    \"\"\"Batch sample from the language model.\n\n    Args:\n        prompt_token_ids_list (list[list[int]]): The token IDs of the prompts.\n        max_tokens (int): The maximum number of tokens to generate.\n        eos_token_ids (list[int]): The token IDs of the end-of-sequence token.\n        temperature (float): The temperature to use for the logits.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        (list[list[int]]): The sampled token IDs.\n    \"\"\"\n    return await asyncio.gather(\n        *[\n            self.sample(\n                prompt_token_ids=prompt_token_ids,\n                max_tokens=max_tokens,\n                eos_token_ids=eos_token_ids,\n                temperature=temperature,\n                seed=seed,\n            )\n            for prompt_token_ids in prompt_token_ids_list\n        ]\n    )\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.MockAsyncLM","title":"<code>MockAsyncLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Mock implementation of AsyncLM used for testing.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>class MockAsyncLM(AsyncLM):\n    \"\"\"Mock implementation of AsyncLM used for testing.\"\"\"\n\n    def __init__(self, tokenizer):\n        \"\"\"Initialize a `MockAsyncLM` instance.\n\n        Args:\n            tokenizer: Hugging Face tokenizer instance\n        \"\"\"\n        super().__init__(tokenizer)\n        self._rng = np.random.RandomState(42)\n\n    @classmethod\n    def from_name(cls, model_name, **kwargs):\n        \"\"\"Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.\n\n        Args:\n            model_name (str): Name of pretrained model to load tokenizer from\n            **kwargs: Additional arguments passed to `MockAsyncLM` constructor\n\n        Returns:\n            (MockAsyncLM): `MockAsyncLM` instance initialized with tokenizer from `model_name`\n        \"\"\"\n        from transformers import AutoTokenizer\n\n        return cls(AutoTokenizer.from_pretrained(model_name), **kwargs)\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Get next token log probabilities asynchronously.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self._get_logprobs(token_ids)\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Get next token log probabilities synchronously.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self._get_logprobs(token_ids)\n\n    def _get_logprobs(self, token_ids):\n        \"\"\"Generate random but deterministic log probabilities for given tokens.\n\n        Uses token_ids to seed the random generator, ensuring same inputs produce same outputs.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        seed = sum([(i + 1) * t for i, t in enumerate(token_ids)])\n        self._rng.seed(seed)\n        logits = torch.from_numpy(\n            self._rng.rand(len(self.tokenizer)).astype(np.float32)\n        )\n        return torch.log_softmax(logits, dim=-1)\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.MockAsyncLM.__init__","title":"<code>__init__(tokenizer)</code>","text":"<p>Initialize a <code>MockAsyncLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>Hugging Face tokenizer instance</p> required Source code in <code>genlm/backend/llm/base.py</code> <pre><code>def __init__(self, tokenizer):\n    \"\"\"Initialize a `MockAsyncLM` instance.\n\n    Args:\n        tokenizer: Hugging Face tokenizer instance\n    \"\"\"\n    super().__init__(tokenizer)\n    self._rng = np.random.RandomState(42)\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.MockAsyncLM.from_name","title":"<code>from_name(model_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of pretrained model to load tokenizer from</p> required <code>**kwargs</code> <p>Additional arguments passed to <code>MockAsyncLM</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>MockAsyncLM</code> <p><code>MockAsyncLM</code> instance initialized with tokenizer from <code>model_name</code></p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, **kwargs):\n    \"\"\"Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.\n\n    Args:\n        model_name (str): Name of pretrained model to load tokenizer from\n        **kwargs: Additional arguments passed to `MockAsyncLM` constructor\n\n    Returns:\n        (MockAsyncLM): `MockAsyncLM` instance initialized with tokenizer from `model_name`\n    \"\"\"\n    from transformers import AutoTokenizer\n\n    return cls(AutoTokenizer.from_pretrained(model_name), **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.MockAsyncLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Get next token log probabilities asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Get next token log probabilities asynchronously.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self._get_logprobs(token_ids)\n</code></pre>"},{"location":"reference/genlm/backend/llm/base/#genlm.backend.llm.base.MockAsyncLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Get next token log probabilities synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/base.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Get next token log probabilities synchronously.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self._get_logprobs(token_ids)\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/","title":"hf","text":""},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.Query","title":"<code>Query</code>","text":"<p>A query to a language model, waiting to be batched.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>class Query:\n    \"\"\"A query to a language model, waiting to be batched.\"\"\"\n\n    def __init__(self, prompt, future, past=None):\n        self.prompt = prompt\n        self.future = future\n        self.past = past\n\n        if self.past is not None:\n            self.past_len = past[\n                0\n            ][\n                0\n            ].shape[\n                2\n            ]  # layers, key or value, batch size, num heads, num tokens, head repr length\n        else:\n            self.past_len = 0\n\n    @torch.no_grad()\n    def past_padded(self, layer, j, to_length, dtype, device, past_shape):\n        if self.past is not None:\n            return torch.cat(\n                (\n                    self.past[layer][j],\n                    torch.zeros(\n                        1,\n                        past_shape[1],\n                        to_length - self.past_len,\n                        past_shape[3],\n                        dtype=dtype,\n                        device=device,\n                    ),\n                ),\n                dim=2,\n            )\n        else:\n            return torch.zeros(\n                1, past_shape[1], to_length, past_shape[3], dtype=dtype, device=device\n            )\n\n    def prompt_padded(self, pad_token, to_length):\n        return [*self.prompt, *[pad_token for _ in range(to_length - len(self.prompt))]]\n\n    def attention_mask(self, total_past_length, total_seq_length):\n        return [\n            *[1 for _ in range(self.past_len)],\n            *[0 for _ in range(total_past_length - self.past_len)],\n            *[1 for _ in range(len(self.prompt))],\n            *[0 for _ in range(total_seq_length - len(self.prompt))],\n        ]\n\n    def position_ids(self, total_past_length, total_seq_length):\n        return [\n            *range(self.past_len, self.past_len + len(self.prompt)),\n            *[0 for _ in range(total_seq_length - len(self.prompt))],\n        ]\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer","title":"<code>AsyncTransformer</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Asynchronous wrapper around a HuggingFace causal language model with caching support.</p> <p>This class provides an asynchronous interface to HuggingFace language models with automatic batching and caching (output and KV) for improved efficiency.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>class AsyncTransformer(AsyncLM):\n    \"\"\"Asynchronous wrapper around a HuggingFace causal language model with caching support.\n\n    This class provides an asynchronous interface to HuggingFace language models with automatic batching\n    and caching (output and KV) for improved efficiency.\n    \"\"\"\n\n    @classmethod\n    def from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n        \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n        Args:\n            model_id (str): Model identifier in HuggingFace's model hub.\n            bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n                Defaults to None.\n            hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n                Defaults to None.\n            **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n        Returns:\n            (AsyncTransformer): An initialized `AsyncTransformer` instance.\n        \"\"\"\n        if bitsandbytes_opts:\n            bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n        else:\n            bnb_config = None\n\n        _hf_opts = {\n            \"device_map\": \"auto\",\n            \"torch_dtype\": \"auto\",\n        }\n        if hf_opts:\n            _hf_opts.update(hf_opts)\n\n        tok = AutoTokenizer.from_pretrained(model_id)\n        mod = AutoModelForCausalLM.from_pretrained(\n            model_id, quantization_config=bnb_config, **_hf_opts\n        )\n\n        return cls(mod, tok, **kwargs)\n\n    @torch.no_grad()\n    def __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n        \"\"\"Initialize an AsyncTransformer instance.\n\n        Args:\n            hf_model: A HuggingFace CausalLM model instance.\n            hf_tokenizer: A HuggingFace Tokenizer.\n            batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n                Defaults to 20.\n            timeout (float, optional): Seconds to wait since last query before processing current batch.\n                Defaults to 0.02.\n        \"\"\"\n        self.model = hf_model\n        self.tokenizer = hf_tokenizer\n        self.device = hf_model.device\n        self.cache = TokenTrie()\n\n        # Queries to be batched. Each query is a sequence of tokens,\n        # and a Future to be called when the query is resolved.\n        self.queries = []\n        self.batch_size = batch_size\n        self.timeout = timeout\n        self.timer = None\n\n        self.model.eval()\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    def clear_cache(self):\n        \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n        self.cache = TokenTrie()\n\n    def clear_kv_cache(self):\n        \"\"\"Clear any key and value vectors from the cache.\"\"\"\n        self.cache.clear_kv_cache()\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n        to completion.\"\"\"\n        self.queries = []\n\n    @torch.no_grad()\n    def cache_kv(self, prompt_tokens):\n        \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n        Args:\n            prompt_tokens (list[int]): token ids for the prompt to cache.\n        \"\"\"\n        result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n        node = self.cache.extend_cache(0, prompt_tokens, result.logits[0], 0)\n        node.past_key_values = result.past_key_values\n\n    @torch.no_grad()\n    def batch_evaluate_queries(self):\n        \"\"\"\n        Process a batch of queued language model queries.\n\n        This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n        \"\"\"\n\n        queries, self.queries = self.queries, []\n        if len(queries) == 0:\n            return\n\n        query_groups = defaultdict(list)\n        for query in queries:\n            key = tuple(query.prompt)  # XXX: cache based on past_len too?\n            query_groups[key].append(query)\n\n        # Use one representative query from each group\n        unique_queries = [group[0] for group in query_groups.values()]\n\n        past_example = next((q.past for q in unique_queries if q.past), False)\n        max_past_length = max(q.past_len for q in unique_queries)\n        max_query_length = max(len(q.prompt) for q in unique_queries)\n\n        padding_token_id = (\n            self.tokenizer.pad_token_id\n            if self.tokenizer.pad_token_id is not None\n            else 0\n        )\n\n        input_ids = torch.tensor(\n            [\n                q.prompt_padded(padding_token_id, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        attn_masks = torch.tensor(\n            [\n                q.attention_mask(max_past_length, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        posn_ids = torch.tensor(\n            [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n        ).to(self.device)\n        if past_example:\n            pasts = [\n                [\n                    torch.cat(\n                        (\n                            *(\n                                q.past_padded(\n                                    layer,\n                                    j,\n                                    max_past_length,\n                                    past_example[0][0].dtype,\n                                    self.device,\n                                    past_example[0][0].shape,\n                                )\n                                for q in unique_queries\n                            ),\n                        ),\n                        dim=0,\n                    )\n                    for j in range(2)\n                ]\n                for layer in range(len(past_example))\n            ]\n        else:\n            pasts = None\n\n        pasts = DynamicCache.from_legacy_cache(pasts)\n\n        results = self.model(\n            input_ids,\n            attention_mask=attn_masks,\n            position_ids=posn_ids,\n            past_key_values=pasts,\n            use_cache=pasts is not None,\n        )\n\n        assert len(results.logits) == len(unique_queries)\n\n        for i, q in enumerate(unique_queries):\n            result = results.logits[i]\n            for dup_query in query_groups[tuple(q.prompt)]:\n                dup_query.future.set_result(result)\n\n    @torch.no_grad()\n    def add_query(self, query, future, past):\n        \"\"\"Add a query to be evaluated in the next batch.\n\n        This method is called internally when a `next_token_logprobs` request is made.\n\n        Args:\n            query (list[int]): Token IDs representing the query prompt\n            future (asyncio.Future): Future to store the result in\n            past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n                or None if this is a new query\n        \"\"\"\n        self.queries.append(Query(query, future, past))\n\n        if self.timer:\n            self.timer.cancel()\n            self.timer = None\n        if len(self.queries) &gt;= self.batch_size:\n            self.batch_evaluate_queries()\n        else:\n            self.timer = asyncio.get_running_loop().call_later(\n                self.timeout, lambda: self.batch_evaluate_queries()\n            )\n\n    def walk_cache(self, token_ids):\n        \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n        Args:\n            token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n        Returns:\n            tuple:\n                - CacheNode: The deepest node in the cache tree that matches the token sequence\n                - int: Number of tokens matched from the start of token_ids\n                - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                    or None if no cached states were found\n                - int: Base index indicating where the past states start in token_ids\n        \"\"\"\n        # Walk while tokens can be found\n        node = self.cache\n        next_token_index = 0\n\n        past = None\n        base = 0\n        while next_token_index &lt; len(token_ids):\n            if node.past_key_values is not None:\n                past = node.past_key_values\n                base = next_token_index\n            if node.has_token(token_ids[next_token_index]):\n                node = node.get_token(token_ids[next_token_index])\n                next_token_index += 1\n            else:\n                break\n\n        return node, next_token_index, past, base\n\n    @torch.no_grad()\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        # If we processed all tokens, then we're done.\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        # Create a future with the prompt\n        future = asyncio.get_running_loop().create_future()\n        self.add_query(token_ids[base:], future, past)\n        logits = await future\n\n        # Create new nodes\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    @torch.no_grad()\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        # Walk while tokens can be found\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        logits = self.model(\n            torch.tensor([token_ids[base:]]).to(self.device),\n            past_key_values=node.past_key_values,\n            use_cache=node.past_key_values is not None,\n        ).logits[0]\n\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    def next_token_logprobs_uncached(self, token_ids):\n        \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        with torch.no_grad():\n            logits = self.model(\n                torch.tensor([token_ids]).to(self.device),\n                past_key_values=None,\n                use_cache=False,\n            ).logits[0]\n            return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.from_name","title":"<code>from_name(model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an AsyncTransformer instance from a pretrained HuggingFace model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Model identifier in HuggingFace's model hub.</p> required <code>bitsandbytes_opts</code> <code>dict</code> <p>Additional configuration options for bitsandbytes quantization. Defaults to None.</p> <code>None</code> <code>hf_opts</code> <code>dict</code> <p>Additional configuration options for loading the HuggingFace model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the <code>AsyncTransformer</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTransformer</code> <p>An initialized <code>AsyncTransformer</code> instance.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@classmethod\ndef from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n    \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n    Args:\n        model_id (str): Model identifier in HuggingFace's model hub.\n        bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n            Defaults to None.\n        hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n            Defaults to None.\n        **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n    Returns:\n        (AsyncTransformer): An initialized `AsyncTransformer` instance.\n    \"\"\"\n    if bitsandbytes_opts:\n        bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n    else:\n        bnb_config = None\n\n    _hf_opts = {\n        \"device_map\": \"auto\",\n        \"torch_dtype\": \"auto\",\n    }\n    if hf_opts:\n        _hf_opts.update(hf_opts)\n\n    tok = AutoTokenizer.from_pretrained(model_id)\n    mod = AutoModelForCausalLM.from_pretrained(\n        model_id, quantization_config=bnb_config, **_hf_opts\n    )\n\n    return cls(mod, tok, **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.__init__","title":"<code>__init__(hf_model, hf_tokenizer, batch_size=20, timeout=0.02)</code>","text":"<p>Initialize an AsyncTransformer instance.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <p>A HuggingFace CausalLM model instance.</p> required <code>hf_tokenizer</code> <p>A HuggingFace Tokenizer.</p> required <code>batch_size</code> <code>int</code> <p>Maximum queries to process in one batch during auto-batching. Defaults to 20.</p> <code>20</code> <code>timeout</code> <code>float</code> <p>Seconds to wait since last query before processing current batch. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n    \"\"\"Initialize an AsyncTransformer instance.\n\n    Args:\n        hf_model: A HuggingFace CausalLM model instance.\n        hf_tokenizer: A HuggingFace Tokenizer.\n        batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n            Defaults to 20.\n        timeout (float, optional): Seconds to wait since last query before processing current batch.\n            Defaults to 0.02.\n    \"\"\"\n    self.model = hf_model\n    self.tokenizer = hf_tokenizer\n    self.device = hf_model.device\n    self.cache = TokenTrie()\n\n    # Queries to be batched. Each query is a sequence of tokens,\n    # and a Future to be called when the query is resolved.\n    self.queries = []\n    self.batch_size = batch_size\n    self.timeout = timeout\n    self.timer = None\n\n    self.model.eval()\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache of log probabilities and key/value pairs.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n    self.cache = TokenTrie()\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.clear_kv_cache","title":"<code>clear_kv_cache()</code>","text":"<p>Clear any key and value vectors from the cache.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def clear_kv_cache(self):\n    \"\"\"Clear any key and value vectors from the cache.\"\"\"\n    self.cache.clear_kv_cache()\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing to completion.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n    to completion.\"\"\"\n    self.queries = []\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.cache_kv","title":"<code>cache_kv(prompt_tokens)</code>","text":"<p>Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_tokens</code> <code>list[int]</code> <p>token ids for the prompt to cache.</p> required Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef cache_kv(self, prompt_tokens):\n    \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n    Args:\n        prompt_tokens (list[int]): token ids for the prompt to cache.\n    \"\"\"\n    result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n    node = self.cache.extend_cache(0, prompt_tokens, result.logits[0], 0)\n    node.past_key_values = result.past_key_values\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.batch_evaluate_queries","title":"<code>batch_evaluate_queries()</code>","text":"<p>Process a batch of queued language model queries.</p> <p>This method is called internally when the <code>batch_size</code> has been met or the <code>timeout</code> has expired.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef batch_evaluate_queries(self):\n    \"\"\"\n    Process a batch of queued language model queries.\n\n    This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n    \"\"\"\n\n    queries, self.queries = self.queries, []\n    if len(queries) == 0:\n        return\n\n    query_groups = defaultdict(list)\n    for query in queries:\n        key = tuple(query.prompt)  # XXX: cache based on past_len too?\n        query_groups[key].append(query)\n\n    # Use one representative query from each group\n    unique_queries = [group[0] for group in query_groups.values()]\n\n    past_example = next((q.past for q in unique_queries if q.past), False)\n    max_past_length = max(q.past_len for q in unique_queries)\n    max_query_length = max(len(q.prompt) for q in unique_queries)\n\n    padding_token_id = (\n        self.tokenizer.pad_token_id\n        if self.tokenizer.pad_token_id is not None\n        else 0\n    )\n\n    input_ids = torch.tensor(\n        [\n            q.prompt_padded(padding_token_id, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    attn_masks = torch.tensor(\n        [\n            q.attention_mask(max_past_length, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    posn_ids = torch.tensor(\n        [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n    ).to(self.device)\n    if past_example:\n        pasts = [\n            [\n                torch.cat(\n                    (\n                        *(\n                            q.past_padded(\n                                layer,\n                                j,\n                                max_past_length,\n                                past_example[0][0].dtype,\n                                self.device,\n                                past_example[0][0].shape,\n                            )\n                            for q in unique_queries\n                        ),\n                    ),\n                    dim=0,\n                )\n                for j in range(2)\n            ]\n            for layer in range(len(past_example))\n        ]\n    else:\n        pasts = None\n\n    pasts = DynamicCache.from_legacy_cache(pasts)\n\n    results = self.model(\n        input_ids,\n        attention_mask=attn_masks,\n        position_ids=posn_ids,\n        past_key_values=pasts,\n        use_cache=pasts is not None,\n    )\n\n    assert len(results.logits) == len(unique_queries)\n\n    for i, q in enumerate(unique_queries):\n        result = results.logits[i]\n        for dup_query in query_groups[tuple(q.prompt)]:\n            dup_query.future.set_result(result)\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.add_query","title":"<code>add_query(query, future, past)</code>","text":"<p>Add a query to be evaluated in the next batch.</p> <p>This method is called internally when a <code>next_token_logprobs</code> request is made.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[int]</code> <p>Token IDs representing the query prompt</p> required <code>future</code> <code>Future</code> <p>Future to store the result in</p> required <code>past</code> <code>list[tuple[Tensor]] | None</code> <p>Past key/value states from previous evaluation, or None if this is a new query</p> required Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef add_query(self, query, future, past):\n    \"\"\"Add a query to be evaluated in the next batch.\n\n    This method is called internally when a `next_token_logprobs` request is made.\n\n    Args:\n        query (list[int]): Token IDs representing the query prompt\n        future (asyncio.Future): Future to store the result in\n        past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n            or None if this is a new query\n    \"\"\"\n    self.queries.append(Query(query, future, past))\n\n    if self.timer:\n        self.timer.cancel()\n        self.timer = None\n    if len(self.queries) &gt;= self.batch_size:\n        self.batch_evaluate_queries()\n    else:\n        self.timer = asyncio.get_running_loop().call_later(\n            self.timeout, lambda: self.batch_evaluate_queries()\n        )\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.walk_cache","title":"<code>walk_cache(token_ids)</code>","text":"<p>Walk the cache tree to find the deepest node matching a sequence of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Sequence of token IDs to follow in the cache tree</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>CacheNode: The deepest node in the cache tree that matches the token sequence</li> <li>int: Number of tokens matched from the start of token_ids</li> <li>list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,     or None if no cached states were found</li> <li>int: Base index indicating where the past states start in token_ids</li> </ul> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def walk_cache(self, token_ids):\n    \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n    Args:\n        token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n    Returns:\n        tuple:\n            - CacheNode: The deepest node in the cache tree that matches the token sequence\n            - int: Number of tokens matched from the start of token_ids\n            - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                or None if no cached states were found\n            - int: Base index indicating where the past states start in token_ids\n    \"\"\"\n    # Walk while tokens can be found\n    node = self.cache\n    next_token_index = 0\n\n    past = None\n    base = 0\n    while next_token_index &lt; len(token_ids):\n        if node.past_key_values is not None:\n            past = node.past_key_values\n            base = next_token_index\n        if node.has_token(token_ids[next_token_index]):\n            node = node.get_token(token_ids[next_token_index])\n            next_token_index += 1\n        else:\n            break\n\n    return node, next_token_index, past, base\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    # If we processed all tokens, then we're done.\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    # Create a future with the prompt\n    future = asyncio.get_running_loop().create_future()\n    self.add_query(token_ids[base:], future, past)\n    logits = await future\n\n    # Create new nodes\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token. Not asynchronous, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    # Walk while tokens can be found\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    logits = self.model(\n        torch.tensor([token_ids[base:]]).to(self.device),\n        past_key_values=node.past_key_values,\n        use_cache=node.past_key_values is not None,\n    ).logits[0]\n\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm/backend/llm/hf/#genlm.backend.llm.hf.AsyncTransformer.next_token_logprobs_uncached","title":"<code>next_token_logprobs_uncached(token_ids)</code>","text":"<p>Request log probabilities of next token. No KV or output caching, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/hf.py</code> <pre><code>def next_token_logprobs_uncached(self, token_ids):\n    \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    with torch.no_grad():\n        logits = self.model(\n            torch.tensor([token_ids]).to(self.device),\n            past_key_values=None,\n            use_cache=False,\n        ).logits[0]\n        return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/","title":"mlx","text":""},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.Query","title":"<code>Query</code>  <code>dataclass</code>","text":"<p>A query to a language model, waiting to be batched.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>list[int]</code> <p>Token IDs representing the input prompt.</p> <code>future</code> <code>Future</code> <p>Future object to store the result when the query is processed.</p> <code>past</code> <code>array</code> <p>Past key-value cache states from previous computations. Defaults to None.</p> <code>node</code> <code>DynamicTokenTrie</code> <p>The cache node where this query should be stored. Defaults to None.</p> <code>next_token_index</code> <code>int</code> <p>The index in the prompt where new tokens start (after cached prefix). Defaults to None.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>@dataclass\nclass Query:\n    \"\"\"A query to a language model, waiting to be batched.\n\n    Attributes:\n        prompt (list[int]): Token IDs representing the input prompt.\n        future (asyncio.Future): Future object to store the result when\n            the query is processed.\n        past (mx.array, optional): Past key-value cache states from\n            previous computations. Defaults to None.\n        node (DynamicTokenTrie, optional): The cache node where this query\n            should be stored. Defaults to None.\n        next_token_index (int, optional): The index in the prompt where\n            new tokens start (after cached prefix). Defaults to None.\n    \"\"\"\n\n    prompt: list[int]\n    future: asyncio.Future\n    past: mx.array | None = None\n    node: DynamicTokenTrie | None = None\n    next_token_index: int | None = None\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM","title":"<code>AsyncMlxLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Asynchronous MLX-based language model wrapper.</p> <p>This class provides an async interface to MLX language models with automatic batching, caching, and KV cache management. It extends AsyncLM to provide efficient batched inference with prefix caching.</p> <p>The model automatically batches concurrent requests and uses a trie-based cache to store computed log probabilities and KV states for reuse.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>class AsyncMlxLM(AsyncLM):\n    \"\"\"Asynchronous MLX-based language model wrapper.\n\n    This class provides an async interface to MLX language models with\n    automatic batching, caching, and KV cache management. It extends\n    AsyncLM to provide efficient batched inference with prefix caching.\n\n    The model automatically batches concurrent requests and uses a trie-based\n    cache to store computed log probabilities and KV states for reuse.\n    \"\"\"\n\n    def __init__(\n        self,\n        mlx_lm_model,\n        tokenizer,\n        batch_size=5,\n        timeout=0.001,\n        prefill_step_size=2048,\n        cache_size=400,\n    ):\n        \"\"\"Initialize an `AsyncMlxLM` instance.\n\n        Args:\n            mlx_lm_model: The MLX language model instance.\n            tokenizer: The tokenizer for encoding/decoding text.\n            batch_size (int, optional): Maximum number of queries to batch\n                together.\n            timeout (float, optional): Maximum time in seconds to wait\n                before processing a batch, even if batch_size is not met.\n            prefill_step_size (int, optional): Number of tokens to process\n                per step during prompt prefilling.\n            cache_size (int, optional): Maximum number of KV cache entries\n                to keep in memory.\n        \"\"\"\n        self.mlx_lm_model = mlx_lm_model\n        self.tokenizer = tokenizer\n        self.cache = DynamicTokenTrie()\n        self.generation_stream = mx.new_stream(mx.default_device())\n        self.queries = []\n        self.timeout = timeout\n        self.timer = None\n        self.prefill_step_size = prefill_step_size\n        self.cache_size = cache_size\n\n        self.batch_size = batch_size\n        self.kv_cachable = self._kv_cachable(self.mlx_lm_model)\n        if not self.kv_cachable:\n            warnings.warn(\n                f\"Model {type(self.mlx_lm_model).__name__} does not support KV caching; \"\n                f\"prefix caching will be disabled.\",\n                UserWarning,\n                stacklevel=2,\n            )\n        super().__init__(tokenizer=self.tokenizer)\n\n    @classmethod\n    def from_name(cls, model_name, **kwargs):\n        \"\"\"Create an `AsyncMlxLM` instance from a model name.\n\n        Args:\n            model_name (str): Name of the model to load. Can be a Hugging Face\n                model identifier or local path.\n            **kwargs: Additional arguments passed to `AsyncMlxLM` constructor,\n                such as `batch_size`, `timeout`, `prefill_step_size`, `cache_size`.\n\n        Returns:\n            AsyncMlxLM: An `AsyncMlxLM` instance with the loaded model and tokenizer.\n        \"\"\"\n\n        model, tokenizer = mlx_lm.load(model_name)\n        return cls(model, tokenizer, **kwargs)\n\n    @staticmethod\n    def _to_torch(logprobs):\n        \"\"\"Convert MLX arrays into PyTorch tensors.\"\"\"\n        if logprobs.dtype == mx.bfloat16:\n            logprobs = logprobs.astype(mx.float16)\n        return torch.tensor(logprobs)\n\n    @staticmethod\n    def _kv_cachable(mlx_lm_model):\n        \"\"\"Check if an MLX model supports KV cache storage.\n\n        A model is KV-cacheable if all its cache layers are KVCache or\n        RotatingKVCache with keep=0.\n        \"\"\"\n        if not hasattr(mlx_lm_model, \"make_cache\"):\n            return True\n        cache = mlx_lm_model.make_cache()\n        return all(\n            isinstance(c, KVCache)\n            or (isinstance(c, RotatingKVCache) and c.keep == 0)\n            for c in cache\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear the output cache and MLX device cache.\n\n        This method resets the internal token trie cache and clears\n        any cached arrays on the MLX device to free memory.\n        \"\"\"\n        if self.cache is not None:\n            self.cache = DynamicTokenTrie()\n        mx.clear_cache()\n\n    def walk_cache(self, token_ids):\n        \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n        Args:\n            token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n        Returns:\n            tuple: A 5-tuple containing:\n                - node: The deepest node in the cache tree that matches\n                    the token sequence, irregardless of whether its kv is cached or not\n                - next_token_index: Number of tokens matched from the start of token_ids\n                - past_kvs: Past key/value states concatenated from cached nodes, or None if no cached states were found\n                - kv_node: The cache node where KV states start\n                - kv_next_token_index: Number of tokens matched from the start of token_ids for the KV states\n        \"\"\"\n        # Walk while tokens can be found\n        node = self.cache\n        kv_next_token_index = 0\n        kv_node = node\n        collecting = True\n        next_token_index = 0\n        past_kvs = []\n\n        while next_token_index &lt; len(token_ids):\n            if node.past_key_values is not None and collecting:\n                past_kvs.append(node.past_key_values)\n                kv_node = node\n                kv_next_token_index = next_token_index\n            elif next_token_index &gt; 0:\n                collecting = False\n            if node.has_token(token_ids[next_token_index]):\n                node = node.get_token(token_ids[next_token_index])\n                next_token_index += 1\n            else:\n                break\n\n        past_kvs = None if len(past_kvs) == 0 else mx.concatenate(past_kvs, axis=3)\n\n        return node, next_token_index, past_kvs, kv_node, kv_next_token_index\n\n    def cache_kv(self, token_ids):\n        \"\"\"Pre-compute and cache KV states for a given token sequence.\"\"\"\n        query = Query(token_ids, None, None, self.cache, 0)\n        self._batch_logits_custom([query])\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n        to completion.\"\"\"\n        self.queries = []\n\n    def add_to_cache(self, queries, prompt_cache=None, logprobs=None):\n        \"\"\"Add computed log probabilities and KV states to the cache tree.\"\"\"\n        left_paddings = prompt_cache[0].left_padding.tolist()\n        for i, query in enumerate(queries):\n            token_ids, node, next_token_index = (\n                query.prompt,\n                query.node,\n                query.next_token_index,\n            )\n            if node is None or next_token_index is None:\n                node = self.cache\n                next_token_index = 0\n            lp = left_paddings[i]\n            if prompt_cache is not None and self.kv_cachable:\n                keys = [\n                    c.keys[i, :, lp + next_token_index : lp + len(token_ids), :]\n                    for c in prompt_cache\n                ]\n                values = [\n                    c.values[i, :, lp + next_token_index : lp + len(token_ids), :]\n                    for c in prompt_cache\n                ]\n                keys = mx.stack(keys, axis=0)\n                values = mx.stack(values, axis=0)\n                keys_values = mx.stack([keys, values], axis=0)\n                node.extend_cache(\n                    next_token_index, token_ids, logprobs[i], keys_values\n                )\n            else:\n                node.extend_cache(next_token_index, token_ids, logprobs[i])\n\n        self.cache.evict_lru_kv(self.cache_size)\n\n    def _process_kv(self, left_paddings, prompt_cache, pasts=None, step_size=256):\n        \"\"\"Process and integrate past KV cache states into prompt cache.\n\n        This method takes past key-value cache states from the cache tree\n        and integrates them into the prompt cache for efficient prefix\n        reuse. It handles padding and alignment of cache states across\n        different query lengths.\n\n        Args:\n            left_paddings (list[int]): Left padding amounts for each query\n                in the batch.\n            prompt_cache (list): List of cache objects to update with\n                past states.\n            pasts (list[mx.array], optional): List of past KV cache states,\n                one per query.\n            step_size (int, optional): Step size for cache size alignment.\n\n        Returns:\n            tuple: A 2-tuple containing:\n                - list: Updated prompt_cache objects\n                - cached_len: Number of tokens that were cached\n        \"\"\"\n        if pasts is None or all(past is None for past in pasts):\n            return prompt_cache, 0\n        max_match_lengths = [0 if past is None else past.shape[3] for past in pasts]\n        min_pos_cached = min(\n            ml + lp for ml, lp in zip(max_match_lengths, left_paddings)\n        )\n        cache_grabs = [max(min_pos_cached - lp, 0) for lp in left_paddings]\n        non_zero_index = next(\n            (i for i, grab in enumerate(cache_grabs) if grab), None\n        )\n        if non_zero_index is None:\n            return prompt_cache, 0\n        _, num_layers, N, _, D = pasts[non_zero_index].shape\n        cache_size = (step_size + min_pos_cached - 1) // step_size * step_size\n        right_paddings = [\n            max(cache_size - lp - max_len, 0)\n            for lp, max_len in zip(left_paddings, max_match_lengths)\n        ]\n        padded_pasts = []\n        for past, lp, rp in zip(pasts, left_paddings, right_paddings):\n            if past is None:\n                padded_pasts.append(mx.zeros((2, num_layers, N, cache_size, D)))\n            else:\n                padded_pasts.append(\n                    mx.pad(\n                        past[:, :, :, : cache_size - lp, :],\n                        ((0, 0), (0, 0), (0, 0), (lp, rp), (0, 0)),\n                    )\n                )\n\n        padded_pasts = mx.stack(padded_pasts, axis=2)\n        for i, cache in enumerate(prompt_cache):\n            cache.keys = padded_pasts[0, i]\n            cache.values = padded_pasts[1, i]\n            cache.offset += min_pos_cached\n            cache._idx += min_pos_cached\n        return prompt_cache, min_pos_cached\n\n    def _process_prompts(self, queries):\n        \"\"\"Process a batch of prompts and compute next-token log probabilities.\"\"\"\n        inputs = [q.prompt for q in queries]\n        pasts = [q.past for q in queries]\n        lengths = [len(p) for p in inputs]\n        max_length = max(lengths)\n        left_padding = [max_length - length for length in lengths]\n        prompt_cache = _make_cache(self.mlx_lm_model, left_padding)\n        inputs_padded = _left_pad_prompts(inputs, max_length=max_length)\n\n        if self.kv_cachable:\n            prompt_cache, cached_len = self._process_kv(\n                left_padding, prompt_cache, pasts\n            )\n        else:\n            cached_len = 0\n        inputs_padded = inputs_padded[:, cached_len:]\n\n        while inputs_padded.shape[1] &gt; 1:\n            n_to_process = min(self.prefill_step_size, inputs_padded.shape[1] - 1)\n            self.mlx_lm_model(inputs_padded[:, :n_to_process], cache=prompt_cache)\n            mx.eval([c.state for c in prompt_cache])\n            inputs_padded = inputs_padded[:, n_to_process:]\n\n        logits = self.mlx_lm_model(inputs_padded, cache=prompt_cache)\n        logits = logits[:, -1, :]\n        logprobs = logits - mx.logsumexp(logits, axis=-1, keepdims=True)\n        mx.async_eval(logprobs)\n\n        return logprobs, prompt_cache\n\n    def _batch_logits_custom(\n        self,\n        queries,\n    ):\n        \"\"\"Compute next-token log probabilities for each query in a batch and add to cache.\n        Args:\n            queries (list[Query]): List of query objects to process.\n        Returns:\n            logprobs (list[torch.Tensor]): List of normalized log probability tensors.\"\"\"\n        with wired_limit(self.mlx_lm_model, [self.generation_stream]):\n            logprobs, prompt_cache = self._process_prompts(queries)\n            logprobs = AsyncMlxLM._to_torch(logprobs)\n        mx.clear_cache()\n        self.add_to_cache(queries, prompt_cache, logprobs)\n        return logprobs\n\n    def batch_evaluate_queries(self):\n        \"\"\"Process a batch of queued language model queries.\"\"\"\n\n        queries, self.queries = self.queries, []\n        if len(queries) == 0:\n            return\n\n        query_groups = defaultdict(list)\n        for query in queries:\n            key = tuple(query.prompt)\n            query_groups[key].append(query)\n\n        # Use one representative query from each group\n        unique_queries = [group[0] for group in query_groups.values()]\n\n        results = self._batch_logits_custom(unique_queries)\n\n        assert len(results) == len(unique_queries)\n\n        for i, q in enumerate(unique_queries):\n            for dup_query in query_groups[tuple(q.prompt)]:\n                dup_query.future.set_result(results[i])\n\n    def add_query(self, query):\n        \"\"\"Add a query to be evaluated in the next batch and reset the timeout.\"\"\"\n        self.queries.append(query)\n\n        if self.timer:\n            self.timer.cancel()\n            self.timer = None\n        if len(self.queries) &gt;= self.batch_size:\n            self.batch_evaluate_queries()\n        else:\n            self.timer = asyncio.get_running_loop().call_later(\n                self.timeout, lambda: self.batch_evaluate_queries()\n            )\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, kv_node, kv_next_token_index = (\n            self.walk_cache(token_ids)\n        )\n        if next_token_index == len(token_ids) and node.logprobs is not None:\n            return node.logprobs\n\n        future = asyncio.get_running_loop().create_future()\n        query = Query(token_ids, future, past, kv_node, kv_next_token_index)\n        self.add_query(query)\n        logprobs = await future\n\n        return logprobs\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, kv_node, kv_next_token_index = (\n            self.walk_cache(token_ids)\n        )\n        if next_token_index == len(token_ids) and node.logprobs is not None:\n            return node.logprobs\n\n        query = Query(token_ids, None, past, kv_node, kv_next_token_index)\n        logprobs = self._batch_logits_custom([query])[0]\n\n        return logprobs\n\n    async def sample(\n        self,\n        prompt_token_ids,\n        max_tokens,\n        eos_token_ids,\n        temperature=1.0,\n        seed=None,\n    ):\n        \"\"\"Sample from the language model.\n\n        Args:\n            prompt_token_ids (list[int]): The token IDs of the prompt to\n                start generation from.\n            max_tokens (int): The maximum number of tokens to generate.\n            eos_token_ids (list[int]): The token IDs that signal\n                end-of-sequence. Generation stops when one of these is\n                sampled.\n            temperature (float, optional): The temperature to use for\n                sampling. Higher values make the distribution more uniform,\n                lower values make it more peaked. Defaults to 1.0.\n            seed (int, optional): The seed for the random number generator.\n                If provided, sets the random seed before sampling.\n                Defaults to None.\n\n        Returns:\n            (list[int]): The sampled token IDs.\n        \"\"\"\n\n        if seed is not None:\n            mx.random.seed(seed)\n\n        sampler = make_sampler(temp=temperature)\n        prompt_token_ids_array = mx.array(prompt_token_ids)\n        token_generator = generate_step(\n            prompt_token_ids_array,\n            self.mlx_lm_model,\n            max_tokens=max_tokens,\n            sampler=sampler,\n        )\n        generated_token_ids = []\n        for sampled, _ in token_generator:\n            if sampled in eos_token_ids:\n                break\n            generated_token_ids.append(sampled)\n        return generated_token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.__init__","title":"<code>__init__(mlx_lm_model, tokenizer, batch_size=5, timeout=0.001, prefill_step_size=2048, cache_size=400)</code>","text":"<p>Initialize an <code>AsyncMlxLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>mlx_lm_model</code> <p>The MLX language model instance.</p> required <code>tokenizer</code> <p>The tokenizer for encoding/decoding text.</p> required <code>batch_size</code> <code>int</code> <p>Maximum number of queries to batch together.</p> <code>5</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait before processing a batch, even if batch_size is not met.</p> <code>0.001</code> <code>prefill_step_size</code> <code>int</code> <p>Number of tokens to process per step during prompt prefilling.</p> <code>2048</code> <code>cache_size</code> <code>int</code> <p>Maximum number of KV cache entries to keep in memory.</p> <code>400</code> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def __init__(\n    self,\n    mlx_lm_model,\n    tokenizer,\n    batch_size=5,\n    timeout=0.001,\n    prefill_step_size=2048,\n    cache_size=400,\n):\n    \"\"\"Initialize an `AsyncMlxLM` instance.\n\n    Args:\n        mlx_lm_model: The MLX language model instance.\n        tokenizer: The tokenizer for encoding/decoding text.\n        batch_size (int, optional): Maximum number of queries to batch\n            together.\n        timeout (float, optional): Maximum time in seconds to wait\n            before processing a batch, even if batch_size is not met.\n        prefill_step_size (int, optional): Number of tokens to process\n            per step during prompt prefilling.\n        cache_size (int, optional): Maximum number of KV cache entries\n            to keep in memory.\n    \"\"\"\n    self.mlx_lm_model = mlx_lm_model\n    self.tokenizer = tokenizer\n    self.cache = DynamicTokenTrie()\n    self.generation_stream = mx.new_stream(mx.default_device())\n    self.queries = []\n    self.timeout = timeout\n    self.timer = None\n    self.prefill_step_size = prefill_step_size\n    self.cache_size = cache_size\n\n    self.batch_size = batch_size\n    self.kv_cachable = self._kv_cachable(self.mlx_lm_model)\n    if not self.kv_cachable:\n        warnings.warn(\n            f\"Model {type(self.mlx_lm_model).__name__} does not support KV caching; \"\n            f\"prefix caching will be disabled.\",\n            UserWarning,\n            stacklevel=2,\n        )\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.from_name","title":"<code>from_name(model_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an <code>AsyncMlxLM</code> instance from a model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load. Can be a Hugging Face model identifier or local path.</p> required <code>**kwargs</code> <p>Additional arguments passed to <code>AsyncMlxLM</code> constructor, such as <code>batch_size</code>, <code>timeout</code>, <code>prefill_step_size</code>, <code>cache_size</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AsyncMlxLM</code> <p>An <code>AsyncMlxLM</code> instance with the loaded model and tokenizer.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, **kwargs):\n    \"\"\"Create an `AsyncMlxLM` instance from a model name.\n\n    Args:\n        model_name (str): Name of the model to load. Can be a Hugging Face\n            model identifier or local path.\n        **kwargs: Additional arguments passed to `AsyncMlxLM` constructor,\n            such as `batch_size`, `timeout`, `prefill_step_size`, `cache_size`.\n\n    Returns:\n        AsyncMlxLM: An `AsyncMlxLM` instance with the loaded model and tokenizer.\n    \"\"\"\n\n    model, tokenizer = mlx_lm.load(model_name)\n    return cls(model, tokenizer, **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the output cache and MLX device cache.</p> <p>This method resets the internal token trie cache and clears any cached arrays on the MLX device to free memory.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the output cache and MLX device cache.\n\n    This method resets the internal token trie cache and clears\n    any cached arrays on the MLX device to free memory.\n    \"\"\"\n    if self.cache is not None:\n        self.cache = DynamicTokenTrie()\n    mx.clear_cache()\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.walk_cache","title":"<code>walk_cache(token_ids)</code>","text":"<p>Walk the cache tree to find the deepest node matching a sequence of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Sequence of token IDs to follow in the cache tree</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A 5-tuple containing: - node: The deepest node in the cache tree that matches     the token sequence, irregardless of whether its kv is cached or not - next_token_index: Number of tokens matched from the start of token_ids - past_kvs: Past key/value states concatenated from cached nodes, or None if no cached states were found - kv_node: The cache node where KV states start - kv_next_token_index: Number of tokens matched from the start of token_ids for the KV states</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def walk_cache(self, token_ids):\n    \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n    Args:\n        token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n    Returns:\n        tuple: A 5-tuple containing:\n            - node: The deepest node in the cache tree that matches\n                the token sequence, irregardless of whether its kv is cached or not\n            - next_token_index: Number of tokens matched from the start of token_ids\n            - past_kvs: Past key/value states concatenated from cached nodes, or None if no cached states were found\n            - kv_node: The cache node where KV states start\n            - kv_next_token_index: Number of tokens matched from the start of token_ids for the KV states\n    \"\"\"\n    # Walk while tokens can be found\n    node = self.cache\n    kv_next_token_index = 0\n    kv_node = node\n    collecting = True\n    next_token_index = 0\n    past_kvs = []\n\n    while next_token_index &lt; len(token_ids):\n        if node.past_key_values is not None and collecting:\n            past_kvs.append(node.past_key_values)\n            kv_node = node\n            kv_next_token_index = next_token_index\n        elif next_token_index &gt; 0:\n            collecting = False\n        if node.has_token(token_ids[next_token_index]):\n            node = node.get_token(token_ids[next_token_index])\n            next_token_index += 1\n        else:\n            break\n\n    past_kvs = None if len(past_kvs) == 0 else mx.concatenate(past_kvs, axis=3)\n\n    return node, next_token_index, past_kvs, kv_node, kv_next_token_index\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.cache_kv","title":"<code>cache_kv(token_ids)</code>","text":"<p>Pre-compute and cache KV states for a given token sequence.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def cache_kv(self, token_ids):\n    \"\"\"Pre-compute and cache KV states for a given token sequence.\"\"\"\n    query = Query(token_ids, None, None, self.cache, 0)\n    self._batch_logits_custom([query])\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing to completion.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n    to completion.\"\"\"\n    self.queries = []\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.add_to_cache","title":"<code>add_to_cache(queries, prompt_cache=None, logprobs=None)</code>","text":"<p>Add computed log probabilities and KV states to the cache tree.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def add_to_cache(self, queries, prompt_cache=None, logprobs=None):\n    \"\"\"Add computed log probabilities and KV states to the cache tree.\"\"\"\n    left_paddings = prompt_cache[0].left_padding.tolist()\n    for i, query in enumerate(queries):\n        token_ids, node, next_token_index = (\n            query.prompt,\n            query.node,\n            query.next_token_index,\n        )\n        if node is None or next_token_index is None:\n            node = self.cache\n            next_token_index = 0\n        lp = left_paddings[i]\n        if prompt_cache is not None and self.kv_cachable:\n            keys = [\n                c.keys[i, :, lp + next_token_index : lp + len(token_ids), :]\n                for c in prompt_cache\n            ]\n            values = [\n                c.values[i, :, lp + next_token_index : lp + len(token_ids), :]\n                for c in prompt_cache\n            ]\n            keys = mx.stack(keys, axis=0)\n            values = mx.stack(values, axis=0)\n            keys_values = mx.stack([keys, values], axis=0)\n            node.extend_cache(\n                next_token_index, token_ids, logprobs[i], keys_values\n            )\n        else:\n            node.extend_cache(next_token_index, token_ids, logprobs[i])\n\n    self.cache.evict_lru_kv(self.cache_size)\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.batch_evaluate_queries","title":"<code>batch_evaluate_queries()</code>","text":"<p>Process a batch of queued language model queries.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def batch_evaluate_queries(self):\n    \"\"\"Process a batch of queued language model queries.\"\"\"\n\n    queries, self.queries = self.queries, []\n    if len(queries) == 0:\n        return\n\n    query_groups = defaultdict(list)\n    for query in queries:\n        key = tuple(query.prompt)\n        query_groups[key].append(query)\n\n    # Use one representative query from each group\n    unique_queries = [group[0] for group in query_groups.values()]\n\n    results = self._batch_logits_custom(unique_queries)\n\n    assert len(results) == len(unique_queries)\n\n    for i, q in enumerate(unique_queries):\n        for dup_query in query_groups[tuple(q.prompt)]:\n            dup_query.future.set_result(results[i])\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.add_query","title":"<code>add_query(query)</code>","text":"<p>Add a query to be evaluated in the next batch and reset the timeout.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def add_query(self, query):\n    \"\"\"Add a query to be evaluated in the next batch and reset the timeout.\"\"\"\n    self.queries.append(query)\n\n    if self.timer:\n        self.timer.cancel()\n        self.timer = None\n    if len(self.queries) &gt;= self.batch_size:\n        self.batch_evaluate_queries()\n    else:\n        self.timer = asyncio.get_running_loop().call_later(\n            self.timeout, lambda: self.batch_evaluate_queries()\n        )\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, kv_node, kv_next_token_index = (\n        self.walk_cache(token_ids)\n    )\n    if next_token_index == len(token_ids) and node.logprobs is not None:\n        return node.logprobs\n\n    future = asyncio.get_running_loop().create_future()\n    query = Query(token_ids, future, past, kv_node, kv_next_token_index)\n    self.add_query(query)\n    logprobs = await future\n\n    return logprobs\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, kv_node, kv_next_token_index = (\n        self.walk_cache(token_ids)\n    )\n    if next_token_index == len(token_ids) and node.logprobs is not None:\n        return node.logprobs\n\n    query = Query(token_ids, None, past, kv_node, kv_next_token_index)\n    logprobs = self._batch_logits_custom([query])[0]\n\n    return logprobs\n</code></pre>"},{"location":"reference/genlm/backend/llm/mlx/#genlm.backend.llm.mlx.AsyncMlxLM.sample","title":"<code>sample(prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids</code> <code>list[int]</code> <p>The token IDs of the prompt to start generation from.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs that signal end-of-sequence. Generation stops when one of these is sampled.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use for sampling. Higher values make the distribution more uniform, lower values make it more peaked. Defaults to 1.0.</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator. If provided, sets the random seed before sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/mlx.py</code> <pre><code>async def sample(\n    self,\n    prompt_token_ids,\n    max_tokens,\n    eos_token_ids,\n    temperature=1.0,\n    seed=None,\n):\n    \"\"\"Sample from the language model.\n\n    Args:\n        prompt_token_ids (list[int]): The token IDs of the prompt to\n            start generation from.\n        max_tokens (int): The maximum number of tokens to generate.\n        eos_token_ids (list[int]): The token IDs that signal\n            end-of-sequence. Generation stops when one of these is\n            sampled.\n        temperature (float, optional): The temperature to use for\n            sampling. Higher values make the distribution more uniform,\n            lower values make it more peaked. Defaults to 1.0.\n        seed (int, optional): The seed for the random number generator.\n            If provided, sets the random seed before sampling.\n            Defaults to None.\n\n    Returns:\n        (list[int]): The sampled token IDs.\n    \"\"\"\n\n    if seed is not None:\n        mx.random.seed(seed)\n\n    sampler = make_sampler(temp=temperature)\n    prompt_token_ids_array = mx.array(prompt_token_ids)\n    token_generator = generate_step(\n        prompt_token_ids_array,\n        self.mlx_lm_model,\n        max_tokens=max_tokens,\n        sampler=sampler,\n    )\n    generated_token_ids = []\n    for sampled, _ in token_generator:\n        if sampled in eos_token_ids:\n            break\n        generated_token_ids.append(sampled)\n    return generated_token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/","title":"vllm","text":""},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.PassThroughLogitsProcessor","title":"<code>PassThroughLogitsProcessor</code>","text":"<p>A logits processor that stores the logprobs and passes the logits through.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>class PassThroughLogitsProcessor:\n    \"\"\"A logits processor that stores the logprobs and passes the logits through.\"\"\"\n\n    def __init__(self):\n        self.log_probs = None\n\n    def __call__(self, past_token_ids, logits):\n        assert self.log_probs is None, (\n            \"Log probs already set. This should never happen.\"\n        )\n        self.log_probs = torch.log_softmax(logits, dim=-1, dtype=logits.dtype)\n        return logits\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM","title":"<code>AsyncVirtualLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>class AsyncVirtualLM(AsyncLM):\n    default_params = {\n        \"max_tokens\": 1,\n        \"n\": 1,\n        \"detokenize\": False,\n        \"stop\": None,\n        \"ignore_eos\": True,\n    }\n\n    def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n        \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n        Args:\n            async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n            cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n            cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm.backend.cache.OutputCache] constructor. Defaults to {}.\n\n        Note:\n            The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n        \"\"\"\n        self.async_llm_engine = async_llm_engine\n        self.tokenizer = async_llm_engine.engine.get_tokenizer()\n        self.request_counter = Counter()\n        self.cache = (\n            OutputCache(maxsize=cache_size, **cache_opts)\n            if cache_size &gt; 0\n            else None\n        )\n\n        async_llm_engine.engine.log_stats = False\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    @classmethod\n    def from_name(cls, model_name, engine_opts=None, **kwargs):\n        \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n                configured with prefix caching enabled and async output processing disabled by default.\n            **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n        Returns:\n            (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n        \"\"\"\n        if not HAS_VLLM:\n            raise ImportError(  # pragma: no cover\n                \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n            )\n\n        if engine_opts is not None and \"enable_chunked_prefill\" in engine_opts:\n            if engine_opts[\"enable_chunked_prefill\"]:\n                warnings.warn(  # pragma: no cover\n                    \"Setting enable_chunked_prefill to True may interfere with AsyncVirtualLM's \"\n                    \"custom sampling functionality.\"\n                )\n\n        engine_opts = {\n            \"enable_prefix_caching\": True,\n            \"disable_log_requests\": True,\n            \"disable_async_output_proc\": True,  # This parameter forces vLLM to use v0, which is currently what we want to do.\n            **(engine_opts or {}),\n        }\n\n        engine = AsyncLLMEngine.from_engine_args(\n            AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n        )\n\n        return cls(engine, **kwargs)\n\n    @property\n    def underlying_model(self):\n        return self.async_llm_engine.engine.model_executor.driver_worker.model_runner.model\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            result (torch.Tensor): Normalized log probability tensor.\n\n        Warning:\n            Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n            For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n        \"\"\"\n        key = tuple(token_ids)\n\n        if self.cache is not None and key in self.cache:\n            return self.cache[key]\n\n        result = await self._next_token_logprobs(key)\n\n        if self.cache is not None:\n            self.cache[key] = result\n\n        return result\n\n    async def _next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        req_id = str(next(self.request_counter))\n        prompt = TokensPrompt(prompt_token_ids=token_ids)\n\n        outputs = []\n        processor = PassThroughLogitsProcessor()\n        async for output in self.async_llm_engine.generate(\n            prompt=prompt,\n            sampling_params=SamplingParams(\n                **self.default_params, logits_processors=[processor]\n            ),\n            request_id=req_id,\n        ):\n            if output.finished:\n                outputs.append(output)\n\n        assert processor.log_probs is not None, (\n            \"Log probs should be set by the logits processor.\"\n        )\n        return processor.log_probs\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self.batch_next_token_logprobs_sync([token_ids])[0]\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"\n        Request log probabilities of next tokens in a batch synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n        \"\"\"\n        req_ids = []\n        req_id2processors = {}\n        for token_ids in token_ids_list:\n            req_id = str(next(self.request_counter))\n            req_ids.append(req_id)\n            processor = PassThroughLogitsProcessor()\n            req_id2processors[req_id] = processor\n            self.async_llm_engine.engine.add_request(\n                prompt=TokensPrompt(prompt_token_ids=token_ids),\n                params=SamplingParams(\n                    **self.default_params, logits_processors=[processor]\n                ),\n                request_id=req_id,\n            )\n\n        while self.async_llm_engine.engine.has_unfinished_requests():\n            output = self.async_llm_engine.engine.step()\n            for out in output:\n                if out.finished:\n                    assert out.request_id in req_id2processors, (\n                        f\"{out.request_id} not in requested IDs\"\n                    )\n\n        return torch.stack(\n            [req_id2processors[req_id].log_probs for req_id in req_ids]\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear output cache.\"\"\"\n        if self.cache:\n            self.cache.clear()\n\n    def __del__(self):\n        \"\"\"Clean up resources on deletion.\"\"\"\n        self._cleanup_engine()\n\n    def _cleanup_engine(self):\n        \"\"\"Clean up the vLLM engine and associated resources.\"\"\"\n        if async_engine := getattr(self, \"async_llm_engine\", None):\n            async_engine.shutdown_background_loop()\n            destroy_model_parallel()\n            destroy_distributed_environment()\n\n    async def sample(\n        self,\n        prompt_token_ids,\n        max_tokens,\n        eos_token_ids,\n        temperature=1.0,\n        seed=None,\n    ):\n        \"\"\"Sample from the language model.\n\n        Args:\n            prompt_token_ids (list[int]): The token IDs of the prompt.\n            eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n            temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n            max_tokens (int): The maximum number of tokens to generate.\n            seed (int, optional): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            (list[int]): The sampled token IDs.\n        \"\"\"\n        async for output in self.async_llm_engine.generate(\n            prompt=TokensPrompt(prompt_token_ids=prompt_token_ids),\n            sampling_params=SamplingParams(\n                n=1,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                seed=seed,\n                stop=[self.byte_vocab[i].decode() for i in eos_token_ids],\n            ),\n            request_id=str(next(self.request_counter)),\n        ):\n            if output.finished:\n                assert len(output.outputs) == 1, (\n                    \"Expected exactly one sequence group\"\n                )\n                token_ids = list(output.outputs[0].token_ids)\n                if token_ids[-1] in eos_token_ids:\n                    token_ids = token_ids[:-1]\n                return token_ids\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM.__init__","title":"<code>__init__(async_llm_engine, cache_size=0, cache_opts={})</code>","text":"<p>Initialize an <code>AsyncVirtualLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>async_llm_engine</code> <code>AsyncLLMEngine</code> <p>The async vLLM engine instance.</p> required <code>cache_size</code> <code>int</code> <p>Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.</p> <code>0</code> <code>cache_opts</code> <code>dict</code> <p>Additional options to pass to the <code>OutputCache</code> constructor. Defaults to {}.</p> <code>{}</code> Note <p>The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n    \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n    Args:\n        async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n        cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n        cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm.backend.cache.OutputCache] constructor. Defaults to {}.\n\n    Note:\n        The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n    \"\"\"\n    self.async_llm_engine = async_llm_engine\n    self.tokenizer = async_llm_engine.engine.get_tokenizer()\n    self.request_counter = Counter()\n    self.cache = (\n        OutputCache(maxsize=cache_size, **cache_opts)\n        if cache_size &gt; 0\n        else None\n    )\n\n    async_llm_engine.engine.log_stats = False\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM.from_name","title":"<code>from_name(model_name, engine_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>AsyncVirtualLM</code> instance from a model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>engine_opts</code> <code>dict</code> <p>Additional options to pass to the <code>AsyncLLMEngine</code>. The engine will be configured with prefix caching enabled and async output processing disabled by default.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to <code>AsyncVirtualLM</code> constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncVirtualLM</code> <p>An <code>AsyncVirtualLM</code> instance.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, engine_opts=None, **kwargs):\n    \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n            configured with prefix caching enabled and async output processing disabled by default.\n        **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n    Returns:\n        (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n    \"\"\"\n    if not HAS_VLLM:\n        raise ImportError(  # pragma: no cover\n            \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n        )\n\n    if engine_opts is not None and \"enable_chunked_prefill\" in engine_opts:\n        if engine_opts[\"enable_chunked_prefill\"]:\n            warnings.warn(  # pragma: no cover\n                \"Setting enable_chunked_prefill to True may interfere with AsyncVirtualLM's \"\n                \"custom sampling functionality.\"\n            )\n\n    engine_opts = {\n        \"enable_prefix_caching\": True,\n        \"disable_log_requests\": True,\n        \"disable_async_output_proc\": True,  # This parameter forces vLLM to use v0, which is currently what we want to do.\n        **(engine_opts or {}),\n    }\n\n    engine = AsyncLLMEngine.from_engine_args(\n        AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n    )\n\n    return cls(engine, **kwargs)\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token asynchronously with output caching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>Tensor</code> <p>Normalized log probability tensor.</p> Warning <p>Do not use <code>asyncio.run(next_token_logprobs())</code> as it may interfere with vLLM's background loop. For synchronous usage, use the <code>next_token_logprobs_sync()</code> method instead.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        result (torch.Tensor): Normalized log probability tensor.\n\n    Warning:\n        Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n        For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n    \"\"\"\n    key = tuple(token_ids)\n\n    if self.cache is not None and key in self.cache:\n        return self.cache[key]\n\n    result = await self._next_token_logprobs(key)\n\n    if self.cache is not None:\n        self.cache[key] = result\n\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self.batch_next_token_logprobs_sync([token_ids])[0]\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Request log probabilities of next tokens in a batch synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists, each representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of normalized log probability tensors, one for each prompt in the input list.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"\n    Request log probabilities of next tokens in a batch synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n    \"\"\"\n    req_ids = []\n    req_id2processors = {}\n    for token_ids in token_ids_list:\n        req_id = str(next(self.request_counter))\n        req_ids.append(req_id)\n        processor = PassThroughLogitsProcessor()\n        req_id2processors[req_id] = processor\n        self.async_llm_engine.engine.add_request(\n            prompt=TokensPrompt(prompt_token_ids=token_ids),\n            params=SamplingParams(\n                **self.default_params, logits_processors=[processor]\n            ),\n            request_id=req_id,\n        )\n\n    while self.async_llm_engine.engine.has_unfinished_requests():\n        output = self.async_llm_engine.engine.step()\n        for out in output:\n            if out.finished:\n                assert out.request_id in req_id2processors, (\n                    f\"{out.request_id} not in requested IDs\"\n                )\n\n    return torch.stack(\n        [req_id2processors[req_id].log_probs for req_id in req_ids]\n    )\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear output cache.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear output cache.\"\"\"\n    if self.cache:\n        self.cache.clear()\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM.__del__","title":"<code>__del__()</code>","text":"<p>Clean up resources on deletion.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up resources on deletion.\"\"\"\n    self._cleanup_engine()\n</code></pre>"},{"location":"reference/genlm/backend/llm/vllm/#genlm.backend.llm.vllm.AsyncVirtualLM.sample","title":"<code>sample(prompt_token_ids, max_tokens, eos_token_ids, temperature=1.0, seed=None)</code>  <code>async</code>","text":"<p>Sample from the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_token_ids</code> <code>list[int]</code> <p>The token IDs of the prompt.</p> required <code>eos_token_ids</code> <code>list[int]</code> <p>The token IDs of the end-of-sequence tokens.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use to rescale the logits. Defaults to 1.0.</p> <code>1.0</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>The sampled token IDs.</p> Source code in <code>genlm/backend/llm/vllm.py</code> <pre><code>async def sample(\n    self,\n    prompt_token_ids,\n    max_tokens,\n    eos_token_ids,\n    temperature=1.0,\n    seed=None,\n):\n    \"\"\"Sample from the language model.\n\n    Args:\n        prompt_token_ids (list[int]): The token IDs of the prompt.\n        eos_token_ids (list[int]): The token IDs of the end-of-sequence tokens.\n        temperature (float, optional): The temperature to use to rescale the logits. Defaults to 1.0.\n        max_tokens (int): The maximum number of tokens to generate.\n        seed (int, optional): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        (list[int]): The sampled token IDs.\n    \"\"\"\n    async for output in self.async_llm_engine.generate(\n        prompt=TokensPrompt(prompt_token_ids=prompt_token_ids),\n        sampling_params=SamplingParams(\n            n=1,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            seed=seed,\n            stop=[self.byte_vocab[i].decode() for i in eos_token_ids],\n        ),\n        request_id=str(next(self.request_counter)),\n    ):\n        if output.finished:\n            assert len(output.outputs) == 1, (\n                \"Expected exactly one sequence group\"\n            )\n            token_ids = list(output.outputs[0].token_ids)\n            if token_ids[-1] in eos_token_ids:\n                token_ids = token_ids[:-1]\n            return token_ids\n</code></pre>"},{"location":"reference/genlm/backend/tokenization/__init__/","title":"tokenization","text":""},{"location":"reference/genlm/backend/tokenization/__init__/#genlm.backend.tokenization.decode_vocab","title":"<code>decode_vocab(tokenizer, byte2str_fallback='tokenizer')</code>","text":"<p>Convert tokenizer vocabulary into byte and string representations.</p> Warning <p>The byte representation is the canonical form. The string representation is provided for convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte2str_fallback</code> <code>str</code> <p>Strategy for converting invalid UTF-8 bytes to strings. Options:</p> <ul> <li>'tokenizer': Use tokenizer's <code>convert_ids_to_tokens</code> (default)</li> <li>'latin1': Decode using latin1 encoding</li> <li>'replace': Use Unicode replacement character '\ufffd'</li> </ul> <code>'tokenizer'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(byte_vocab, str_vocab)</p> Source code in <code>genlm/backend/tokenization/vocab.py</code> <pre><code>def decode_vocab(tokenizer, byte2str_fallback=\"tokenizer\"):\n    \"\"\"Convert tokenizer vocabulary into byte and string representations.\n\n    Warning:\n        The byte representation is the canonical form. The string representation is provided for\n        convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte2str_fallback (str): Strategy for converting invalid UTF-8 bytes to strings. Options:\\n\n            - 'tokenizer': Use tokenizer's `convert_ids_to_tokens` (default)\n            - 'latin1': Decode using latin1 encoding\n            - 'replace': Use Unicode replacement character '\ufffd'\n\n    Returns:\n        (tuple): (byte_vocab, str_vocab)\n    \"\"\"\n    if byte2str_fallback not in [\"latin1\", \"tokenizer\", \"replace\"]:\n        raise ValueError(f\"Unknown byte2str_fallback strategy: {byte2str_fallback}\")\n\n    if tokenizer.is_fast:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer.name_or_path, use_fast=False\n        )\n\n    # Try slow tokenizer.\n    try:\n        byte_vocab = get_byte_vocab(tokenizer)\n    except ByteVocabError:\n        # warnings.warn(\"Could not decode vocabulary from slow tokenizer. Trying using fast tokenizer.\")\n\n        # Try fast tokenizer.\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer.name_or_path, use_fast=True)\n        try:\n            byte_vocab = get_byte_vocab(tokenizer)\n        except ByteVocabError as e:\n            raise ValueError(\n                f\"Could not decode byte representation of token vocabuary from tokenizer {tokenizer.name_or_path}\"\n            ) from e\n\n    str_vocab = bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback)\n\n    return byte_vocab, str_vocab\n</code></pre>"},{"location":"reference/genlm/backend/tokenization/bytes/","title":"bytes","text":"<p>Functions to get the byte vocabulary from a HuggingFace tokenizer</p>"},{"location":"reference/genlm/backend/tokenization/bytes/#genlm.backend.tokenization.bytes.get_byte_vocab","title":"<code>get_byte_vocab(tokenizer)</code>","text":"<p>Extract byte vocabulary from a tokenizer using various methods.</p> <p>This function attempts to extract the byte representation of each token in the vocabulary using multiple methods, trying each in sequence until one succeeds:</p> <ol> <li>If the tokenizer has a byte_decoder attribute, attempt to use that directly</li> <li>If the tokenizer has an sp_model (SentencePiece) attribute, use that</li> <li>Try encoding the token strings directly</li> <li>Fall back to using the default GPT2 byte decoder</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance.</p> required <p>Returns:</p> Type Description <code>list[byte]</code> <p>List of byte representations of tokens.</p> <p>Raises:</p> Type Description <code>ByteVocabError</code> <p>If vocabulary cannot be decoded using any of the available methods.</p> Source code in <code>genlm/backend/tokenization/bytes.py</code> <pre><code>def get_byte_vocab(tokenizer):\n    \"\"\"Extract byte vocabulary from a tokenizer using various methods.\n\n    This function attempts to extract the byte representation of each token in the vocabulary\n    using multiple methods, trying each in sequence until one succeeds:\n\n    1. If the tokenizer has a byte_decoder attribute, attempt to use that directly\n    2. If the tokenizer has an sp_model (SentencePiece) attribute, use that\n    3. Try encoding the token strings directly\n    4. Fall back to using the default GPT2 byte decoder\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance.\n\n    Returns:\n        (list[byte]): List of byte representations of tokens.\n\n    Raises:\n        ByteVocabError: If vocabulary cannot be decoded using any of the available methods.\n    \"\"\"\n    # Try byte decoder.\n    if hasattr(tokenizer, \"byte_decoder\"):\n        try:\n            byte_decoder = tokenizer.byte_decoder\n            check_byte_decoder(tokenizer, byte_decoder)\n            return get_byte_tokens_from_byte_decoder(tokenizer, byte_decoder)\n        except ByteDecoderError:\n            pass\n            # warnings.warn(f\"Could not decode vocabulary using byte_decoder: {e!r}\")\n\n    # Try SentencePiece model.\n    if hasattr(tokenizer, \"sp_model\"):\n        return get_byte_tokens_from_sp(tokenizer)\n\n    # Try using GPT2 byte decoder.\n    try:\n        byte_decoder = _get_default_byte_decoder()\n        check_byte_decoder(tokenizer, byte_decoder)\n        return get_byte_tokens_from_byte_decoder(tokenizer, byte_decoder)\n    except ByteDecoderError as e:\n        raise ByteVocabError(\n            \"Could not decode vocabulary by falling back to GPT2 byte decoder.\"\n        ) from e\n</code></pre>"},{"location":"reference/genlm/backend/tokenization/bytes/#genlm.backend.tokenization.bytes.get_byte_tokens_from_byte_decoder","title":"<code>get_byte_tokens_from_byte_decoder(tokenizer, byte_decoder)</code>","text":"<p>Convert tokens to bytes using a byte decoder mapping.</p> <p>Special tokens are handled by directly encoding their string representation.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte_decoder</code> <code>dict</code> <p>Dictionary mapping characters to bytes</p> required <p>Returns:</p> Name Type Description <code>byte_tokens</code> <code>list[byte]</code> <p>List of byte representations for each token</p> Source code in <code>genlm/backend/tokenization/bytes.py</code> <pre><code>def get_byte_tokens_from_byte_decoder(tokenizer, byte_decoder):\n    \"\"\"Convert tokens to bytes using a byte decoder mapping.\n\n    Special tokens are handled by directly encoding their string representation.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte_decoder (dict): Dictionary mapping characters to bytes\n\n    Returns:\n        byte_tokens (list[byte]): List of byte representations for each token\n    \"\"\"\n    special_tokens_map = {v: k for k, v in tokenizer.get_added_vocab().items()}\n    byte_tokens = [\n        bytes([byte_decoder[b] for b in tokenizer.convert_ids_to_tokens(i)])\n        if i not in special_tokens_map\n        else special_tokens_map[i].encode()\n        for i in range(len(tokenizer))\n    ]\n    return byte_tokens\n</code></pre>"},{"location":"reference/genlm/backend/tokenization/bytes/#genlm.backend.tokenization.bytes.get_byte_tokens_from_sp","title":"<code>get_byte_tokens_from_sp(tokenizer)</code>","text":"<p>Convert tokens to their byte representations using a SentencePiece model.</p> <p>Uses the SentencePiece model's id_to_piece method to get the raw byte representation of each token, handling special tokens separately. Converts any hex-encoded bytes (in &lt;0xXX&gt; format) to their actual byte values and replaces the SentencePiece prefix space marker with a regular space.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance with a SentencePiece model</p> required <p>Returns:</p> Name Type Description <code>byte_tokens</code> <code>list[byte]</code> <p>List of byte representations for each token in the vocabulary</p> Note <p>Special tokens are handled by directly encoding their string representation, while normal tokens go through the SentencePiece conversion process.</p> Source code in <code>genlm/backend/tokenization/bytes.py</code> <pre><code>def get_byte_tokens_from_sp(tokenizer):\n    \"\"\"Convert tokens to their byte representations using a SentencePiece model.\n\n    Uses the SentencePiece model's id_to_piece method to get the raw byte representation\n    of each token, handling special tokens separately. Converts any hex-encoded bytes\n    (in &lt;0xXX&gt; format) to their actual byte values and replaces the SentencePiece\n    prefix space marker with a regular space.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance with a SentencePiece model\n\n    Returns:\n        byte_tokens (list[byte]): List of byte representations for each token in the vocabulary\n\n    Note:\n        Special tokens are handled by directly encoding their string representation,\n        while normal tokens go through the SentencePiece conversion process.\n    \"\"\"\n    special_tokens_map = {\n        token_id: token for token, token_id in tokenizer.get_added_vocab().items()\n    }\n    byte_tokens = [b\"\"] * len(tokenizer)\n    prefix_space = \"\u2581\".encode()\n    for i in range(len(tokenizer)):\n        if i in special_tokens_map:\n            byte_coded = special_tokens_map[i].encode()\n        else:\n            byte_coded = re.sub(\n                rb\"&lt;0x(..)&gt;\",\n                lambda x: bytes.fromhex(x[1].decode()),\n                tokenizer.sp_model.id_to_piece(i).encode(),\n            )\n        byte_tokens[i] = byte_coded.replace(prefix_space, b\" \")\n    return byte_tokens\n</code></pre>"},{"location":"reference/genlm/backend/tokenization/bytes/#genlm.backend.tokenization.bytes.check_byte_decoder","title":"<code>check_byte_decoder(tokenizer, byte_decoder)</code>","text":"<p>Verify that a byte decoder can properly handle all tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte_decoder</code> <code>dict</code> <p>Dictionary mapping characters to bytes</p> required <p>Raises:</p> Type Description <code>ByteDecoderError</code> <p>If byte decoder fails validation checks</p> Source code in <code>genlm/backend/tokenization/bytes.py</code> <pre><code>def check_byte_decoder(tokenizer, byte_decoder):\n    \"\"\"Verify that a byte decoder can properly handle all tokens.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte_decoder (dict): Dictionary mapping characters to bytes\n\n    Raises:\n        ByteDecoderError: If byte decoder fails validation checks\n    \"\"\"\n    _check_byte_decoder_has_all_bytes(tokenizer, byte_decoder)\n    _check_complex_roundtrip(tokenizer, byte_decoder)\n</code></pre>"},{"location":"reference/genlm/backend/tokenization/vocab/","title":"vocab","text":"<p>Functions to get and check HuggingFace tokenizer vocabularies</p>"},{"location":"reference/genlm/backend/tokenization/vocab/#genlm.backend.tokenization.vocab.decode_vocab","title":"<code>decode_vocab(tokenizer, byte2str_fallback='tokenizer')</code>","text":"<p>Convert tokenizer vocabulary into byte and string representations.</p> Warning <p>The byte representation is the canonical form. The string representation is provided for convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte2str_fallback</code> <code>str</code> <p>Strategy for converting invalid UTF-8 bytes to strings. Options:</p> <ul> <li>'tokenizer': Use tokenizer's <code>convert_ids_to_tokens</code> (default)</li> <li>'latin1': Decode using latin1 encoding</li> <li>'replace': Use Unicode replacement character '\ufffd'</li> </ul> <code>'tokenizer'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(byte_vocab, str_vocab)</p> Source code in <code>genlm/backend/tokenization/vocab.py</code> <pre><code>def decode_vocab(tokenizer, byte2str_fallback=\"tokenizer\"):\n    \"\"\"Convert tokenizer vocabulary into byte and string representations.\n\n    Warning:\n        The byte representation is the canonical form. The string representation is provided for\n        convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte2str_fallback (str): Strategy for converting invalid UTF-8 bytes to strings. Options:\\n\n            - 'tokenizer': Use tokenizer's `convert_ids_to_tokens` (default)\n            - 'latin1': Decode using latin1 encoding\n            - 'replace': Use Unicode replacement character '\ufffd'\n\n    Returns:\n        (tuple): (byte_vocab, str_vocab)\n    \"\"\"\n    if byte2str_fallback not in [\"latin1\", \"tokenizer\", \"replace\"]:\n        raise ValueError(f\"Unknown byte2str_fallback strategy: {byte2str_fallback}\")\n\n    if tokenizer.is_fast:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer.name_or_path, use_fast=False\n        )\n\n    # Try slow tokenizer.\n    try:\n        byte_vocab = get_byte_vocab(tokenizer)\n    except ByteVocabError:\n        # warnings.warn(\"Could not decode vocabulary from slow tokenizer. Trying using fast tokenizer.\")\n\n        # Try fast tokenizer.\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer.name_or_path, use_fast=True)\n        try:\n            byte_vocab = get_byte_vocab(tokenizer)\n        except ByteVocabError as e:\n            raise ValueError(\n                f\"Could not decode byte representation of token vocabuary from tokenizer {tokenizer.name_or_path}\"\n            ) from e\n\n    str_vocab = bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback)\n\n    return byte_vocab, str_vocab\n</code></pre>"},{"location":"reference/genlm/backend/tokenization/vocab/#genlm.backend.tokenization.vocab.bytes_to_strs","title":"<code>bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback)</code>","text":"<p>Convert byte representations to UTF-8 strings.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte_vocab</code> <code>list[bytes]</code> <p>List of byte representations of tokens</p> required <code>byte2str_fallback</code> <code>str</code> <p>Strategy for converting invalid UTF-8 bytes to strings: - 'tokenizer': Use tokenizer's convert_ids_to_tokens (default) - 'latin1': Decode using latin1 encoding - 'replace': Use Unicode replacement character '\ufffd'</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of string representations of tokens</p> Note <p>May produce duplicate strings for different token IDs. A warning is issued if duplicates are found.</p> Source code in <code>genlm/backend/tokenization/vocab.py</code> <pre><code>def bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback):\n    \"\"\"Convert byte representations to UTF-8 strings.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte_vocab (list[bytes]): List of byte representations of tokens\n        byte2str_fallback (str): Strategy for converting invalid UTF-8 bytes to strings:\n            - 'tokenizer': Use tokenizer's convert_ids_to_tokens (default)\n            - 'latin1': Decode using latin1 encoding\n            - 'replace': Use Unicode replacement character '\ufffd'\n\n    Returns:\n        (list[str]): List of string representations of tokens\n\n    Note:\n        May produce duplicate strings for different token IDs. A warning is issued if duplicates are found.\n    \"\"\"\n    str_vocab = []\n    seen_tokens = {}\n    for token_id, raw_token in enumerate(byte_vocab):\n        try:\n            token = raw_token.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            if byte2str_fallback == \"latin1\":\n                try:\n                    token = raw_token.decode(\"latin1\")\n                except UnicodeDecodeError:\n                    token = tokenizer.convert_ids_to_tokens(token_id)\n            elif byte2str_fallback == \"tokenizer\":\n                token = tokenizer.convert_ids_to_tokens(token_id)\n            elif byte2str_fallback == \"replace\":\n                token = raw_token.decode(\"utf-8\", errors=\"replace\")\n\n        if token in seen_tokens:\n            seen_tokens[token].append(token_id)\n        else:\n            seen_tokens[token] = [token_id]\n\n        str_vocab.append(token)\n\n    return str_vocab\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/","title":"trie","text":""},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.TokenCharacterTrie","title":"<code>TokenCharacterTrie</code>","text":"<p>A trie data structure for efficient token-to-character mapping.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>class TokenCharacterTrie:\n    \"\"\"A trie data structure for efficient token-to-character mapping.\"\"\"\n\n    def __init__(self, decode):\n        \"\"\"Initialize a `TokenCharacterTrie`.\n\n        Args:\n            decode (list): List representing the token vocabulary.\n                Each element of the list must be iterable.\n        \"\"\"\n        self.decode = decode\n        self.word2leaf = {}\n        self.children = [{}]  # First node is root\n        self.root = 0\n        self.token_id_to_leaf = []\n\n        for token_id, word in enumerate(self.decode):\n            curr = self.root\n            for letter in word:\n                if letter not in self.children[curr]:\n                    self.children[curr][letter] = len(self.children)\n                    self.children.append({})\n                curr = self.children[curr][letter]\n\n            self.children[curr][None] = last = len(self.children)\n            self.children.append({})\n            assert word not in self.word2leaf, (\n                \"Can't have duplicate words in vocabulary\"\n            )\n            self.word2leaf[word] = last\n\n            self.token_id_to_leaf.append((token_id, last))\n\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n        )\n        self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n        # Renumber the states of the trie so that they are named by a contiguous\n        # range of integers and those integers respect the are topologically\n        # ordering of the trie topology.  This improves the efficiency of the\n        # updating the trie as it improves memory locality.\n        ordering = {}\n        for i, x in enumerate(self._order_full(self.root)):\n            ordering[x] = i\n        self._rename(f=lambda x: ordering[x])\n\n        node2prefix = {self.root: []}\n        for x in reversed(range(len(self.children))):\n            for letter, y in self.children[x].items():\n                if letter is None:\n                    node2prefix[y] = node2prefix[x]\n                else:\n                    node2prefix[y] = node2prefix[x] + [letter]\n        self.node2prefix = node2prefix\n\n    def _rename(self, f):\n        \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n        Args:\n            f (callable): Function that maps old node indices to new node indices\n        \"\"\"\n        N = len(self.children)\n\n        new_children = [{} for _ in range(N)]\n        nodes = range(N)\n\n        for x in nodes:\n            for letter, y in self.children[x].items():\n                new_children[f(x)][letter] = f(y)\n\n        self.root = f(self.root)\n        self.children = new_children\n        self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n        self.token_id_to_leaf = np.array(\n            [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n        )\n\n        self.ordering = np.array([f(x) for x in self.ordering])\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n        )\n\n    def _alloc_weights(self):\n        \"\"\"Allocate an array to store weight values for all nodes.\n\n        Returns:\n            np.ndarray: Zero-initialized array for storing weight values\n        \"\"\"\n        return np.zeros(len(self.children), dtype=np.float64)\n\n    def _preprocess_ws(self, ws):\n        \"\"\"Preprocess the weight vector to ensure it is a numpy array and on the correct device.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight vector\n        \"\"\"\n        if isinstance(ws, torch.Tensor):\n            if ws.device.type != \"cpu\":\n                ws = ws.cpu()\n            ws = ws.numpy()\n        return ws\n\n    def weight_sum(self, ws):\n        \"\"\"Compute weight sum for each node in the trie.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Summed weights for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_sum(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def weight_max(self, ws):\n        \"\"\"Compute weight max for each node in the trie.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight max values for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_max(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batched equivalent of `weight_sum`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_sum(ws) for ws in ws])\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batched equivalent of `weight_max`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_max(ws) for ws in ws])\n\n    def _order(self, node):\n        \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            int: Node indices in topological order\n        \"\"\"\n        for a in self.children[node]:\n            if a is None:\n                pass\n            else:\n                yield from self._order(self.children[node][a])\n        yield node\n\n    def _order_full(self, node):\n        \"\"\"Generate a complete topological ordering including all child nodes.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            (int): Node indices in complete topological order\n        \"\"\"\n        for a in self.children[node]:\n            yield from self._order_full(self.children[node][a])\n        yield node\n\n    def visualize(self, ws=None):\n        \"\"\"Visualize the trie structure using Graphviz.\n\n        Args:\n            ws (np.ndarray|None): Optional weight vector to display at each node.\n                                Should be of length `len(self.children)`.\n\n        Returns:\n            (graphviz.Digraph): The generated graph object\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:  # pragma: no cover\n            raise ImportError(\n                \"Please install graphviz: pip install graphviz\"\n            )  # pragma: no cover\n\n        if ws is not None and len(ws) != len(self.children):\n            raise ValueError(\n                f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n            )\n\n        dot = graphviz.Digraph(comment=\"Token Character Trie\")\n        dot.attr(rankdir=\"LR\")\n\n        # Create a subgraph for the legend\n        with dot.subgraph(name=\"cluster_legend\") as legend:\n            legend.attr(label=\"Legend\", fontsize=\"10\")\n            legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n            # Example internal node\n            legend.node(\n                \"legend_internal\",\n                \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n                shape=\"circle\",\n            )\n\n            # Example leaf node\n            legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n            legend.edge(\n                \"legend_internal\",\n                \"legend_leaf\",\n                label=\"Token item\",\n                fontsize=\"10\",\n            )\n\n            # Align legend horizontally\n            legend.attr(rankdir=\"TB\")\n            legend.attr(rank=\"same\")\n\n        # Add the main trie nodes and edges\n        for node_id in range(len(self.children)):\n            prefix = self.node2prefix[node_id]\n\n            if ws is not None:\n                label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n            else:\n                label = f\"{node_id}\\n'{prefix}'\"\n\n            # Color nodes based on mass if provided\n            if ws is not None:\n                max_ws = ws.max()\n                if max_ws &gt; 0:\n                    intensity = int(255 * (1 - ws[node_id] / max_ws))\n                    color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n                else:\n                    color = \"#ffffff\"  # white for zero mass\n            else:\n                color = \"#ffffff\"  # default white\n\n            if node_id in self.leaf2word:\n                dot.node(\n                    str(node_id),\n                    label,\n                    shape=\"doublecircle\",\n                    style=\"filled\",\n                    fillcolor=color,\n                )\n            else:\n                dot.node(\n                    str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n                )\n\n        for node_id, children in enumerate(self.children):\n            for char, child_id in children.items():\n                if char is not None:\n                    edge_label = str(char)\n                else:\n                    edge_label = \"End-of-Token\"\n\n                dot.edge(str(node_id), str(child_id), label=edge_label)\n\n        return dot\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.TokenCharacterTrie.__init__","title":"<code>__init__(decode)</code>","text":"<p>Initialize a <code>TokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>decode</code> <code>list</code> <p>List representing the token vocabulary. Each element of the list must be iterable.</p> required Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def __init__(self, decode):\n    \"\"\"Initialize a `TokenCharacterTrie`.\n\n    Args:\n        decode (list): List representing the token vocabulary.\n            Each element of the list must be iterable.\n    \"\"\"\n    self.decode = decode\n    self.word2leaf = {}\n    self.children = [{}]  # First node is root\n    self.root = 0\n    self.token_id_to_leaf = []\n\n    for token_id, word in enumerate(self.decode):\n        curr = self.root\n        for letter in word:\n            if letter not in self.children[curr]:\n                self.children[curr][letter] = len(self.children)\n                self.children.append({})\n            curr = self.children[curr][letter]\n\n        self.children[curr][None] = last = len(self.children)\n        self.children.append({})\n        assert word not in self.word2leaf, (\n            \"Can't have duplicate words in vocabulary\"\n        )\n        self.word2leaf[word] = last\n\n        self.token_id_to_leaf.append((token_id, last))\n\n    self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n    self.jump = List(\n        [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n    )\n    self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n    # Renumber the states of the trie so that they are named by a contiguous\n    # range of integers and those integers respect the are topologically\n    # ordering of the trie topology.  This improves the efficiency of the\n    # updating the trie as it improves memory locality.\n    ordering = {}\n    for i, x in enumerate(self._order_full(self.root)):\n        ordering[x] = i\n    self._rename(f=lambda x: ordering[x])\n\n    node2prefix = {self.root: []}\n    for x in reversed(range(len(self.children))):\n        for letter, y in self.children[x].items():\n            if letter is None:\n                node2prefix[y] = node2prefix[x]\n            else:\n                node2prefix[y] = node2prefix[x] + [letter]\n    self.node2prefix = node2prefix\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.TokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Compute weight sum for each node in the trie.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Compute weight sum for each node in the trie.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Summed weights for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_sum(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.TokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Compute weight max for each node in the trie.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Weight max values for each node in the trie.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Compute weight max for each node in the trie.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Weight max values for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_max(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.TokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batched equivalent of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batched equivalent of `weight_sum`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_sum(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.TokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batched equivalent of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight max values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batched equivalent of `weight_max`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_max(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.TokenCharacterTrie.visualize","title":"<code>visualize(ws=None)</code>","text":"<p>Visualize the trie structure using Graphviz.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>ndarray | None</code> <p>Optional weight vector to display at each node.                 Should be of length <code>len(self.children)</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Digraph</code> <p>The generated graph object</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def visualize(self, ws=None):\n    \"\"\"Visualize the trie structure using Graphviz.\n\n    Args:\n        ws (np.ndarray|None): Optional weight vector to display at each node.\n                            Should be of length `len(self.children)`.\n\n    Returns:\n        (graphviz.Digraph): The generated graph object\n    \"\"\"\n    try:\n        import graphviz\n    except ImportError:  # pragma: no cover\n        raise ImportError(\n            \"Please install graphviz: pip install graphviz\"\n        )  # pragma: no cover\n\n    if ws is not None and len(ws) != len(self.children):\n        raise ValueError(\n            f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n        )\n\n    dot = graphviz.Digraph(comment=\"Token Character Trie\")\n    dot.attr(rankdir=\"LR\")\n\n    # Create a subgraph for the legend\n    with dot.subgraph(name=\"cluster_legend\") as legend:\n        legend.attr(label=\"Legend\", fontsize=\"10\")\n        legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n        # Example internal node\n        legend.node(\n            \"legend_internal\",\n            \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n            shape=\"circle\",\n        )\n\n        # Example leaf node\n        legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n        legend.edge(\n            \"legend_internal\",\n            \"legend_leaf\",\n            label=\"Token item\",\n            fontsize=\"10\",\n        )\n\n        # Align legend horizontally\n        legend.attr(rankdir=\"TB\")\n        legend.attr(rank=\"same\")\n\n    # Add the main trie nodes and edges\n    for node_id in range(len(self.children)):\n        prefix = self.node2prefix[node_id]\n\n        if ws is not None:\n            label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n        else:\n            label = f\"{node_id}\\n'{prefix}'\"\n\n        # Color nodes based on mass if provided\n        if ws is not None:\n            max_ws = ws.max()\n            if max_ws &gt; 0:\n                intensity = int(255 * (1 - ws[node_id] / max_ws))\n                color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n            else:\n                color = \"#ffffff\"  # white for zero mass\n        else:\n            color = \"#ffffff\"  # default white\n\n        if node_id in self.leaf2word:\n            dot.node(\n                str(node_id),\n                label,\n                shape=\"doublecircle\",\n                style=\"filled\",\n                fillcolor=color,\n            )\n        else:\n            dot.node(\n                str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n            )\n\n    for node_id, children in enumerate(self.children):\n        for char, child_id in children.items():\n            if char is not None:\n                edge_label = str(char)\n            else:\n                edge_label = \"End-of-Token\"\n\n            dot.edge(str(node_id), str(child_id), label=edge_label)\n\n    return dot\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.AsyncTokenCharacterTrie","title":"<code>AsyncTokenCharacterTrie</code>","text":"<p>An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>class AsyncTokenCharacterTrie:\n    \"\"\"An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.\"\"\"\n\n    def __init__(self, trie):\n        \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n        Args:\n            trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n        \"\"\"\n        self.trie = trie\n        self._queue = None\n        self._task = None\n\n    @classmethod\n    def from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n        \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n        Args:\n            vocab (list): The vocabulary over which the trie will be defined.\n            backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                    Defaults to 'parallel' which uses GPU acceleration when available.\n            **kwargs: Additional arguments passed to the trie constructor\n\n        Returns:\n            (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n        \"\"\"\n        if backend == \"sequential\":\n            trie = TokenCharacterTrie(decode=vocab, **kwargs)\n        elif backend == \"parallel\":\n            trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n        else:\n            raise ValueError(\n                f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n            )\n        return cls(trie)\n\n    async def _queue_request(self, request, op):\n        if not self._task or self._task.done():\n            self.start()\n\n        future = asyncio.Future()\n        await self._queue.put((request, future, op))\n        return future\n\n    async def weight_sum(self, ws):\n        \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated mass sums for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"sum\")\n        result = await future\n        return result\n\n    async def weight_max(self, ws):\n        \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated max weights for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"max\")\n        result = await future\n        return result\n\n    def start(self):\n        \"\"\"Start the background processing task if not already running.\"\"\"\n        if not self._task or self._task.done():\n            self._queue = (\n                asyncio.Queue()\n            )  # Create a new queue so that it is bound to the current event loop\n            self._task = asyncio.create_task(self._background_loop())\n\n    def _do_weight_sums(self, batch_weights):\n        return self.trie.batch_weight_sum(batch_weights)\n\n    def _do_weight_maxs(self, batch_weights):\n        return self.trie.batch_weight_max(batch_weights)\n\n    async def _background_loop(self):\n        \"\"\"Background task that processes queued weight sum and max requests.\n\n        Continuously monitors the queue for new requests and processes them in batches\n        using the underlying trie implementation.\n\n        Raises:\n            Exception: If any error occurs during processing, it is propagated to all\n                      pending futures in the current batch.\n        \"\"\"\n        while True:\n            try:\n                op_groups = defaultdict(list)\n\n                request, future, op = await self._queue.get()\n                op_groups[op].append((request, future))\n\n                while not self._queue.empty():\n                    request, future, op = await self._queue.get()\n                    op_groups[op].append((request, future))\n\n                for op, group in op_groups.items():\n                    requests, futures = zip(*group)\n\n                    if op == \"sum\":\n                        logger.debug(f\"processing {len(requests)} sum requests\")\n                        results = self._do_weight_sums(requests)\n                    elif op == \"max\":\n                        logger.debug(f\"processing {len(requests)} max requests\")\n                        results = self._do_weight_maxs(requests)\n                    else:\n                        raise ValueError(f\"Unknown operation: {op}\")\n\n                    for future, result in zip(futures, results):\n                        future.set_result(result)\n\n            except Exception as e:\n                for group in op_groups.values():\n                    for _, future in group:\n                        if not future.done():\n                            future.set_exception(e)\n                raise\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        if self._task and not self._task.done():\n            self._task.cancel()\n            try:\n                await self._task\n            except asyncio.CancelledError:\n                pass\n            self._task = None\n\n    def shutdown(self):\n        \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n        if self._task is not None:\n            try:\n                self._task.cancel()\n            except RuntimeError:\n                # Ignore runtime errors that might occur if event loop is closed\n                pass\n            self._task = None\n\n    def __del__(self):\n        self.shutdown()\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.AsyncTokenCharacterTrie.__init__","title":"<code>__init__(trie)</code>","text":"<p>Initialize an <code>AsyncTokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trie</code> <code>TokenCharacterTrie | ParallelTokenCharacterTrie</code> <p>The underlying <code>TokenCharacterTrie</code> or <code>ParallelTokenCharacterTrie</code> instance</p> required Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def __init__(self, trie):\n    \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n    Args:\n        trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n    \"\"\"\n    self.trie = trie\n    self._queue = None\n    self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.AsyncTokenCharacterTrie.from_vocab","title":"<code>from_vocab(vocab, backend='parallel', **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AsyncTokenCharacterTrie</code> from a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>list</code> <p>The vocabulary over which the trie will be defined.</p> required <code>backend</code> <code>str</code> <p>The trie implementation to use - either 'sequential' or 'parallel'.     Defaults to 'parallel' which uses GPU acceleration when available.</p> <code>'parallel'</code> <code>**kwargs</code> <p>Additional arguments passed to the trie constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTokenCharacterTrie</code> <p>The initialized asynchronous trie instance.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>@classmethod\ndef from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n    \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n    Args:\n        vocab (list): The vocabulary over which the trie will be defined.\n        backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                Defaults to 'parallel' which uses GPU acceleration when available.\n        **kwargs: Additional arguments passed to the trie constructor\n\n    Returns:\n        (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n    \"\"\"\n    if backend == \"sequential\":\n        trie = TokenCharacterTrie(decode=vocab, **kwargs)\n    elif backend == \"parallel\":\n        trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n    else:\n        raise ValueError(\n            f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n        )\n    return cls(trie)\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.AsyncTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_sum</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated mass sums for the given distribution.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def weight_sum(self, ws):\n    \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated mass sums for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"sum\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.AsyncTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_max</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated max weights for the given distribution.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def weight_max(self, ws):\n    \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated max weights for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"max\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.AsyncTokenCharacterTrie.start","title":"<code>start()</code>","text":"<p>Start the background processing task if not already running.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def start(self):\n    \"\"\"Start the background processing task if not already running.\"\"\"\n    if not self._task or self._task.done():\n        self._queue = (\n            asyncio.Queue()\n        )  # Create a new queue so that it is bound to the current event loop\n        self._task = asyncio.create_task(self._background_loop())\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.AsyncTokenCharacterTrie.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    if self._task and not self._task.done():\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.AsyncTokenCharacterTrie.shutdown","title":"<code>shutdown()</code>","text":"<p>Stop the background processing task and cleanup resources.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def shutdown(self):\n    \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n    if self._task is not None:\n        try:\n            self._task.cancel()\n        except RuntimeError:\n            # Ignore runtime errors that might occur if event loop is closed\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.ParallelTokenCharacterTrie","title":"<code>ParallelTokenCharacterTrie</code>","text":"<p>               Bases: <code>TokenCharacterTrie</code></p> <p>A GPU-optimized version of <code>TokenCharacterTrie</code> that performs weight sum and max operations in parallel.</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>class ParallelTokenCharacterTrie(TokenCharacterTrie):\n    \"\"\"A GPU-optimized version of `TokenCharacterTrie` that performs weight sum and max operations in parallel.\"\"\"\n\n    def __init__(self, decode, device=None, **kwargs):\n        super().__init__(decode, **kwargs)\n\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if self.device not in [\"cpu\", \"cuda\"]:\n            raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n        self._build_reachability_matrix()\n        self.token_ids = torch.tensor(\n            self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n        )\n\n    def _build_parent_map(self):\n        \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n        Returns:\n            (dict): A dictionary where keys are child nodes and values are their parent nodes.\n        \"\"\"\n        parent = {}\n        for node in range(len(self.children)):\n            for child in self.jump[node]:\n                parent[child] = node\n        return parent\n\n    def _build_reachability_matrix(self):\n        \"\"\"Constructs a sparse reachability matrix for efficient weight propagation.\n\n        The matrix M is constructed such that M[i,j] = 1 if node j is either:\n        - The leaf node i itself (self-connection)\n        - An ancestor of leaf node i in the trie\n        \"\"\"\n        leaf_indices = self.token_id_to_leaf[:, 1]\n        parent = self._build_parent_map()\n\n        rows, cols = [], []\n        for i, node in enumerate(leaf_indices):\n            # self connections\n            rows.append(i)\n            cols.append(node)\n\n            current = node\n            while current in parent:  # Walk up to root\n                ancestor = parent[current]\n                rows.append(i)\n                cols.append(ancestor)\n                current = ancestor\n\n        self.src_indices = torch.tensor(rows, dtype=torch.long, device=self.device)\n        self.dst_indices = torch.tensor(cols, dtype=torch.long, device=self.device)\n\n        indices = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n        values = torch.ones(len(rows), device=self.device)\n\n        self.M = torch.sparse_coo_tensor(\n            indices, values, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n    def _preprocess_ws(self, batch_ws):\n        processed_batch_ws = []\n        for ws in batch_ws:\n            if not isinstance(ws, torch.Tensor):\n                ws = torch.tensor(ws, device=self.device, dtype=torch.float32)\n            elif ws.device != self.device or ws.dtype != torch.float32:\n                ws = ws.to(device=self.device, dtype=torch.float32)\n            assert ws.shape[0] == len(self.decode), [ws.shape[0], len(self.decode)]\n            processed_batch_ws.append(ws)\n        return torch.stack(processed_batch_ws)\n\n    def weight_sum(self, ws):\n        \"\"\"Computes weight sums given token weights.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n        with a pre-computed reachability matrix.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batch version of `weight_sum`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n        return masses.cpu().numpy()\n\n    def weight_max(self, ws):\n        \"\"\"Computes the max weights given the token weights.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n        operations on GPU.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batch version of `weight_max`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n\n        # Get leaf weights\n        leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n        batch_size = leaf_weights.shape[0]\n\n        # Use scatter_reduce to propagate maximum values in parallel\n        result = torch.zeros((batch_size, len(self.children)), device=self.device)\n        result.scatter_reduce_(\n            dim=1,\n            index=self.dst_indices.expand(batch_size, -1),\n            src=leaf_weights[:, self.src_indices],\n            reduce=\"amax\",\n            include_self=False,\n        )\n\n        return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.ParallelTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Computes weight sums given token weights.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using sparse matrix multiplication with a pre-computed reachability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Computes weight sums given token weights.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n    with a pre-computed reachability matrix.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.ParallelTokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batch version of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batch version of `weight_sum`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n    return masses.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.ParallelTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Computes the max weights given the token weights.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using parallel scatter_reduce operations on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Computes the max weights given the token weights.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n    operations on GPU.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm/backend/trie/__init__/#genlm.backend.trie.ParallelTokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batch version of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batch version of `weight_max`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n\n    # Get leaf weights\n    leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n    batch_size = leaf_weights.shape[0]\n\n    # Use scatter_reduce to propagate maximum values in parallel\n    result = torch.zeros((batch_size, len(self.children)), device=self.device)\n    result.scatter_reduce_(\n        dim=1,\n        index=self.dst_indices.expand(batch_size, -1),\n        src=leaf_weights[:, self.src_indices],\n        reduce=\"amax\",\n        include_self=False,\n    )\n\n    return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm/backend/trie/async_impl/","title":"async_impl","text":""},{"location":"reference/genlm/backend/trie/async_impl/#genlm.backend.trie.async_impl.AsyncTokenCharacterTrie","title":"<code>AsyncTokenCharacterTrie</code>","text":"<p>An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>class AsyncTokenCharacterTrie:\n    \"\"\"An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.\"\"\"\n\n    def __init__(self, trie):\n        \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n        Args:\n            trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n        \"\"\"\n        self.trie = trie\n        self._queue = None\n        self._task = None\n\n    @classmethod\n    def from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n        \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n        Args:\n            vocab (list): The vocabulary over which the trie will be defined.\n            backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                    Defaults to 'parallel' which uses GPU acceleration when available.\n            **kwargs: Additional arguments passed to the trie constructor\n\n        Returns:\n            (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n        \"\"\"\n        if backend == \"sequential\":\n            trie = TokenCharacterTrie(decode=vocab, **kwargs)\n        elif backend == \"parallel\":\n            trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n        else:\n            raise ValueError(\n                f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n            )\n        return cls(trie)\n\n    async def _queue_request(self, request, op):\n        if not self._task or self._task.done():\n            self.start()\n\n        future = asyncio.Future()\n        await self._queue.put((request, future, op))\n        return future\n\n    async def weight_sum(self, ws):\n        \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated mass sums for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"sum\")\n        result = await future\n        return result\n\n    async def weight_max(self, ws):\n        \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated max weights for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"max\")\n        result = await future\n        return result\n\n    def start(self):\n        \"\"\"Start the background processing task if not already running.\"\"\"\n        if not self._task or self._task.done():\n            self._queue = (\n                asyncio.Queue()\n            )  # Create a new queue so that it is bound to the current event loop\n            self._task = asyncio.create_task(self._background_loop())\n\n    def _do_weight_sums(self, batch_weights):\n        return self.trie.batch_weight_sum(batch_weights)\n\n    def _do_weight_maxs(self, batch_weights):\n        return self.trie.batch_weight_max(batch_weights)\n\n    async def _background_loop(self):\n        \"\"\"Background task that processes queued weight sum and max requests.\n\n        Continuously monitors the queue for new requests and processes them in batches\n        using the underlying trie implementation.\n\n        Raises:\n            Exception: If any error occurs during processing, it is propagated to all\n                      pending futures in the current batch.\n        \"\"\"\n        while True:\n            try:\n                op_groups = defaultdict(list)\n\n                request, future, op = await self._queue.get()\n                op_groups[op].append((request, future))\n\n                while not self._queue.empty():\n                    request, future, op = await self._queue.get()\n                    op_groups[op].append((request, future))\n\n                for op, group in op_groups.items():\n                    requests, futures = zip(*group)\n\n                    if op == \"sum\":\n                        logger.debug(f\"processing {len(requests)} sum requests\")\n                        results = self._do_weight_sums(requests)\n                    elif op == \"max\":\n                        logger.debug(f\"processing {len(requests)} max requests\")\n                        results = self._do_weight_maxs(requests)\n                    else:\n                        raise ValueError(f\"Unknown operation: {op}\")\n\n                    for future, result in zip(futures, results):\n                        future.set_result(result)\n\n            except Exception as e:\n                for group in op_groups.values():\n                    for _, future in group:\n                        if not future.done():\n                            future.set_exception(e)\n                raise\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        if self._task and not self._task.done():\n            self._task.cancel()\n            try:\n                await self._task\n            except asyncio.CancelledError:\n                pass\n            self._task = None\n\n    def shutdown(self):\n        \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n        if self._task is not None:\n            try:\n                self._task.cancel()\n            except RuntimeError:\n                # Ignore runtime errors that might occur if event loop is closed\n                pass\n            self._task = None\n\n    def __del__(self):\n        self.shutdown()\n</code></pre>"},{"location":"reference/genlm/backend/trie/async_impl/#genlm.backend.trie.async_impl.AsyncTokenCharacterTrie.__init__","title":"<code>__init__(trie)</code>","text":"<p>Initialize an <code>AsyncTokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trie</code> <code>TokenCharacterTrie | ParallelTokenCharacterTrie</code> <p>The underlying <code>TokenCharacterTrie</code> or <code>ParallelTokenCharacterTrie</code> instance</p> required Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def __init__(self, trie):\n    \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n    Args:\n        trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n    \"\"\"\n    self.trie = trie\n    self._queue = None\n    self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/trie/async_impl/#genlm.backend.trie.async_impl.AsyncTokenCharacterTrie.from_vocab","title":"<code>from_vocab(vocab, backend='parallel', **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AsyncTokenCharacterTrie</code> from a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>list</code> <p>The vocabulary over which the trie will be defined.</p> required <code>backend</code> <code>str</code> <p>The trie implementation to use - either 'sequential' or 'parallel'.     Defaults to 'parallel' which uses GPU acceleration when available.</p> <code>'parallel'</code> <code>**kwargs</code> <p>Additional arguments passed to the trie constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTokenCharacterTrie</code> <p>The initialized asynchronous trie instance.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>@classmethod\ndef from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n    \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n    Args:\n        vocab (list): The vocabulary over which the trie will be defined.\n        backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                Defaults to 'parallel' which uses GPU acceleration when available.\n        **kwargs: Additional arguments passed to the trie constructor\n\n    Returns:\n        (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n    \"\"\"\n    if backend == \"sequential\":\n        trie = TokenCharacterTrie(decode=vocab, **kwargs)\n    elif backend == \"parallel\":\n        trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n    else:\n        raise ValueError(\n            f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n        )\n    return cls(trie)\n</code></pre>"},{"location":"reference/genlm/backend/trie/async_impl/#genlm.backend.trie.async_impl.AsyncTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_sum</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated mass sums for the given distribution.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def weight_sum(self, ws):\n    \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated mass sums for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"sum\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/trie/async_impl/#genlm.backend.trie.async_impl.AsyncTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_max</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated max weights for the given distribution.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def weight_max(self, ws):\n    \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated max weights for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"max\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm/backend/trie/async_impl/#genlm.backend.trie.async_impl.AsyncTokenCharacterTrie.start","title":"<code>start()</code>","text":"<p>Start the background processing task if not already running.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def start(self):\n    \"\"\"Start the background processing task if not already running.\"\"\"\n    if not self._task or self._task.done():\n        self._queue = (\n            asyncio.Queue()\n        )  # Create a new queue so that it is bound to the current event loop\n        self._task = asyncio.create_task(self._background_loop())\n</code></pre>"},{"location":"reference/genlm/backend/trie/async_impl/#genlm.backend.trie.async_impl.AsyncTokenCharacterTrie.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    if self._task and not self._task.done():\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/trie/async_impl/#genlm.backend.trie.async_impl.AsyncTokenCharacterTrie.shutdown","title":"<code>shutdown()</code>","text":"<p>Stop the background processing task and cleanup resources.</p> Source code in <code>genlm/backend/trie/async_impl.py</code> <pre><code>def shutdown(self):\n    \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n    if self._task is not None:\n        try:\n            self._task.cancel()\n        except RuntimeError:\n            # Ignore runtime errors that might occur if event loop is closed\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm/backend/trie/base/","title":"base","text":""},{"location":"reference/genlm/backend/trie/base/#genlm.backend.trie.base.TokenCharacterTrie","title":"<code>TokenCharacterTrie</code>","text":"<p>A trie data structure for efficient token-to-character mapping.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>class TokenCharacterTrie:\n    \"\"\"A trie data structure for efficient token-to-character mapping.\"\"\"\n\n    def __init__(self, decode):\n        \"\"\"Initialize a `TokenCharacterTrie`.\n\n        Args:\n            decode (list): List representing the token vocabulary.\n                Each element of the list must be iterable.\n        \"\"\"\n        self.decode = decode\n        self.word2leaf = {}\n        self.children = [{}]  # First node is root\n        self.root = 0\n        self.token_id_to_leaf = []\n\n        for token_id, word in enumerate(self.decode):\n            curr = self.root\n            for letter in word:\n                if letter not in self.children[curr]:\n                    self.children[curr][letter] = len(self.children)\n                    self.children.append({})\n                curr = self.children[curr][letter]\n\n            self.children[curr][None] = last = len(self.children)\n            self.children.append({})\n            assert word not in self.word2leaf, (\n                \"Can't have duplicate words in vocabulary\"\n            )\n            self.word2leaf[word] = last\n\n            self.token_id_to_leaf.append((token_id, last))\n\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n        )\n        self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n        # Renumber the states of the trie so that they are named by a contiguous\n        # range of integers and those integers respect the are topologically\n        # ordering of the trie topology.  This improves the efficiency of the\n        # updating the trie as it improves memory locality.\n        ordering = {}\n        for i, x in enumerate(self._order_full(self.root)):\n            ordering[x] = i\n        self._rename(f=lambda x: ordering[x])\n\n        node2prefix = {self.root: []}\n        for x in reversed(range(len(self.children))):\n            for letter, y in self.children[x].items():\n                if letter is None:\n                    node2prefix[y] = node2prefix[x]\n                else:\n                    node2prefix[y] = node2prefix[x] + [letter]\n        self.node2prefix = node2prefix\n\n    def _rename(self, f):\n        \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n        Args:\n            f (callable): Function that maps old node indices to new node indices\n        \"\"\"\n        N = len(self.children)\n\n        new_children = [{} for _ in range(N)]\n        nodes = range(N)\n\n        for x in nodes:\n            for letter, y in self.children[x].items():\n                new_children[f(x)][letter] = f(y)\n\n        self.root = f(self.root)\n        self.children = new_children\n        self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n        self.token_id_to_leaf = np.array(\n            [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n        )\n\n        self.ordering = np.array([f(x) for x in self.ordering])\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n        )\n\n    def _alloc_weights(self):\n        \"\"\"Allocate an array to store weight values for all nodes.\n\n        Returns:\n            np.ndarray: Zero-initialized array for storing weight values\n        \"\"\"\n        return np.zeros(len(self.children), dtype=np.float64)\n\n    def _preprocess_ws(self, ws):\n        \"\"\"Preprocess the weight vector to ensure it is a numpy array and on the correct device.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight vector\n        \"\"\"\n        if isinstance(ws, torch.Tensor):\n            if ws.device.type != \"cpu\":\n                ws = ws.cpu()\n            ws = ws.numpy()\n        return ws\n\n    def weight_sum(self, ws):\n        \"\"\"Compute weight sum for each node in the trie.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Summed weights for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_sum(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def weight_max(self, ws):\n        \"\"\"Compute weight max for each node in the trie.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight max values for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_max(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batched equivalent of `weight_sum`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_sum(ws) for ws in ws])\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batched equivalent of `weight_max`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_max(ws) for ws in ws])\n\n    def _order(self, node):\n        \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            int: Node indices in topological order\n        \"\"\"\n        for a in self.children[node]:\n            if a is None:\n                pass\n            else:\n                yield from self._order(self.children[node][a])\n        yield node\n\n    def _order_full(self, node):\n        \"\"\"Generate a complete topological ordering including all child nodes.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            (int): Node indices in complete topological order\n        \"\"\"\n        for a in self.children[node]:\n            yield from self._order_full(self.children[node][a])\n        yield node\n\n    def visualize(self, ws=None):\n        \"\"\"Visualize the trie structure using Graphviz.\n\n        Args:\n            ws (np.ndarray|None): Optional weight vector to display at each node.\n                                Should be of length `len(self.children)`.\n\n        Returns:\n            (graphviz.Digraph): The generated graph object\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:  # pragma: no cover\n            raise ImportError(\n                \"Please install graphviz: pip install graphviz\"\n            )  # pragma: no cover\n\n        if ws is not None and len(ws) != len(self.children):\n            raise ValueError(\n                f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n            )\n\n        dot = graphviz.Digraph(comment=\"Token Character Trie\")\n        dot.attr(rankdir=\"LR\")\n\n        # Create a subgraph for the legend\n        with dot.subgraph(name=\"cluster_legend\") as legend:\n            legend.attr(label=\"Legend\", fontsize=\"10\")\n            legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n            # Example internal node\n            legend.node(\n                \"legend_internal\",\n                \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n                shape=\"circle\",\n            )\n\n            # Example leaf node\n            legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n            legend.edge(\n                \"legend_internal\",\n                \"legend_leaf\",\n                label=\"Token item\",\n                fontsize=\"10\",\n            )\n\n            # Align legend horizontally\n            legend.attr(rankdir=\"TB\")\n            legend.attr(rank=\"same\")\n\n        # Add the main trie nodes and edges\n        for node_id in range(len(self.children)):\n            prefix = self.node2prefix[node_id]\n\n            if ws is not None:\n                label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n            else:\n                label = f\"{node_id}\\n'{prefix}'\"\n\n            # Color nodes based on mass if provided\n            if ws is not None:\n                max_ws = ws.max()\n                if max_ws &gt; 0:\n                    intensity = int(255 * (1 - ws[node_id] / max_ws))\n                    color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n                else:\n                    color = \"#ffffff\"  # white for zero mass\n            else:\n                color = \"#ffffff\"  # default white\n\n            if node_id in self.leaf2word:\n                dot.node(\n                    str(node_id),\n                    label,\n                    shape=\"doublecircle\",\n                    style=\"filled\",\n                    fillcolor=color,\n                )\n            else:\n                dot.node(\n                    str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n                )\n\n        for node_id, children in enumerate(self.children):\n            for char, child_id in children.items():\n                if char is not None:\n                    edge_label = str(char)\n                else:\n                    edge_label = \"End-of-Token\"\n\n                dot.edge(str(node_id), str(child_id), label=edge_label)\n\n        return dot\n</code></pre>"},{"location":"reference/genlm/backend/trie/base/#genlm.backend.trie.base.TokenCharacterTrie.__init__","title":"<code>__init__(decode)</code>","text":"<p>Initialize a <code>TokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>decode</code> <code>list</code> <p>List representing the token vocabulary. Each element of the list must be iterable.</p> required Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def __init__(self, decode):\n    \"\"\"Initialize a `TokenCharacterTrie`.\n\n    Args:\n        decode (list): List representing the token vocabulary.\n            Each element of the list must be iterable.\n    \"\"\"\n    self.decode = decode\n    self.word2leaf = {}\n    self.children = [{}]  # First node is root\n    self.root = 0\n    self.token_id_to_leaf = []\n\n    for token_id, word in enumerate(self.decode):\n        curr = self.root\n        for letter in word:\n            if letter not in self.children[curr]:\n                self.children[curr][letter] = len(self.children)\n                self.children.append({})\n            curr = self.children[curr][letter]\n\n        self.children[curr][None] = last = len(self.children)\n        self.children.append({})\n        assert word not in self.word2leaf, (\n            \"Can't have duplicate words in vocabulary\"\n        )\n        self.word2leaf[word] = last\n\n        self.token_id_to_leaf.append((token_id, last))\n\n    self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n    self.jump = List(\n        [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n    )\n    self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n    # Renumber the states of the trie so that they are named by a contiguous\n    # range of integers and those integers respect the are topologically\n    # ordering of the trie topology.  This improves the efficiency of the\n    # updating the trie as it improves memory locality.\n    ordering = {}\n    for i, x in enumerate(self._order_full(self.root)):\n        ordering[x] = i\n    self._rename(f=lambda x: ordering[x])\n\n    node2prefix = {self.root: []}\n    for x in reversed(range(len(self.children))):\n        for letter, y in self.children[x].items():\n            if letter is None:\n                node2prefix[y] = node2prefix[x]\n            else:\n                node2prefix[y] = node2prefix[x] + [letter]\n    self.node2prefix = node2prefix\n</code></pre>"},{"location":"reference/genlm/backend/trie/base/#genlm.backend.trie.base.TokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Compute weight sum for each node in the trie.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Compute weight sum for each node in the trie.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Summed weights for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_sum(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm/backend/trie/base/#genlm.backend.trie.base.TokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Compute weight max for each node in the trie.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Weight max values for each node in the trie.</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Compute weight max for each node in the trie.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Weight max values for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_max(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm/backend/trie/base/#genlm.backend.trie.base.TokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batched equivalent of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batched equivalent of `weight_sum`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_sum(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm/backend/trie/base/#genlm.backend.trie.base.TokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batched equivalent of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight max values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batched equivalent of `weight_max`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_max(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm/backend/trie/base/#genlm.backend.trie.base.TokenCharacterTrie.visualize","title":"<code>visualize(ws=None)</code>","text":"<p>Visualize the trie structure using Graphviz.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>ndarray | None</code> <p>Optional weight vector to display at each node.                 Should be of length <code>len(self.children)</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Digraph</code> <p>The generated graph object</p> Source code in <code>genlm/backend/trie/base.py</code> <pre><code>def visualize(self, ws=None):\n    \"\"\"Visualize the trie structure using Graphviz.\n\n    Args:\n        ws (np.ndarray|None): Optional weight vector to display at each node.\n                            Should be of length `len(self.children)`.\n\n    Returns:\n        (graphviz.Digraph): The generated graph object\n    \"\"\"\n    try:\n        import graphviz\n    except ImportError:  # pragma: no cover\n        raise ImportError(\n            \"Please install graphviz: pip install graphviz\"\n        )  # pragma: no cover\n\n    if ws is not None and len(ws) != len(self.children):\n        raise ValueError(\n            f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n        )\n\n    dot = graphviz.Digraph(comment=\"Token Character Trie\")\n    dot.attr(rankdir=\"LR\")\n\n    # Create a subgraph for the legend\n    with dot.subgraph(name=\"cluster_legend\") as legend:\n        legend.attr(label=\"Legend\", fontsize=\"10\")\n        legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n        # Example internal node\n        legend.node(\n            \"legend_internal\",\n            \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n            shape=\"circle\",\n        )\n\n        # Example leaf node\n        legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n        legend.edge(\n            \"legend_internal\",\n            \"legend_leaf\",\n            label=\"Token item\",\n            fontsize=\"10\",\n        )\n\n        # Align legend horizontally\n        legend.attr(rankdir=\"TB\")\n        legend.attr(rank=\"same\")\n\n    # Add the main trie nodes and edges\n    for node_id in range(len(self.children)):\n        prefix = self.node2prefix[node_id]\n\n        if ws is not None:\n            label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n        else:\n            label = f\"{node_id}\\n'{prefix}'\"\n\n        # Color nodes based on mass if provided\n        if ws is not None:\n            max_ws = ws.max()\n            if max_ws &gt; 0:\n                intensity = int(255 * (1 - ws[node_id] / max_ws))\n                color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n            else:\n                color = \"#ffffff\"  # white for zero mass\n        else:\n            color = \"#ffffff\"  # default white\n\n        if node_id in self.leaf2word:\n            dot.node(\n                str(node_id),\n                label,\n                shape=\"doublecircle\",\n                style=\"filled\",\n                fillcolor=color,\n            )\n        else:\n            dot.node(\n                str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n            )\n\n    for node_id, children in enumerate(self.children):\n        for char, child_id in children.items():\n            if char is not None:\n                edge_label = str(char)\n            else:\n                edge_label = \"End-of-Token\"\n\n            dot.edge(str(node_id), str(child_id), label=edge_label)\n\n    return dot\n</code></pre>"},{"location":"reference/genlm/backend/trie/parallel/","title":"parallel","text":""},{"location":"reference/genlm/backend/trie/parallel/#genlm.backend.trie.parallel.ParallelTokenCharacterTrie","title":"<code>ParallelTokenCharacterTrie</code>","text":"<p>               Bases: <code>TokenCharacterTrie</code></p> <p>A GPU-optimized version of <code>TokenCharacterTrie</code> that performs weight sum and max operations in parallel.</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>class ParallelTokenCharacterTrie(TokenCharacterTrie):\n    \"\"\"A GPU-optimized version of `TokenCharacterTrie` that performs weight sum and max operations in parallel.\"\"\"\n\n    def __init__(self, decode, device=None, **kwargs):\n        super().__init__(decode, **kwargs)\n\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if self.device not in [\"cpu\", \"cuda\"]:\n            raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n        self._build_reachability_matrix()\n        self.token_ids = torch.tensor(\n            self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n        )\n\n    def _build_parent_map(self):\n        \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n        Returns:\n            (dict): A dictionary where keys are child nodes and values are their parent nodes.\n        \"\"\"\n        parent = {}\n        for node in range(len(self.children)):\n            for child in self.jump[node]:\n                parent[child] = node\n        return parent\n\n    def _build_reachability_matrix(self):\n        \"\"\"Constructs a sparse reachability matrix for efficient weight propagation.\n\n        The matrix M is constructed such that M[i,j] = 1 if node j is either:\n        - The leaf node i itself (self-connection)\n        - An ancestor of leaf node i in the trie\n        \"\"\"\n        leaf_indices = self.token_id_to_leaf[:, 1]\n        parent = self._build_parent_map()\n\n        rows, cols = [], []\n        for i, node in enumerate(leaf_indices):\n            # self connections\n            rows.append(i)\n            cols.append(node)\n\n            current = node\n            while current in parent:  # Walk up to root\n                ancestor = parent[current]\n                rows.append(i)\n                cols.append(ancestor)\n                current = ancestor\n\n        self.src_indices = torch.tensor(rows, dtype=torch.long, device=self.device)\n        self.dst_indices = torch.tensor(cols, dtype=torch.long, device=self.device)\n\n        indices = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n        values = torch.ones(len(rows), device=self.device)\n\n        self.M = torch.sparse_coo_tensor(\n            indices, values, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n    def _preprocess_ws(self, batch_ws):\n        processed_batch_ws = []\n        for ws in batch_ws:\n            if not isinstance(ws, torch.Tensor):\n                ws = torch.tensor(ws, device=self.device, dtype=torch.float32)\n            elif ws.device != self.device or ws.dtype != torch.float32:\n                ws = ws.to(device=self.device, dtype=torch.float32)\n            assert ws.shape[0] == len(self.decode), [ws.shape[0], len(self.decode)]\n            processed_batch_ws.append(ws)\n        return torch.stack(processed_batch_ws)\n\n    def weight_sum(self, ws):\n        \"\"\"Computes weight sums given token weights.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n        with a pre-computed reachability matrix.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batch version of `weight_sum`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n        return masses.cpu().numpy()\n\n    def weight_max(self, ws):\n        \"\"\"Computes the max weights given the token weights.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n        operations on GPU.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batch version of `weight_max`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n\n        # Get leaf weights\n        leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n        batch_size = leaf_weights.shape[0]\n\n        # Use scatter_reduce to propagate maximum values in parallel\n        result = torch.zeros((batch_size, len(self.children)), device=self.device)\n        result.scatter_reduce_(\n            dim=1,\n            index=self.dst_indices.expand(batch_size, -1),\n            src=leaf_weights[:, self.src_indices],\n            reduce=\"amax\",\n            include_self=False,\n        )\n\n        return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm/backend/trie/parallel/#genlm.backend.trie.parallel.ParallelTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Computes weight sums given token weights.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using sparse matrix multiplication with a pre-computed reachability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Computes weight sums given token weights.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n    with a pre-computed reachability matrix.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm/backend/trie/parallel/#genlm.backend.trie.parallel.ParallelTokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batch version of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batch version of `weight_sum`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n    return masses.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm/backend/trie/parallel/#genlm.backend.trie.parallel.ParallelTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Computes the max weights given the token weights.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using parallel scatter_reduce operations on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Computes the max weights given the token weights.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n    operations on GPU.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm/backend/trie/parallel/#genlm.backend.trie.parallel.ParallelTokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batch version of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm/backend/trie/parallel.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batch version of `weight_max`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n\n    # Get leaf weights\n    leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n    batch_size = leaf_weights.shape[0]\n\n    # Use scatter_reduce to propagate maximum values in parallel\n    result = torch.zeros((batch_size, len(self.children)), device=self.device)\n    result.scatter_reduce_(\n        dim=1,\n        index=self.dst_indices.expand(batch_size, -1),\n        src=leaf_weights[:, self.src_indices],\n        reduce=\"amax\",\n        include_self=False,\n    )\n\n    return result.cpu().numpy()\n</code></pre>"}]}